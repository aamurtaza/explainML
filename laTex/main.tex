% --- Template for thesis / report with tktltiki2 class ---
% 
% last updated 2015/02/03 for tkltiki2 v1.03

\documentclass[english]{tktltiki2}

% tktltiki2 automatically loads babel, so you can simply
% give the language parameter (e.g. finnish, swedish, english, british) as
% a parameter for the class: \documentclass[finnish]{tktltiki2}.
% The information on title and abstract is generated automatically depending on
% the language, see below if you need to change any of these manually.
% 
% Class options:
% - grading                 -- Print labels for grading information on the front page.
% - disablelastpagecounter  -- Disables the automatic generation of page number information
%                              in the abstract. See also \numberofpagesinformation{} command below.
%
% The class also respects the following options of article class:
%   10pt, 11pt, 12pt, final, draft, oneside, twoside,
%   openright, openany, onecolumn, twocolumn, leqno, fleqn
%
% The default font size is 11pt. The paper size used is A4, other sizes are not supported.
%
% rubber: module pdftex

% Line spacing 
%
% Use \linespread{1.3} for "one and a half" line spacing, and
% \linespread{1.6} for "double" line spacing. Normally the lines
% are not spread, so the default line spread factor is 1.
\linespread{1.3}

% --- General packages ---

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}
\usepackage{multirow}
% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% --- Language-related settings ---
%
% these should be modified according to your language

% babelbib for non-english bibliography using bibtex
\usepackage[fixlanguage]{babelbib}
\usepackage{apalike}
\iflanguage{finnish}{\selectbiblanguage{finnish}}{}

% add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}
\iflanguage{finnish}{\settocbibname{Lähteet}}{}

% --- Theorem environment definitions ---

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

% --- tktltiki2 options ---
%
% The following commands define the information used to generate title and
% abstract pages. The following entries should be always specified:

\title{Interpreting "Black Box" models with Explanations Methods}
\author{Adnan Ali Murtaza}
\date{\today}
\level{Thesis}
\abstract{
%The challenge is that machine learning models become complicated black boxes when applied on a large scale to solve business problems. Such models are not explainable to end consumers, which raises trust, transparency and privacy concerns. Interpretability and explainability try to solve this challenge; researchers have developed explanation methods which approximate the inner working of the black box model by taking model and testing instances as an input. However, a new challenge has appeared to evaluate explanations produced by explanation methods. In this research, we present a new statistical approach based on similarity measures to evaluate explanations. We tested our framework mainly on classification (binary, multiple, multi-class) problems, including numerical, categorical and textual datasets (iris, titanic, 20-newsgroup).
}


% The following can be used to specify keywords and classification of the paper:

\keywords{Black Box, Interpretability, Agnostic Explanation Models}

% classification according to ACM Computing Classification System (http://www.acm.org/about/class/)
% This is probably mostly relevant for computer scientists
% uncomment the following; contents of \classification will be printed under the abstract with a title
% "ACM Computing Classification System (CCS):"
% \classification{}

% If the automatic page number counting is not working as desired in your case,
% uncomment the following to manually set the number of pages displayed in the abstract page:
%
% \numberofpagesinformation{16 pages + 10 appendix pages}
%
% If you are not a computer scientist, you will want to uncomment the following by hand and specify
% your department, faculty and subject by hand:
%
% \faculty{Faculty of Science}
% \department{Department of Computer Science}
% \subject{Computer Science}
%
% If you are not from the University of Helsinki, then you will most likely want to set these also:
%
% \university{University of Helsinki}
% \universitylong{HELSINGIN YLIOPISTO --- HELSINGFORS UNIVERSITET --- UNIVERSITY OF HELSINKI} % displayed on the top of the abstract page
% \city{Helsinki}
%

%%%%%%%%%%%%% Added Packages
%% package hyperref
\usepackage{hyperref}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}

%% package algorithm plus algorithmic style
%\usepackage{algorithm}
%\usepackage{algorithmic}
%
% package algorithm plus algpseudocode style
\usepackage{algorithm}
%\usepackage{arevmath}     % For math symbols
\usepackage[noend]{algpseudocode}

%% package algorithm2e style
%\usepackage{xcolor}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

%% package citations
%\usepackage{cite}
\usepackage{natbib}

%% NEW COMMANDS
% Algorithm definitions
\newcommand{\commentsymbol}{//}% or \% or $\triangleright$
\newcommand{\multicommentsymbolstart}{/*}
\newcommand{\multicommentsymbolend}{*/}
\algrenewcommand\algorithmiccomment[1]{\hfill \commentsymbol{} #1}
\makeatletter
\newcommand{\LineComment}[2][\algorithmicindent]{\Statex \hspace{#1}\commentsymbol{} #2}
\newcommand{\MultiLineComment}[2][\algorithmicindent]{\Statex \hspace{#1}\multicommentsymbolstart{} #2 \multicommentsymbolend{}}
\newcommand{\onespace}{\;}
\newcommand{\twospaces}{\;\;}

\graphicspath{{./plots/}}
\begin{document}

% --- Front matter ---

\frontmatter      % roman page numbering for front matter

\maketitle        % title page
\makeabstract     % abstract page

\tableofcontents  % table of contents

% --- Main matter ---
\mainmatter       % clear page, start arabic page numbering
\section{Introduction} %\section{One}
%\textit{Section Scope.}
%\begin{itemize}
%	\item \textit{Start introduction.}
%	\item \textit{Go through artificial intelligence, machine learning and the need of explainable machine-learning.}
%	\item - \textit{Add figure or diagram to show the relation among these fields.}
%\end{itemize} 
%\par Over a hundred years ago, scientists believed in a dream of having a computer programme who can think and learn like humans. Back then visualizing it would have been a mythical desire, but it is no longer an unrealistic aspiration. Artificial intelligence (AI) is now a growing arena, impacting on every aspect of research and business. Users do not often realize, but a simple Google search involves complex AI algorithms running behind the scenes. From intelligent software to automate routine labor or to make diagnoses in medicine, AI has a wide range of active experimentation topics and practical plans. \par
%Though most of the services and products are incorporating AI enhancements, often the core definitions of AI, machine learning (ML), and deep learning (DL) conceive some confusion. \citet{Goodfellow-et-al-2016} provides thorough explanations which describe the relationship among these fields. A logical inference by a machine, based on a list of formal mathematical rules expresses an early concept of AI. In contrast to hard-coded rules, the capability of solving intuitive problems which involve an ability to learn from the knowledge, and make decisions is machine learning (ML). A simple machine learning algorithm can recommend a book based on user history, or it can filter an email as spam or legitimate. \par
%However, in real-world applications, algorithms require an in-depth representation of data to learn and decide. For instance, speech or image detection needs a sophisticated, approximately human-level recognition. Deep learning (DL) undertakes these problematic tasks; it builds a complex representation by combining simple concepts in a hierarchy, such as combining the corners and edges of a car image \citep{Goodfellow-et-al-2016}. Deep Learning also expands the horizons to solve complicated, real-time learning problems, e.g., autonomous cars, language translations, and chatbots. Such deep models are becoming more accurate in learning, though the explanation behind the predictive performance gets compromised \citep{lipton2016mythos, ribeiro2016should, lundberg2017unified}. \par
%Moreover, providing a consistent, human understandable explanation is nearly impossible when an algorithm complexity is substantial. Such models become black boxes which are hard to interpret, affecting user's trust, thus, raising many concerns about transparency and interpretability. Explainable machine learning addresses these problems and aims to make machine learning interpretable and understandable to everyday users. \par

%However, it is difficult in some cases where an algorithm complexity is substantial and providing a consistent, human understandable explanation is nearly impossible. Such models become black boxes which are hard to interpret and more vulnerable especially in high-security systems. An increasing amount of black box models have raised many concerns about their interpretability. Explainable machine learning (XML) addresses these problems and aim to make machine learning interpretability and understandable.


\subsection{Background} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain the background of a problem.}
%\end{itemize}
%AIqballabs.ai has supported this thesis project, which adds valuable work to provides a new explanation evaluation scheme for state-of-art explanation methods. AIqballabs.ai is a digital initiative and platform about Artificial Intelligence and its ethical values. It aims to contribute and increase awareness of implementing explainable machine learning on a practical scale. It invites professionals and students from diverse backgrounds to participate and learn about how interpretability and explainability in Artificial Intelligence impact positively on our society.

\subsection{Motivation} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Write motivation for selecting the problem.}
%	\item \textit{Write why its important to solve the issue.}
%	\item \textit{Write high-level definition of relevant sub-problem.}
%\end{itemize}
%An increasing amount of real-time intelligent services have established the user needs for trust and transparency. In simple cases, users simply put trust on a system and do not feel the urgency of receiving an explanation behind predictions, e.g. email spam filtering. However, in advanced applications, consumers desire to know the rational logic behind specific predictions. For instance, patients who use online medical assistants often require that how a certain medicine is predicted by a system. \par
%On the other side, critical services who are using intricate machine learning models try to provide explanations to non-technical users, but it is a challenging task. Due to lack of transparency, users perceive unclarity, which directly impacts on the trust factor and services. This research project aims to explore one of the piece of this challenging problem. It focuses on how existing explanations methods interpret machine learning classification models and how to evaluate and compare them.

\subsection{Social and Ethical Concerns} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Write how machine learning and data usage is impacting society.}
%	\item \textit{Write how these new technologies raising social and ethical concerns.}
%	\item \textit{Highlight primary factors using some good examples.}
%\end{itemize}

%By its very nature, black box models are more vulnerable especially in high-security systems it is mandatory to place a debuggable and interpretable model. Moreover, after data regulations users demands for explanations has increased, pushing companies to provide transparency. Particularly in sectors such as insurance, health or banking it is crucial to interpret predictive models which assists in decision-making. It is also important to emphasize that companies who often receive personal data consent from users appear suspicious whenever they operate opposite to users consent \citep{lipton2016mythos}. For example, annoying advertisements on Facebook based on browsing history, rejection of a bank loan applications by a system often ignite the user's critical reasoning \citep{honegger2018shedding}. \par
%In such cases where a user needs to know the reason behind a specific decision. The ethical aspect of machine learning gives a user right to ask about an explanation. It is essential to provide and accommodate user when he/she has some discrimination concerns. European union general data protection regulation (GDPR) empowers data subject in such situations and place regulations restrictions on businesses to provide necessary explanations \citep{doshi2017accountability, goodman2017european} . Article 13 and 14 in GDPR asserts that a data subject which is included in algorithmic decision-making has the right to ask “meaningful information about the logic involved” \citep{goodman2017european}.

%Increasing amount of these {\itshape Black Box} models are also raising societal and ethical concerns among communities. Intelligent machine learning models learn from the digital traces of people which serve as data features. Thus, in a case when a person needs to know a reason behind a specific decision taken by a system, which is his/her right as according to General Data Protection Regulation (GDPR) \cite{doshi2017accountability}, how these {\itshape Black Box} models contribute? Most of the these are incapable of providing an answer because they are hard to interpret. \newline
%[ IMPROVEMENT REQUIRED Problems which are significantly essential to address are most often challenging to solve. Unfortunately, in March 2018 a self-driving car hit and killed the pedestrian in Tempe, Arizona \cite{uberselfdrivingcar}. Interpreting such Deep Neural Network (DNN) models can be very tricky due to the complex architectures, e.g. architecture for a self-driving car, probably manipulates millions of neurons and their activations which are difficult to interpret. This particular example is imperative because it also highlights the urgency of solving it. \newline IMPROVEMENT REQUIRED ]
\subsection{Explanations in the context of Machine Learning} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Write about what are explanations in machine learning.}
%	\item \textit{Highlight how explanations in machine learning address societal and ethical concerns.}
%\end{itemize}

%In explainable machine learning, explanations established interpretability and transparency of a model by answering {\itshape "why question" } behind the predictions \citep{honegger2018shedding}. \citet{honegger2018shedding} further categorizes interpretability into three types: global model interpretability, algorithm interpretability, and local model interpretability. On the other hand, \citet{lipton2016mythos} suggests establishing interpretability by breaking explanations into two pieces, transparency and post-hoc explanations. First covers {\itshape "how the model works?"} and, the latter {\itshape "how much more model can tell about the predictions?"}. \par
%These questions focus on finding possible ways to enhance user's trust in model and trust in predictions. Indeed, the answers to these questions vary and depend on the application domain. However, several explanation methods have been proposed which partially gives explanations for the model predictions. Though, due to a lack of consensus on {\itshape "what is a good explanation?"} solving these questions are challenging.\par

%[ IMPROVEMENT REQUIRED To provide solutions to open {\itshape Black Box} models, researchers have suggested multiple frameworks and terminologies \cite{guidotti2018survey}.  \cite{lipton2016mythos} also categories the solution into two primary ways "transparency" and "post-hoc" explanations, first one covers the question of {\itshape "how the model works"} and, the latter one try to address {\itshape "how much more model can tell about the predictions."} 
%One important thing is also discussing the difference between {\itshape interpretability } and {\itshape explanations.} \cite{honegger2018shedding} provides this differentiation by stating that explanations established interpretability of a model by answering {\itshape "why question" } behind the predictions.
%Moreover, interpretable machine learning models are which can provide human-understandable explanations for predictions, thus can be called as {\itshape interpretable } models in contrast to {\itshape black box} models. \newline
%For many years, researchers have been actively working and proposing various methods to make {\itshape black box } models interpretable \cite{guidotti2018survey}. They further organize these methods into two classes: (1) Model Dependent Explanation Methods, which make black box model interpretable but tightly coupled with specific model architecture (2) Model Agnostic Explanation Methods; these methods are generalizable, thus, applicable to multiple black box models which deal with tabular, images, textual data. \newline
%Local Interpretable Model-Agnostic Explanations (LIME) \cite{ribeiro2016should} and Shapely Additive Explanations (SHAP) \cite{lundberg2017unified} are few of the most prominent agnostic models identified in the literature \cite{guidotti2018survey,honegger2018shedding}, mainly provides the framework for local model interpretability, that is, producing a human-understandable explanation for a particular prediction. These explanation methods can open complex models such as DNN and work with several kinds of datasets.\newline IMPROVEMENT REQUIRED ]

\subsection{Problem Statements} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Define the problem statement.}
%	\item \textit{Explain why the above problem worth exploration and solution.}
%	\item \textit{Define sub-problems}
%\end{itemize}
%The challenge is that machine learning models become complicated black boxes when applied on a large scale to solve business problems. Such models are not explainable to end consumers, which raises trust, transparency and privacy concerns. Interpretability and explainability try to solve this challenge; researchers have developed explanation methods which approximate the inner working of the black box model by taking model and testing instances as an input. However, a new challenge has appeared to evaluate explanations produced by explanation methods. In this research, we present a new statistical approach based on similarity measures to evaluate explanations. We tested our framework mainly on classification (binary, multiple, multi-class) problems, including numerical, categorical and textual datasets (iris, titanic, 20-newsgroup).
\subsubsection{Research Questions} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Formulate research questions using problem definitions.}
%	\item \textit{Formulate sub-research questions to define the scope.}
%\end{itemize}

\begin{enumerate}
	\item RQ: How can explanation evaluation framework enhance the interpretability in machine learning?
	
	\begin{enumerate}\addtocounter{enumii}{0}
		\item RQ1: What are the current approaches to interpret machine learning models?
		\begin{enumerate}
			\item What are the explanation methods and what are their types?
			\item How to interpret predictive models and extract explanations using  explanations methods?
		\end{enumerate}
	\end{enumerate}
	
	\begin{enumerate}\addtocounter{enumii}{1}
		\item RQ2: What are the existing ways of evaluating and comparing explanations methods?
		\begin{enumerate}
			\item What are the limitations in evaluating explanations?
		\end{enumerate}
	\end{enumerate}
	
	\begin{enumerate}\addtocounter{enumii}{2}
		\item RQ3: How can we perform stability test among explanations methods?
		\begin{enumerate}
			\item What it means to have stability in explanations?
			\item How to define an explanation evaluation framework which measures stability?
			\item How to evaluate the quality of an explanation evaluation framework?
		\end{enumerate}
	\end{enumerate}
\end{enumerate}
\subsubsection{Research Methodology} %\subsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Write about research methodology you have used to conduct the research.}
%	\item \textit{Write the usefulness of selected methodology.}
%	\item \textit{Formulate the main steps of research methodology.}
%	\item \textit{- Design the diagram to show concept.}
%\end{itemize}

\subsection{Thesis Outline} %\subsection{}
%The thesis outline is as Section (2) gives an overview on specific properties of explanations, types of explanation methods and existing work on evaluating explanations. Section (3) describes the intuition behind explanation methods chosen for this research and defines our new approach to evaluate explanations. Section (4) defines our experimental setup and shows our experiments and results. Section (5) tests our framework scalability, complexity by defining another experimental setup and running experiments on multi-class textual classification problem. In the end Section (6) concludes our results, discusses limitations and future work.

\section{Interpretability in Machine Learning} %\section{Two}
%\textit{Section Scope.}
%\begin{itemize}
%	\item \textit{Explain about interpretability in machine learning.}
%	\item \textit{Highlight its importance and value.}
%	\item \textit{Explain high-level about its works and how it has been evolving in research and practice}
%\end{itemize}

\subsection{Interpretable models vs Black Box Models} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Write on interpretable models}
%	\item \textit{Write about what are black box models}
%\end{itemize}

\subsubsection{Linear models}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain linear models}
%	\item \textit{Explain how they interpret black box models}
%\end{itemize}

\subsubsection{Tree models}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain Tree models and how they work}
%	\item \textit{Brief how they interpret black box models}
%\end{itemize}

\subsection{Explanation Methods in Machine Learning} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what are explanations methods in machine learning}
%	\item \textit{Write about their research aspect and value.}
%	\item \textit{What are primary types of explanation methods}
%\end{itemize}


\subsubsection{Model-Dependent Explanation Methods} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what are model-dependent methods}
%	\item \textit{Explain what are popular methods present in literature}
%	\item \textit{Write on high-level about their working, pros and cons}
%	\item \textit{- Add Table to show the survey (RQ1 Review Literature)}
%\end{itemize}

\subsubsection{Model-Agnostic Explanation Methods} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what are model-agnostic methods}
%	\item \textit{Explain what are popular methods present in literature}
%	\item \textit{Write on high-level about their working, pros and cons}
%	\item \textit{- Add Table to show the survey (RQ1 Review Literature)}
%\end{itemize}

\subsection{Properties of Explanations} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain the challenge to quantify the explanations}
%	\item \textit{Explain the properties of good explanation present in literature}
%	\item \textit{Write on high-level about their working, pros and cons}
%	\item \textit{- Add Table to show the survey (RQ2 Review Literature)}
%\end{itemize}

\subsection{Evaluating Explanations} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Highlight how research has been doing work on evaluating explanations}
%	\item \textit{Write explanation evaluation schemes present in literature}
%	\item \textit{Explain their mechanism, pros and cons.}
%	\item \textit{- Add Table to show the survey (RQ2 Review Literature)}
%\end{itemize}


\section{Methodologies} %\section{Three}
%\textit{Section Scope.}
%\begin{itemize}
%	\item \textit{Communicate about primary methodologies which are selected}
%\end{itemize}
This section explains the methodologies which are carried from research to measure explanations robustness. Explanations are generated using methods; Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP) \citep{ribeiro2016should,lundberg2017unified}. To measure the robustness of explanations, local Lipschitz Continuity technique \citep{alvarez2018robustness} is implemented and tested out on UCI classification datasets.

On a high-level, the steps work as follow. 1) Predictive models are trained to perform classification tasks; these models behave as black-box models in the process. 2) Explanation methods interpret black-box models to find the reasons behind the predictions. 3) Explanations robustness is measured using local Lipschitz estimation technique \citep{alvarez2018robustness}.

Let $f_c: X \rightarrow y$ as a black-box classifier who classifies $x \in X; X = {\rm I\!R}^d$ to class $c_x; c_x \in y = \{c_1, ..., c_n\}$. Now, let $z \in X$ be an instance from input space which requires an explanation for a prediction $f_c(z) = c_z$. An explanation model $f_e(z)$ outputs an explanation feature vector $ e = {\rm I\!R}^d$ for which the estimation of robustness or stability is required. The main objective of this section is to define the notion of stability using local Lipschitz esitmation which requires a neighborhood $N_{\epsilon}(z)$ around an instance $z \in X$. Verbally, the notion of stability is the estimation of \textit{"how similar are the explanations for similar instances"} and vice versa indicates the notion of unstabiliy.

An overview of predictive models and explanation methods is presented in sub-section 3.1 and 3.2, respectively. Sub-section 3.3 defines and explains the robustness measure and presents algorithms used in the implementation.

\subsection{Predictive Models}%\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain about selected training methods}
%\end{itemize}

\subsubsection{Logistic Regression} %\subsubsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what is logistic regression}
%	\item \textit{Explain how it works, learn and predict}
%\end{itemize}

\subsubsection{Random Forest} %\subsubsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what is random forest}
%	\item \textit{Explain how it works, learn and predict}
%\end{itemize}

\subsection{Explanation Methods} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain about selected explanation methods}
%\end{itemize}

\subsubsection{Local Surrogate (LIME)} %\subsubsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what is Local Surrogate and LIME}
%	\item \textit{Explain how it works and interpret black box models}
%	\item \textit{Communicate the pros and cons of LIME.}
%	\item \textit{- Add figure for an explanation}
%\end{itemize}

\subsubsection{Shapley Values (SHAP)} %\subsubsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what is Shapley Values and SHAP}
%	\item \textit{Explain how it works and interpret black box models}
%	\item \textit{Communicate the pros and cons of LIME.}
%	\item \textit{- Add figure for an explanation}
%\end{itemize}

\subsection{Defining Similarity}\label{sec:defining_similarity} %\subsubsection{}
To formulate the explanation evaluation framework the pre-requistie is to specify a similarity measure. One trivial way is using euclidean distance which is an ordinary straight line distance between two points in euclidean space (wikipedia). For instance, in euclidean n-space the distance between two points $x = \{x_1, x_2, ..., x_n\}$ and $y = \{y_1, y_2, ..., y_n\}$ is:

\begin{align*}
d(x,y) = d(y,x) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
\end{align*}

Using Euclidean distance choice, it is straightforward to define a small neighbourhood around any instance. However, estimating neighbourhood is challenging task itself, but euclidean distance allows us to find similar points trivially. The Definition~\ref{def:1} and Definition~\ref{def:2} formalize the notion of defining a ball of radius $r$ centered at instance $a$ as a neighbourhood policy. The subtle difference between the definitions is that closed balls contain all "exterior" points as compared to open balls.
\begin{definition}\label{def:1}{Let $a \in {\rm I\!R}^n$: Then the \textbf{Open Ball centered at $a$ with Radius $r > 0$} denoted $B(a,r)$ is defined to be a set of all points $x \in {\rm I\!R}^2$ such that $||x-a|| < r$, that is, $\{x \in {\rm I\!R}^2 |\;||x - a|| < r\}.$}
\end{definition}
Now, if n=2 then for each $a = (a_1,a_2) \in {\rm I\!R}^2$ and $r \in {\rm I\!R}^2$ where $r > 0$ we have an open ball centered at $a$ with radius $r$ is:
\begin{align*}
B(a,r) = \{x \in {\rm I\!R}^2\ : \sqrt{(x_1 - a_1)^2 + (x_2 - a_2)^2} < r\}
\end{align*}
%% Definition of closed ball
\begin{definition}\label{def:2}{Let $a \in {\rm I\!R}^n$: Then the \textbf{Closed Ball centered at $a$ with Radius $r > 0$} denoted $B(a,r)$ is defined to be a set of all points $x \in {\rm I\!R}^2$ such that $||x-a|| \leq r$, that is, $\{x \in {\rm I\!R}^2 |\;||x - a|| \leq r\}.$}
\end{definition}
And, if n=2 then for each $a = (a_1,a_2) \in {\rm I\!R}^2$ and $r \in {\rm I\!R}^2$ where $r > 0$ we have the closed ball centered at $a$ with radius $r$ is:
\begin{align*}
B(a,r) = \{x \in {\rm I\!R}^2\ : \sqrt{(x_1 - a_1)^2 + (x_2 - a_2)^2} \leq r\}
\end{align*}

Now, with having above definitions we can proceed to next section.

\subsection{Defining Explanation Evaluation Framework} %\subsection{}
The section attempts to define a robustness measure which seeks a variability in the explanations generated by interpretable methods for similar inputs. Intuitively, the idea is if a variation in inputs is subtle enough that it does not change the model's predictions, then it should not change explanations either. \citet{alvarez2018robustness} proposes a notion of robustness measure which forms the basis of this work.

The critical argument was that the understanding of a complex model using a single point-wise explanations might lead to a false understanding. One way to address this would be looking beyond the point of interest and covering its neighbourhood to examine the behaviour of a model. Therefore a crucial property such as \textit{robustness} is required to measure the explanations generated by the interpretability methods. It ensures that an explanation of a point is roughly constant in its vicinity regardless of the models (linear, tree-based).  In a simple form, it means that explanations for similar instances should not widely differentiate.

The critical argument was that the understanding of a complex model using a single point-wise explanations might lead to a false understanding. One way to address this would be looking beyond the point of interest and covering its neighbourhood to examine the behaviour of a model. Therefore a crucial property such as \textit{robustness} is required to measure the explanations generated by the interpretability methods. It ensures that an explanation of a point is roughly constant in its vicinity regardless of the models (linear, tree-based).  In a simple form, it means that explanations for similar instances should not widely differentiate.

\begin{wrapfigure}{r}{0.5\textwidth}
	\centering
	\vspace*{-7mm}
	\includegraphics[width=1.0\linewidth]{alvarez2018_robustness_figure1}
	\vspace*{-10mm}
	\caption{LIME and SHAP explanations for binary classifiers \citep{alvarez2018robustness}: The heatmaps show models’ positive-class probabilities and bar charts represent explanations (attribution values where x in green and y in purple) for predictions. It is visible that both LIME and SHAP explanations are stable for linear SVM model (top) but significantly vary for a non-linear two-layer neural network (bottom).}
	\label{fig:alvarez2018_robustness_figure1}
\end{wrapfigure}

Figure~\ref{fig:alvarez2018_robustness_figure1} presented in original work \citep{alvarez2018robustness} shows the explanations generated by LIME \cite{ribeiro2016should} and SHAP \cite{lundberg2017unified} for binary classifier predictions trained on a two-dimensional dataset. It explained that the explanations for a complex neural network model (bottom row) are often inconsistent and vary noticeably for neighbouring points. On the other side, explanations for linear SVM's predictions (top row) are relatively stable. This instability motivated the investigation of such a phenomenon and a need for an objective tool to quantify stability.

In light of this, Lipschitz continuity as function stability has been suggested to measure the relative change in output concerning the inputs. It measures the substantial relative deviations (global) throughout the input space. Since in the context of interpretable methods, explanations do not require uniformity for very distant inputs. Therefore, a local notion of stability (Definition~\ref{def:3}) has been proposed in the form of local Lipschitz continuity (Hein and Andriushchenko, 2017; Weng et al., 2018), which measure the relative deviations in a small neighbourhood input space.
\begin{definition}\label{def:3}{\textit{local notion of stability}:}
	$f: X \subseteq {\rm I\!R}^n \rightarrow {\rm I\!R}^m$ is \textbf{\textit{locally Lipschitz}} \textit{if for every $x_0$ there exist $\delta > 0$ and $L \in {\rm I\!R}$ such that $||x - x_0|| < \delta$ implies $||f(x) - f(x_0)|| \leq L||x - x_0||$.}
\end{definition}

In the Definition~\ref{def:3}, $\delta$ and $L$ are dependent on an anchor point $x_i$. It allows quantifying the robustness of the explanations generated by a model $f$ in terms of constant $L$. Usually, this term is not known, and it requires an estimation. One straightforward way to solve it for every point of interest $x_i$ is to do as an optimization problem (Equation~\ref{eq:1}).
%Computing this quantity is a challenge itself because a function is not differentiable end-to-end. The computation needs a restricted evaluation budget, and luckily there are various approaches off the shelf to manage computation expenses, i.e., Bayesian Optimization  (Snoek et al. , 2012, and references therein).
\begin{equation}\label{eq:1}
\hat{L}(x_i) = \operatorname*{argmax}_{x_j \in B_{\epsilon}(x_i)}  \frac{||f(x_i) - f(x_j)||_{2}}{||x_i - x_j||_{2}}
\end{equation}

Where $N_{\epsilon}(x_{i})$ is ball of radius $\epsilon$ centered at $x_i$. Using the formulation in \hyperref[sec:defining_similarity]{Section 3.3}, a neigrbourhood can be defined as below equation; This same form is used in the original work \citep{alvarez2018robustness} and it is noticeable that this relates to the closed ball Definiton~\ref{def:2}.
\begin{align*}
N_{\epsilon}(x_i) = \{x_j \in X|\;||x_i - x_j|| \leq \epsilon\}
\end{align*}  

%\begin{definition}{\textit{local notion of stability}:}
%	$f: X \subseteq {\rm I\!R}^n \rightarrow {\rm I\!R}^m$ is \textbf{\textit{locally Lipschitz}} \textit{if for every $x_{0}$ there exist $\delta > 0$ and $L \in {\rm I\!R}$ such that $||x - x_{0}|| < \delta$ implies $||f(x) - f(x_{0})|| \leq L||x - x_{0}||$.}
%\end{definition}

Now, moving towards algorithmic functionality, Algorithm~\ref{alg:Algo1} shows the implementation for local Lipschitz estimations used in various experimental settings. It takes 1) all the test-set points $ \in X$. 2) the explanations (attribution) vectors $f(X)$ generated by explanation model $f$  for all the test-set points $\in X$. 3) the value $\epsilon$ to define the radius.  It follows the intuitive strategy, iterates for each point of interest $x_i$ to find its neighbourhood (Algorithm~\ref{alg:Algo2}) and local Lipschitz value (Algorithm~\ref{alg:Algo3}) and put the value into an array of Lipschitz estimations. At the end of the iterations, it returns an array which contains the local Lipschitz values for all the test-set points.
%%%% Package algorithm plus algpseudocode style
\begin{algorithm}[H]
	\caption{$LipschitzEstimations(X,\onespace f(X),\onespace \epsilon)$}
	\label{alg:Algo1}
	\hspace*{\algorithmicindent} \textbf{Input}\textbf{:} $\{x_i,...,x_n\} \in X$\textbf{:} test set with all the points, $\onespace \{f(x_i),...,f(x_n)\} \in f(X)$\textbf{:} set of generated explanation vectors for all the points in a test set, $\onespace \epsilon$\textbf{:} radius threshold \\
	\hspace*{\algorithmicindent} \textbf{Output}\textbf{:} $\hat{L}(X)$\textbf{:} array which holds local Lipschitz estimation value for all points $\in X$ 
	\begin{algorithmic}[1]
		\State $d$ = $||x_i||$
		\State $\epsilon$ = $\epsilon * \sqrt{d}$ 
		\For{\texttt{$x_{i}$ in X}}
		\State $N_{\epsilon}(x_i) \gets Neighborhood(x_i,\; X,\; \epsilon)$
		\State $\hat{L}(x_{i}) \gets LocalLipschitzEstimate(x_i,\; f(X),\; N_\epsilon(x_i))$
		\State $\hat{L}(X) \gets$ $\hat{L}(x_i)$
		\EndFor
		\MultiLineComment[0\dimexpr\algorithmicindent]{array of local lipschitz estimation for all points in $X$}
		\State \textbf{return} $\hat{L}(X)$
		%		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Similarly, Algorithm~\ref{alg:Algo2} also takes a simple approach to find the neighbourhood points for any instance $x_i$. Algorithm~\ref{alg:Algo1} calls it for every instance and passes 1) anchor point or point of interest $x_i$, which requires a local Lipschitz estimation 2) all the test-set points $ \in X$. 3) the value $\epsilon$ to define the radius. Given the inputs, it calculates the Euclidean distances from anchor point $x_i$ to all points in $X$. If the distance is less than or equal to $\epsilon$ then the point $x_j \in X$ is placed into an array of neighbourhood points $N_{\epsilon}(x_i)$. Finally, it returns the $N_{\epsilon}(x_i)$, which contain all the neighbourhood points from the anchor point $x_i$.
%%%% Package algorithm plus algpseudocode style
\begin{algorithm}[H]
	\caption{$Neighborhood(x_i,\onespace X,\onespace \epsilon)$}
	\label{alg:Algo2}
	\hspace*{\algorithmicindent} \textbf{Input}\textbf{:} $x_i$\textbf{:} point of interest which requires a local Lipschitz estimation, $\onespace \{x_i,...,x_n\} \in X$\textbf{:} test set with all the points, $\onespace \epsilon$\textbf{:} radius threshold  \\
	\hspace*{\algorithmicindent} \textbf{Output}\textbf{:} $N_{\epsilon}(x_i)$\textbf{:} set of points in a small neighbourhood defined by $\epsilon$ around $x_i$  
	\begin{algorithmic}[1]
%	\Comment{$\epsilon$ = radius}
		\For{\texttt{$x_j$ in X}}
			\State $d(x_i, x_j)$ = $\sqrt{(x_j - x_i)^2}$
			\If {$d(x_{i}, x_j) \leq \epsilon$}
				\State $N_{\epsilon}(x_i) \gets x_j$
			\EndIf
		\EndFor
		\MultiLineComment[0\dimexpr\algorithmicindent]{an array contains neighbourhood points}
		\State \textbf{return} $N_{\epsilon}(x_i)$ 
	\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:Algo3} takes 1) anchor point or point of interest $x_i$, which requires a local Lipschitz estimation 2) the explanations (attribution) vectors $f(X)$ generated by explanation model $f$  for all the test-set points $\in X$. 3) the neighbourhood points from the anchor point $x_i$. It estimates the maximal local Lipschitz value for the anchor point $x_i$ by solving the (Equation 1) and outputs the computed value as $\hat{L}(x_i)$.
%%%% Package algorithm plus algpseudocode style
\begin{algorithm}[H]
	\caption{$LocalLipschitzEstimate(x_i,\onespace f(X),\onespace N_\epsilon(x_i))$}
	\label{alg:Algo3}
	\hspace*{\algorithmicindent} \textbf{Input}\textbf{:} $x_i$\textbf{:} point of interest which requires a local Lipschitz estimation, $\onespace \{f(x_i),...,f(x_n)\} \in f(X)$\textbf{:} set of generated explanation vectors for all the points in a test set, $\onespace N_\epsilon(x_i)$\textbf{:} set of points in a small neighbourhood defined by $\epsilon$ around $x_i$ \\
	\hspace*{\algorithmicindent} \textbf{Output}\textbf{:} $\hat{L}(x_i)$\textbf{:} local Lipschitz estimation value computed using equation 1
	\begin{algorithmic}[1]
		\State $\hat{L}(x_i) \gets \operatorname*{argmax}_{x_j \in N_\epsilon(x_i)} \frac{||f(x_i) - f(x_j)||_2}{||x_i - x_j||_2}$ \Comment {equation 1}
		\State \textbf{return} $\hat{L}(x_i)$

	\end{algorithmic}
\end{algorithm}

Now, at the beginning of this section mentioned that local Lipschitz estimation measure relative deviations inside the neighbourhood input space. It is helpful to use Figure~\ref{fig:alvarez2018_robustness_figure2} from \citep{alvarez2018robustness} that demonstrates how $\hat{L}(x_i)$ values computed using above mechanism describe the relative deviations. It supports the idea of choosing the worst-case deviation of any point of interest $x_i$ and reveals an example presented in Figure 2. For instance, the examples from the BOSTON dataset shows that however, the point of interests are incredibly similar, but their explanations by each method for the model’s prediction vary considerably. It also depicts that maximizing equation (1) for $x_i$ measures relative deviation and allows us to enforce the robustness mechanism on explanations methods.
\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{alvarez2018_robustness_figure2}
	\vspace*{-10mm}
	\caption{LIME and SHAP explanations deviation examples: \citep{alvarez2018robustness}: \textbf{Top}: example $x_i$ from the BOSTON dataset and its explanations (attributions). \textbf{Bottom}: explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per equation (1).}
	\label{fig:alvarez2018_robustness_figure2}
\end{figure}




%Quick comment on Alg. 1: it is not clear from the algorithm what is being computed there. Presumably for each $x_{i}$ in X you want to compute maximal |f(x)-f($x_{i}$)|/[x-$x_{i}$| where $x \in N(x_{i})?$ You could also explain what the numbers in Tab. 1 are. The level of detail here is such that I could replicate your experiment, if I had the data and if I wanted to. E.g., you need to be more explicit in the algorithm and also define what you mean by “aggregation” in Tab. 1.


%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explaining and defining Explanation Evaluation Framework (RQ3)}
%\end{itemize}

%\begin{definition}{Stability:}
%	If data points, \(\left\{x_{i},...,x_{n}\right\} \in X,i \in \left\{1,...,N\right\}\) are classified among predicted labels \(\left\{l_{j},...,l_{k}\right\} \in L, j \in \left\{1,...,K\right\}\), then corresponding explanations \(\left\{e_{i},...,e_{n}\right\} \in E\) should form the $K$ clusters  \(\left\{c_{j},...,c_{k}\right\} \in C\) which, follow the similar classification as predicted labels.
%	\begin{align}
%	\forall i,j,k \in N \;\;\; f\left(x_{i}\right) = f\left(x_{j}\right), \;\; f\left(x_{i}\right) \neq f\left(x_{k}\right) \Rightarrow \\
%	sim(g(f(x_{i})), g(f(x_{j}))) < sim(g(f(x_{i})), g(f(x_{k}))) 
%	 \\ symbols new line in maths
%	 \; symbols space in maths
%	\end{align}
%\end{definition}

%\subsubsection{Stability Test} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what stability means in literature}
%	\item \textit{Explain a stability evaluation test algorithm, which is developed in this research.}
%\end{itemize}

%\begin{algorithm}
%	\caption{Training Predictive Models Procedure}
%	\begin{algorithmic}[1]
%		\State Split dataset into training-testing sets.
%		\State Train predictive model \textit{'f'} using training set.
%		\State Predict labels for testing set using predictive model \textit{'f'}
%%		\State Show results
%	\end{algorithmic}
%\end{algorithm} \newline

%\subsubsection{Consistency} %\subsubsection{}

%\subsubsection{Similarity Measures} %\subsection{}
%- Explaining the similarity measure used in definition \newline
%Brief why certain measure is selected and its usefulness \newline

\section{Experimentation and Results} %\section{Four}
%Open text to experimentation section \newline

\subsection{Datasets}
%Brief iris dataset \newline
%Brief ionosphere dataset \newline
%Brief compas dataset \newline
%Brief titanic dataset \newline
%- Add small table explaining datasets \newline
% booktabs style
%\begin{table}[H]
%	\caption{Datasets for quantitative experimentation}
%	\label{tab:quantitative_experimentation_datasets}
%	\begin{center}
%		\begin{tabular}{@{}ccccc@{}}
%			\toprule
%			Datasets & Features \# & Rows \# & Classes \# & Instances per class \# \\ \midrule
%			\multicolumn{1}{|c|}{Iris} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{150} & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{50/50/50} \\ \midrule
%			
%			\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \midrule
%			
%			\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \midrule
%			
%		\end{tabular}
%	\end{center}
%\end{table}

%\begin{center}
%	\begin{itemize}
%		\item \textit{Iris}: Iris is a well-known classification tasks of plants based on four flower leaf characteristics. It has 150 data points and three classes.
%%		\item \textit{Ionosphere}: Radar data consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. The targets were free electrons in the ionosphere, "Good" returns some type of structure in the ionosphere. "Bad" returns are those that do not
%		
%	\end{itemize}
%\end{center}

\subsection{Experimental Setup}
%Brief about the experimental setup \newline
%- Add table or diagram to show setup \newline
%\begin{table}[H]
%	\caption{Training model settings}
%	\label{tab:quantitative_experimentation_training_settings}
%	\begin{center}
%		\begin{tabular}{@{}ccc@{}}
%%		\begine{tabular}{|l|p{80mm}|l}
%			\toprule
%			Model & Fixed Parameters & Grid Search Parameters \\ \midrule
%			
%			\multicolumn{1}{|c|}{Logistic Regression} & \multicolumn{1}{p{40mm}|}{max\_iter=\textit{100}, solver=\textit{'warn'}} & \multicolumn{1}{p{40mm}|}{penalty=\textit{l2}, class\_weight=\textit{'balanced', None(default=1)}, multi\_class=\textit{'warn'}} \\ \midrule
%			
%			\multicolumn{1}{|c|}{Random Forest} & \multicolumn{1}{p{40mm}|}{class\_weight=\textit{\textit{'balanced subsample'}}} & \multicolumn{1}{p{40mm}|}{n\_estimators=\textit{100, 200, 500, 1000}} \\ \bottomrule
%%			\multicolumn{1}{|c|}{} & \multicolumn{1}{p{40mm}|}{} & \multicolumn{1}{p{40mm}|}{} \\ \midrule
%%			
%		\end{tabular}
%	\end{center}
%\end{table}

%\begin{table}[H]
%	\caption{F1 score per classification model}
%	\label{tab:quantitative_experimentation_learning_performance}
%	\begin{center}
%		\begin{tabular}{@{}cccc@{}}
%			\toprule
%			Dataset & LR & RF & Average \\ \midrule
%			
%			\multicolumn{1}{|c|}{Iris} & \multicolumn{1}{c|}{0.96} & \multicolumn{1}{c|}{0.89} & \multicolumn{1}{c|}{0.93} \\ \midrule
%			
%			\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \midrule
%			
%			\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \bottomrule
%			
%		\end{tabular}
%	\end{center}
%\end{table}

\subsection{Training Models}
%- Brief training graph and classification report LR and iris \newline
%\begin{table}[H]
%	\caption{Iris Classification Report of Logistic Regression}
%	\label{tab:quantitative_experimentation_lr_iris_report}
%	\begin{tabular}{@{}ccccccc@{}}
%		\toprule
%		& setosa & versicolor & virginica & precision & recall & f1-score \\ \midrule
%		
%		\multicolumn{1}{|c|}{setosa} & \multicolumn{1}{c|}{10} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \midrule
%		
%		\multicolumn{1}{|c|}{versicolor} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{6} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.92} \\ \midrule
%		
%		\multicolumn{1}{|c|}{virginica} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{13} & \multicolumn{1}{c|}{0.93} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{0.96} \\ \bottomrule
%	\end{tabular}
%\end{table}

%- Brief training graph and classification report LR and ionosphere \newline

%- Brief training graph and classification report RF and iris \newline
%\begin{table}[H]
%	\caption{Iris Classification Report of Random Forest}
%	\label{tab:quantitative_experimentation_rf_iris_report}
%	\begin{tabular}{@{}ccccccc@{}}
%		\midrule
%		& setosa & versicolor & virginica & precision & recall & f1-score \\ \midrule
%		
%		\multicolumn{1}{|c|}{setosa} & \multicolumn{1}{c|}{10} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \midrule
%		
%		\multicolumn{1}{|c|}{versicolor} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0.83} & \multicolumn{1}{c|}{0.71} & \multicolumn{1}{c|}{0.77} \\ \midrule
%		
%		\multicolumn{1}{|c|}{virginica} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.92} & \multicolumn{1}{c|}{0.89} \\ \midrule
%
%	\end{tabular}
%\end{table}

%- Brief training graph and classification report RF and ionosphere \newline

\subsection{Results}
\subsubsection{Explanations Analysis}
\subsubsection{Explanations Evaluation}
\begin{table}[H]
	\caption{Aggregated Local Lipschitz Continuity}
	\label{tab:quantitative_experimentation_iris}
	\begin{center}
		\begin{tabular}{@{}ccccc@{}}
			\toprule
			& $\epsilon$ & LIME & SHAP \\ \midrule
			
			% Logistic Regression, Iris
			\multicolumn{1}{|c|}{Logistic Regression, Iris} &  \multicolumn{1}{c|}{\textit{0.75}} &  \multicolumn{1}{c|}{0.72} & \multicolumn{1}{c|}{0.33} \\ 
			\midrule
			
			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.00}} &  \multicolumn{1}{c|}{0.73} & \multicolumn{1}{c|}{0.34} \\ 
			\midrule 
			
			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.25}} & \multicolumn{1}{c|}{0.74} & \multicolumn{1}{c|}{0.35} \\ 
			\midrule 		
			
			% Random Forest, Iris
			\multicolumn{1}{|c|}{Random Forest, Iris} &  \multicolumn{1}{c|}{\textit{0.75}} &  \multicolumn{1}{c|}{0.91} & \multicolumn{1}{c|}{0.21} \\ 
			\midrule
			
			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.00}} &  \multicolumn{1}{c|}{0.94} & \multicolumn{1}{c|}{0.22} \\ 
			\midrule 
			
			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.25}} & \multicolumn{1}{c|}{0.94} & \multicolumn{1}{c|}{0.22} \\ 
			\midrule 		
		
		\end{tabular}
	\end{center}
\end{table}
%The idea of using Fidelity as evaluation property is discarded because SHAP uses Shapley values for generating explanations
%The idea of using Consistency as evaluation property is also discarded because it over-complicate the process.

\section{Discussions and Conclusion} %\Section{}
\subsection{Results Summary} %\subsection{}
\subsection{Limitations} %\subsection{}
\subsection{Future Work} %\subsection{}
\subsection{Conclusion} %\subsection{}

% \bibliographystyle{babplain-lf}
% \bibliographystyle{abbrv} % numbers
\bibliographystyle{apalike} % alphas
\bibliography{ref}

% --- Appendices ---

% uncomment the following

% \newpage
% \appendix
% 
% \section{Example appendix}

\end{document}
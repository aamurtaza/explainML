% --- Template for thesis / report with tktltiki2 class ---
% 
% last updated 2015/02/03 for tkltiki2 v1.03

\documentclass[english]{tktltiki2}

% tktltiki2 automatically loads babel, so you can simply
% give the language parameter (e.g. finnish, swedish, english, british) as
% a parameter for the class: \documentclass[finnish]{tktltiki2}.
% The information on title and abstract is generated automatically depending on
% the language, see below if you need to change any of these manually.
% 
% Class options:
% - grading                 -- Print labels for grading information on the front page.
% - disablelastpagecounter  -- Disables the automatic generation of page number information
%                              in the abstract. See also \numberofpagesinformation{} command below.
%
% The class also respects the following options of article class:
%   10pt, 11pt, 12pt, final, draft, oneside, twoside,
%   openright, openany, onecolumn, twocolumn, leqno, fleqn
%
% The default font size is 11pt. The paper size used is A4, other sizes are not supported.
%
% rubber: module pdftex

% Line spacing 
%
% Use \linespread{1.3} for "one and a half" line spacing, and
% \linespread{1.6} for "double" line spacing. Normally the lines
% are not spread, so the default line spread factor is 1.
\linespread{1.3}

% --- General packages ---

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}
\usepackage{multirow}
% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% --- Language-related settings ---
%
% these should be modified according to your language

% babelbib for non-english bibliography using bibtex
\usepackage[fixlanguage]{babelbib}
\usepackage{apalike}
\iflanguage{finnish}{\selectbiblanguage{finnish}}{}

% add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}
\iflanguage{finnish}{\settocbibname{Lähteet}}{}

% --- Theorem environment definitions ---

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

% --- tktltiki2 options ---
%
% The following commands define the information used to generate title and
% abstract pages. The following entries should be always specified:

%Interpreting "Black Box" models with Explanations Methods
\title{Interpreting "Black Box" Classifiers to Evaluate Explanations of \newline
	 Explanation Methods}
\author{Adnan Ali Murtaza}
\date{\today}
\level{Thesis}
\abstract{Interpretability in machine learning aims to provide explanations on the behaviours of complex predictive models. Numerous approaches have been proposed as explanation methods which strive to interpret black-box models. Although explaining the global behaviour of a black-box remains an unsolved challenge; these explanation methods attempt to approximate and explains the local behaviour of a model in an human-understandable way. However, the argument is how stable are the locally generated explanations because stability in explanations is a crucial element in interpretability.
	
In this work, we examine and select different types of explanations methods to interpret multiple classifiers trained on UCI benchmark and MNIST handwritten digits datasets. After that, we explore the relevant research which provides the way to evaluate the robustness of explanations. We introduce a new separability measure which goes along with stability measure in order to quantify the robustness of explanations . Our results illustrate that current explanation methods do not perform adequately with regard to separability and stability metrics. Finally, we show the way that our proposed measure can apply the robustness on existing interpretability approaches.}


% The following can be used to specify keywords and classification of the paper:

\keywords{Explanation evaluation, Interpretable models, Black-box classfiers, Interpretability in machine learning}

% classification according to ACM Computing Classification System (http://www.acm.org/about/class/)
% This is probably mostly relevant for computer scientists
% uncomment the following; contents of \classification will be printed under the abstract with a title
% "ACM Computing Classification System (CCS):"
% \classification{}

% If the automatic page number counting is not working as desired in your case,
% uncomment the following to manually set the number of pages displayed in the abstract page:
%
% \numberofpagesinformation{16 pages + 10 appendix pages}
%
% If you are not a computer scientist, you will want to uncomment the following by hand and specify
% your department, faculty and subject by hand:
%
% \faculty{Faculty of Science}
% \department{Department of Computer Science}
% \subject{Computer Science}
%
% If you are not from the University of Helsinki, then you will most likely want to set these also:
%
% \university{University of Helsinki}
% \universitylong{HELSINGIN YLIOPISTO --- HELSINGFORS UNIVERSITET --- UNIVERSITY OF HELSINKI} % displayed on the top of the abstract page
% \city{Helsinki}
%

%%%%%%%%%%%%% Added Packages
%% package hyperref
\usepackage{hyperref}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
%\usepackage{lipsum}

%% package algorithm plus algorithmic style
%\usepackage{algorithm}
%\usepackage{algorithmic}
%
% package algorithm plus algpseudocode style
\usepackage{algorithm}
%\usepackage{arevmath}     % For math symbols
\usepackage[noend]{algpseudocode}

%% package algorithm2e style
%\usepackage{xcolor}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

%% package citations
%\usepackage{cite}
\usepackage{natbib}

%% package up greek math symbols
%\usepackage[upgreek, LGRgreek]{mathastext}
%\usepackage{bm}
\usepackage{newtxtext}
\usepackage[slantedGreek]{newtxmath}

%% package for adding properties
\usepackage{enumitem}
\newlist{properties}{enumerate}{2}
%\setlist[properties]{label=\textbf{Property} \arabic*.,itemindent=*}
\setlist[properties]{label=\textbf{Property} \arabic*, wide, labelwidth=!,labelindent=0pt}

%% package for adding sub figures side by side
\usepackage{subfig}

%\usepackage{subfigure}

%% NEW COMMANDS
% Algorithm definitions
\newcommand{\commentsymbol}{//}% or \% or $\triangleright$
\newcommand{\multicommentsymbolstart}{/*}
\newcommand{\multicommentsymbolend}{*/}
\algrenewcommand\algorithmiccomment[1]{\hfill \commentsymbol{} #1}
\makeatletter
\newcommand{\LineComment}[2][\algorithmicindent]{\Statex \hspace{#1}\commentsymbol{} #2}
\newcommand{\MultiLineComment}[2][\algorithmicindent]{\Statex \hspace{#1}\multicommentsymbolstart{} #2 \multicommentsymbolend{}}
\newcommand{\onespace}{\;}
\newcommand{\twospaces}{\;\;}
% {\rm I\!R}^n = {\mathbb{R}}^n

%% Declare math alphabets or operators
\DeclareMathAlphabet{\mathsf}{OT1}{\sfdefault}{m}{n} % newtxmath has T1
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\graphicspath{{./plots/}}

\begin{document}

% --- Front matter ---

\frontmatter      % roman page numbering for front matter

\maketitle        % title page
\makeabstract     % abstract page

\tableofcontents  % table of contents

% --- Main matter ---
\mainmatter       % clear page, start arabic page numbering
\section{Introduction} %\section{One}
The machine learning is a core element in the advancement of science and technology. However, in comparison to the technological progress in the field, the social aspects and trust concerns are neglected. Today, a wide range of predictive machine learning models are being used in digital products and services. The main concern of the users remains the same as \textit{“can they trust the machine learning models and their predictions”?} It is essential to have trust and transparency on models because it impacts on users decisions and enhances services quality. However, providing a transparent service which gives an understandable explanation behind the predictions and models is a complicated task \citep{lipton2016mythos, miller2018explanation, molnarinterpretable, gilpin2018explaining, murdoch2019interpretable}. 

Such intricate models become black boxes to the users, affecting end-users trust and raising concerns about predictive models interpretability. In the recent past, researchers have attempted to define interpretability in machine learning, though it appears it requires more understanding and investigation \citep{lipton2016mythos, miller2018explanation, molnarinterpretable}. On the other side, experts who build predictive models to solve complex problems lack robust model validation, and testing approaches \citep{murdoch2019interpretable, zhang2019machine}. They mainly utilise standard accuracy measures to evaluate the models which may not indicate the product’s goal every time.  Having blind faith in such measures can not only harm a service quality but also put machine learning models under questions \citep{miller2018explanation, molnarinterpretable}.

An increasing need for making machine learning interpretable and debuggable has established a new sub-field, Explainable Machine Learning, which aim to solve related challenges \citep{adhikari2018example}. Various model-agnostic \citep{ribeiro2016should, lundberg2017unified}, and model-dependent \citep{ribeiro2016should} explanation methods are developed to explain the behaviour of predictive models. These methods strive to interpret black-box models and aim to present explanations in a human-understandable way. However, a new challenge has appeared on how to measure the quality of explanations \citep{lipton2016mythos, miller2018explanation, molnarinterpretable, murdoch2019interpretable}?

This work tackles the above question and tries to study and investigate the components which are useful in evaluating explanations. In the process, multiple explanation methods (Section~\ref{sec:methodologies}) are tested on various kind of classification models (Section~\ref{sec:interpret_radnom_forest_and_logistic_regression} and Section~\ref{sec:interpret_convolutional_neural_network}) and datasets (Section~\ref{sec:datasets_UCI} and Section~\ref{sec:dataset_MNIST}). 


\subsection{Motivation} %\subsection{}
An increasing amount of real-time intelligent services have established the user needs for trust and transparency. In simple cases such as email spam filtering, mostly users put their trust into an application. However, in practical cases, users need to be able to understand the reason behind a decision, i.e., loan rejection \citep{guidotti2018survey, lipton2016mythos, molnarinterpretable}. On the other side, services which aim to provide explanations to non-technical users need stable and consistent explanation method. Failing to provide transparency can increase users concerns, which directly impacts the users’ trust factor. Thus, there is a need to make predictive models interpretable by using methods which provide human-interpretable explanations \citep{guidotti2018survey, murdoch2019interpretable, gilpin2018explaining}. This research is motivated to explore current explanations methods to generate explanations for various classifiers and then evaluating the explanations. 

\subsection{Explanations Needs in Machine Learning} %\subsection{} Social and Ethical Concerns
Explanations are becoming crucial in machine learning, especially with the rise of data usage. Nowadays, users demand more explanations related to their data consumptions and urge businesses to put robust data practices \citep{doshi2017accountability} Moreover, growing data regulations have increased the need for transparency, and multiple industrial segments are striving to adapt to the change. Mainly, it is critical to give explanations in sectors which take users sensitive data to serve it to intelligent models such as insurance, health or banking \citep{molnarinterpretable}. 

Providing explanations is not only essential to retain the users’ trust but also decreases the social and ethical issues related to data exploitation. Moreover, \citet{lipton2016mythos} states that companies appear suspicious to users when they receive personal data consent for a particular use, but in-real, they operate opposite to users consent. For instance, annoying advertisements on Facebook based on general browsing history or rejection of loan applications by a system. Such scenarios often ignite users’ critical thinking and intrigue them to explore the reasons behind the decisions \citep{honegger2018shedding, guidotti2018survey}.

Moreover, pursuant to protection, the ethical aspect of machine learning gives a right to users to ask about an explanation regarding decisions. Additionally, it is now obligatory to businesses to provide and accommodate user when he/she has some discrimination concerns. European Union general data protection regulation (GDPR) empowers data subject in such situations and require businesses  explanations \citep{doshi2017accountability, goodman2017european}. Article 13 and 14 in GDPR asserts that a data subject has the right to ask for “meaningful information about the logic involved in algorithmic decision-making” \citep{goodman2017european}.

\subsection{Research Problem} %\subsection{}
The problem is that high accuracy machine learning models become obscure due to their complex structure. Often, these models behave as black-boxes, and it is hard for practitioners to explain them in a human-understandable way. Explanation methods aim to open such black-boxes and present explanations to a certain degree. However, generated explanations are often lacking stability, consistency and overall faithfulness. It creates a need to measure the quality of explanations. Thus, a research problem can be defined as:

\textbf{\textit{Research Problem:}}
\textit{How to measure the quality the explanations to compare explanations methods?}

As the above problem statement indicates, the primary need is to have a quality measure which helps to make the comparison among explanations and explanations methods. In the past, mostly a questionnaire approach has been used to do qualitative analysis. However, it is expensive and time-consuming, which always require a human-based review and interaction \citep{molnarinterpretable, honegger2018shedding, ribeiro2016should, lundberg2017unified}.

\subsubsection{Research Questions} %\subsubsection{}
An above research problem comprises multiple elements, which can be divied into the following research questions. The primary need is to have a robust framework which evaluates the quality of generated explanations. Thus, the main research question which encapsulates the research challenge can be derived as:

\textbf{\textit{Research Question:}}
\textit{What is a robust framework which evaluates the quality of generated explanations?} % How to design

It is an active cross-domain research challenge which first requires a consensus on what a good explanation means?  The research in social sciences and human behavioural studies give some directions which help to understand the meaning of good explanations \citep{lipton2016mythos, miller2018explanation}. However, the ability to understand the explanations also rely on external factors such as education and age. In this work, we aim to tackle the above research question and further split it into the following research questions (RQs). These research questions frame our approach to explore and investigate current research.

\textbf{\textit{Research Question 1:}}
\textit{In literature, what does it mean to have interpretability in machine learning?}

The first question presents the idea to understand the meaning of having interpretability in machine learning. It mainly consisted of the components to identify the relevant literature by exploring scientific journals, surveys, dissertations, books, and papers. Moreover, narrowing down the research problem by reviewing, gathering, and understanding the related material.

\textbf{\textit{Research Question 2:}}
\textit{In literature, what are the current approaches to generate the explanations for classifiers?}

The second question requires a more systemic review for the material gathered in the first phase. It helps to highlight the research gaps and identify the current approaches for generating explanations. Furthermore, it intends to recognise the various types of explanations methods and understand their inner working with respect to interpret black-box models.

\textbf{\textit{Research Question 3:}}
\textit{In literature, what are the existing frameworks to measure the robustness of generated explanations?}

The third question strives to find and review the existing methodologies, which validate the explanation methods. Primarily, it tries to review and verify that what are the existing ways and framework which can help to measure the robustness of generated explanations. Overall, it analyses and determines the research approach, which further can enhance the evaluation metric for generated explanations.

\textbf{\textit{Research Question 4:}}
\textit{How to design or enhance an existing framework which measures the robustness of generated explanations?}

The fourth question defines the robustness measure, which can contribute to the existing explanations evaluation criteria. It explains the most relevant parts and formalises the research approach to derive the experiments. It also verifies the execution plan for multiple experimental settings and outcomes a plan to run the experiments.


\textbf{\textit{Research Question 5:}}
\textit{How to apply the evaluation framework which measures the robustness of generated explanations for classification cases?}

The last question covers running multiple experiment pipelines and applying evaluation framework. It produces the results and summarises the findings. Moreover, it identifies the limitations, future work conclusions. It is essential to mention that these five questions or phases are iterative, which allows fine-tuning of a research problem, experiments and results. 


%\begin{enumerate}
%	\item \textit{\textbf{Research Question}: In literature, what does it mean to have interpretability in machine learning?}
%	\item \textit{\textbf{Research Question}: In literature, what are the current approaches to generate the explanations for classifiers?}
%	\item \textit{\textbf{Research Question}: In literature, what are the existing frameworks to measure the robustness of generated explanations?}
%	\item \textit{\textbf{Research Question}: How to contribute to an existing framework which measures the robustness of generated explanations?}
%\end{enumerate}
%
%These research questions frame our approach to explore and investigate the current research. The idea is first to understand the meaning of having interpretability in machine learning.  and then examine how our research can contribute to it. 

%\subsubsection{Research Methodology} %\subsection{}
%
%\begin{itemize}
%	\item \textit{Write about research methodology you have used to conduct the research.}
%	\item \textit{Write the usefulness of selected methodology.}
%	\item \textit{Formulate the main steps of research methodology.}
%	\item \textit{- Design the diagram to show concept.}
%\end{itemize}


\subsection{Thesis Outline} %\subsection{}
The thesis outline is as; Section~\ref{sec:interpretability_in_ML} gives an overview of interpretable machine learning and explains the types of interpretable and explanation methods. Moreover, it specifies the certain properties which help to evaluate the explanations. Section~\ref{sec:methodologies} explains the selected methodologies for this research and defines the approach of evaluating generated explanations by explanation methods. Section~\ref{sec:experiments_and_evaluation} presents our experimental setup and shows our experiments and results. In the end, Section~\ref{sec:conclusion} summarises the results, discusses limitations and future work, and conclusion.


\section{Interpretability in Machine Learning}\label{sec:interpretability_in_ML} 
This section presents the recent research which has contributed to defining the notion of interpretability in the context of machine learning. Interpretability plays a vital role in many aspects of machine learning, e.g., fairness, decision-making, model validation, model testing, and implementing data usage regulation \citep{guidotti2018survey, zhang2019machine}. In the recent past, it has received considerable attention, particularly for the topics related to bias and fairness in machine learning models \citep{cathy2017weapons, molnarinterpretable}.

In general, interpretability means to present (information of some form) in understandable terms. It is a broad term that often depends on multiple factors such as a domain, audience and application; therefore, it is hard to define it \citep{ruping2006learning, kim2016examples, miller2018explanation, molnarinterpretable, murdoch2019interpretable}. Likewise, in the context of machine learning, interpretability lacks a widely agreed mathematical definition, though researchers have tried to define it in a non-mathematical way \citep{molnarinterpretable}. \citet{kim2016examples} defines it as \textit{“Interpretability is the degree to which a human can consistently predict the model’s result.”}  On the other side, \citet{miller2018explanation} defines it as \textit{“Interpretability is the degree to which a human can understand the cause of a decision.”} Moreover, \citet{murdoch2019interpretable} states that interpretability means to extract relevant knowledge of domain relationships contained in data while understanding the machine learning model behaviour. 

All of the above definitions point to the need for understanding the machine learning model better. In light of this, researchers further divided the interpretability into sub-categories. \citet{lipton2016mythos} establishes the interpretability by dividing it into two categories: transparency and post-hoc explanations. The first covers \textit{“how the model works?”}, and the latter \textit{“how much more model can tell about the predictions?”}. On the other hand, \citet{molnarinterpretable} categorises the interpretability into three types: algorithm interpretability, global model interpretability, and local model interpretability. The algorithm interpretability precisely tackles the question of \textit{“how does the algorithm create the model?”}. Global interpretability gives an overall perspective for a model and attempts to answer \textit{“how does the trained model make predictions?”}. Local interpretability tries to explain the singular predictions, and mainly addresses the question \textit{“why did the model make a specific prediction?”}.

Similarly, \citet{murdoch2019interpretable} groups the interpretable machine learning generally into two kinds: intrinsic interpretability and post-hoc interpretability. Intrinsic interpretability is for self-explanatory models which inherently owns the interpretable structures such as a decision tree, rule-based model, and linear models. In contrast, the post-hoc interpretability requires developing a secondary model to provide explanations for an existing model. Moreover, \citet{murdoch2019interpretable} states that these two main categories incorporate further global and local interpretability. Global interpretability interprets the inner working of machine learning models and increases their transparency. In contrast,  local interpretability helps to reveal the causal relations between a specific instance and its corresponding model prediction.

Now, with an above overview of interpretability in machine learning, we move forward to the sub-sections. Section~\ref{sec:interpretable_models} presents a detailed explanation of interpretable models who intrinsically have interpretable structures. Section~\ref{sec:explanation_methods} gives detailed descriptions on explanation methods which incorporates interpretable models to mimic the machine learning model behaviour. Section~\ref{sec:evaluation_of_explanation_methods} explains the properties of explanations which help to design and evaluate explanation methods.


\subsection{Interpretable Models}\label{sec:interpretable_models} %\subsection{}

Interpretable models explain a machine learning model predictions. A desideratum which lists the main elements of interpretable models includes Interpretability, Accuracy, and Fidelity. As highlighted earlier, interpretability means that to what extent an explanation of a prediction is human-understandable. Accuracy is to how well a model accurately predicts the unseen instances, and fidelity is to how accurately a model can imitate a black-box predictor \citep{molnarinterpretable, guidotti2018survey}. 

In literature, one of the significant components to measure the interpretability is the complexity of a predictive model such as the model size. It is shown that adding constraints to a predictive model can improve the interpretability, for example, imposing sparsity terms or enforcing semantic monotonicity constraints in classification models \citep{murdoch2019interpretable}. The sparsity terms encourage to drop features which are not relevant for predictions. In contrast, monocity forces the features to have monotonic relations with the prediction. On the other side, components to measure accuracy and fidelity are mainly in terms of scores such as accuracy score or F1-score \citep{molnarinterpretable, guidotti2018survey, murdoch2019interpretable}.

The most recognised interpretable modes are linear models, decision trees, and rules \citep{molnarinterpretable, guidotti2018survey, murdoch2019interpretable}. Furthermore, their extensions such as generalised linear models GLMs, tree-ensemble methods can also serve as interpretable models \citep{molnarinterpretable, murdoch2019interpretable}. The fundamental property of these models is their simplified nature which makes them easily understandable and interpretable for humans. Very often, the constraints play an essential role to keep intact the simplicity in a model and eventually, the crucial part is to manage the trade-off between prediction accuracy and interpretability.

%[27,36,84]. Furthermore, their extensions such as generalised linear models

\subsubsection{Linear Models}
Linear models intrinsically can provide explanations, and the simple way is to analyse and visualise the feature contributions for a given prediction \citep{molnarinterpretable, guidotti2018survey}. The sign and magnitude generally be considered the primary elements in the analyses. It a feature or attribute value has a positive contribution, then it increases the model’s output. In contrast, if the contribution of an attribute value is negative, then it decreases the output of a model. Moreover, the magnitude indicates the influence of a particular feature or attribute on a prediction. If a specific attribute value has a higher contribution as compared to others, then it shows a strong influence on a model’s prediction. 

%[47,84]. The sign and magnitude

The analyses of feature contributions in linear models also help to quantify the performance of a model. The overall feature contributions summarise the difference between prediction produced by a model and an expected prediction. It makes this possible to quantify the changes in model prediction for a particular testing instance — for instance, how each attribute contributes to the change in the prediction of a model \citep{molnarinterpretable, honegger2018shedding}. A series of generalised linear models GLM which include linear regression and logistic regression appeared the most considerable linear interpretable models in analysing such feature contributions \citep{murdoch2019interpretable}. 

Generalised linear models GLM have a linear combination of input features and model parameters which often are fed to some non-linear transformation function \citep{murdoch2019interpretable}. One of the advantage GLM has that their weight parameters direct exhibit the feature contributions or importances. It helps users to visualise them instantly and understand how a model works. However, the downside is that those weight parameters may not reflect the real importance when features are not normalised on the same scale. It makes it crucial to normalise the scales of the features and remove the variations appropriately. Besides, the feature dimensions add a critical factor to maintain the interpretability of an explanation. The large feature dimensions decrease the interpretability and make an explanation less comprehensible to users \citep{molnarinterpretable,miller2018explanation, murdoch2019interpretable}.

\subsubsection{Tree-based Models}
Tree-based models can also provide explanations for predictive models \citep{guidotti2018survey,molnarinterpretable,murdoch2019interpretable}. Inherently, tree models own selective nature that helps to prioritise the essential features based on the highest information gain \citep{hastie09elementsofstatisticallearning}. It leads to less consumption of the available data and boosts performance as well. However, the disadvantage also hides in the inherent tree structure: if a tree model has too many child leaves and long paths, then it loses the comprehensibility thus becoming a black-box model that is hard to explain.

One of the fundamental tree models is a decision tree model that exploits the graph-structure like a tree \citep{guidotti2018survey, molnarinterpretable}. The internal nodes in a decision tree represent tests on attributes or features, e.g., if a variable has a value which is lower or higher or equals to a threshold. The leaf node indicates the class label where each branch symbolises a possible outcome. The overall paths from the root node to the leaves represent the classification rule. It also makes decision tree models capable of being linearised into a set of decision rules using if-then clauses 

%symbolises a possible outcome [79].
%[77,78,26].

Generally, the decision trees which are long and deep are more interpretable than broad and more balanced trees. These constraints make a decision tree model simpler and increase the comprehensibility of model \citep{guidotti2018survey, murdoch2019interpretable}. Though, the partition scheme for a feature space mainly depends on the selected variation of a decision tree. The most commonly-used decision tree approaches in classification and regression cases are (CART), and the C4.5 and its extension the C5.0 \citep{hastie09elementsofstatisticallearning, molnarinterpretable}.

%decision tree [29].

Contrary to simple decision tree models, tree-based ensemble models such as XGBoost, random forests, and gradient boosting machines are obscure to humans \citep{murdoch2019interpretable}. These incomprehensible models can become interpretable by simplifying the structure or using ways to measure the contributions of features. \citet{murdoch2019interpretable} highlighted three main approaches to calculate the feature contributions. The first one calculates the accuracy gain by adds a new branch for a feature. The reason is that a new split to a branch for a feature avoids adding misclassified elements into calculations. The second approach uses feature coverage which calculates the relative quantity for feature observations. The third one counts the number of times a feature is used to split the data. All these approaches, when trying to make inscrutable models interpretable, may cost in prediction accuracy.

% [7] \citep{murdoch2019interpretable}. These incomprehensive models can become


\subsection{Explanation Methods}\label{sec:explanation_methods} 
Explanations methods help to interpret machine learning models which become obscure to their complicated structure such as deep neural networks \citet{bibal2016interpretability, miller2018explanation, honegger2018shedding, molnarinterpretable, guidotti2018survey, murdoch2019interpretable}. These methods generate instance-level explanations to establish post-hoc local interpretability or explanations \citet{honegger2018shedding, murdoch2019interpretable}. Post-hoc local explanation investigates the local behaviour of a model and identifies the feature contributions towards a specific model prediction. The methods which usually explore the local model’s predictions attribute to input’s features thus called attribution methods. Moreover, to develop a global-level understanding of a model, explanation methods aim to build post-hoc global interpretability or explanations. Post-hoc global explanation explains the pre-trained model knowledge in terms of parameters or learned representations that are understandable to humans\citet{murdoch2019interpretable}.

Generally, explanation methods are mainly of two categories: model-agnostic explanations methods and model-dependent explanation methods \citet{bibal2016interpretability, honegger2018shedding, molnarinterpretable}. Theoretically, model-agnostic explanation methods aim to explain any machine learning model. In contrast, model-dependent explanation methods built for models which are complicated in their hidden structure such as deep learning models. In literature, a third category also appears, called model-specific methods \citet{molnarinterpretable, murdoch2019interpretable}. Model-specific methods are interpretable models which inherently own interpretable structure, as described in the previous section. Following sub-sections give an overview of underlying approaches which form the bases of explanation methods.


\subsubsection{Model-Agnostic} %\subsubsection{} 
Model-agnostic methods interpret predictions of a machine learning model without accessing its internal parameters. Generally, agnostic methods are portable and can be applied to any arbitrary machine learning model \citep{ribeiro2016model, bibal2016interpretability, molnarinterpretable}. They generate explanation by treating the models as black-boxes and without investigating the internal model parameters. However, it brings some risks because of the generated explanation cannot guarantee faithfulness to a machine learning model \citep{bibal2016interpretability, guidotti2018survey, miller2018explanation}. Mainly, there are three approaches broadly adapted in agnostic methods which produce interpretations for black-boxes: permutation feature importance, local approximation based explanation, and perturbation based explanation \citep{molnarinterpretable, murdoch2019interpretable}.

Permutation feature importance approach is widely applicable to various machine learning models in several kinds of applications \citep{molnarinterpretable, guidotti2018survey, murdoch2019interpretable}. It estimates the importance of a specific feature on the overall performance of a model. The idea is that applying some permutation to the specific feature and re-calculating the prediction accuracy tells that how much a permutation effects the baseline accuracy. In such a way, permutations are applied for each feature to achieve the feature importances scores, which further be ranked to extract the most important features. The main advantage of such an approach is that it is scalable to almost any machine learning models. Moreover, it is shown that the approach has robust implementations and it does not necessarily require normalising the feature values.

In contrast, local approximation based explanation aims to explain the local behaviour of a machine learning model \citep{ribeiro2016model, guidotti2018survey,molnarinterpretable, murdoch2019interpretable}. It uses an assumption that given a small neighbourhood near the original input, an interpretable white-box model can approximate the machine learning prediction. The main idea is that a sparse linear interpretable model such as Lasso regression mimics the behaviour of a black-box model in the local input space. The dataset mainly contains the generated samples and labels assigned by the black-box model. Once the linear model fits in the sampled dataset, the features contributions are extracted by analysing the internal parameters. Such approximation depicts the black-box model locally and does not provide a global-level explanation and understanding.

Perturbation based explanation follows the approach of calculating the feature contributions by measuring the change in a prediction score when a feature is perturbed or altered \citep{guidotti2018survey, molnarinterpretable, murdoch2019interpretable}. The critical concept is that which parts of the input impacts the most change in predictions. The perturbation is applied for all the features sequentially to analyse the change and extract the contributions. Generally, there are two types of ways to apply perturbation: omission and occlusion. Omission directly removes the feature from the input to estimate the impact on the predictions. Contrarily, occlusion replaced the feature with some reference value such as zero for word embeddings or specific gray value for image pixels \citep{ancona2017towards, murdoch2019interpretable}.

\subsubsection{Model-Dependent} %\subsubsection{}
% missing citation
An alternative to model-agnostic methods is model-dependent explanation methods. These methods are specifically designed for the specific type of models such as deep learning models. Neural networks learn concepts and features in multiple hidden layers, and specially designed model-dependent methods help to uncover such concepts more accurately. Moreover, gradient-based methods are computationally more efficient than model-agnostic methods. Majorly, there are two categories in model-dependent methods: back-propagation based methods and perturbation based methods. However, model-agnostic methods such as partial dependence plots or local models can be utilised to interpret deep learning models as well \citep{guidotti2018survey, molnarinterpretable, murdoch2019interpretable}.

%Nonetheless, there are particular reasons which make model-dependent methods more useful in interpreting neural networks. Neural networks learn concepts and features in multiple hidden layers, and specially designed methods help to uncover such concepts more accurately. Moreover, gradient-based methods are computationally more efficient than model-agnostic methods. Majorly, there are two categories in model-dependent methods: back-propagation based methods and perturbation based methods \citep{,molnarinterpretable, murdoch2019interpretable}.


% missing citation
%Visualising image classification models and saliency maps 2014
%Striving for simplicity: The all convolutional net 2015
%On pixel-wise explanations for non-linear classier decisions by layer-wise relevance 2015

Back-propagation methods estimate the feature contributions by calculating the gradients of a particular output with respect to the input. A straightforward form of computing gradient is using back-propagation, where the gradient magnitude represents the relevance of feature to a particular prediction or output \citep{guidotti2018survey, ancona2017towards, murdoch2019interpretable}. The large magnitude of a gradient indicates a substantial relevance, whereas a small magnitude represents a low relevance. There are other ways of calculating gradients as well, such as dropping negative gradient values while back-propagation or calculating the relevance of the final prediction score while back-propagating to the input layer. On the other side, there are some limitations to back-propagation methods such as these methods are limited in applying heuristic approaches and may produce noisy and unsatisfactory explanations with some irrelevant features \citep{murdoch2019interpretable}.

% missing citation
%Interpretable explanations of black boxes by meaningful perturbation 2017
%Real time image saliency for black box classifiers 2017

Perturbation based explanations described in the previous section get very expensive computational when the input is high dimensional \citep{molnarinterpretable}. In contrast, model-dependent methods use mask perturbation and gradient descent optimisation for neural network architectures. Such methods develop and train a deep neural network which learns to predict the feature attribution mask \citep{molnarinterpretable, ancona2017towards}. It improves the computational efficiency because this requires a single forward pass to yield attribution scores for an input.

\subsection{Evaluation of Explanations}\label{sec:evaluation_of_explanation_methods}\label{sec:evaluation_of_explanations} %\subsection{}

% missing citation

Evaluation of explanations generated by explanation methods is a well-known problem in the research. Since there is no consensus about what interpretability means in machine learning, it is hard to measure it in various kind of domains and scenarios \citep{molnarinterpretable}. However, there have been attempts to formulate some approaches for evaluating the explanations. \citet{bibal2016interpretability} propose that there are three primary levels of evaluating the interpretability: application-level, human-level, and functional-level. Application-level evaluation means that evaluating the explanation by the end-users through embedding it into the real product or service. It requires a real experimental setting where the end-user would be testing the quality of an explanation. In this case, the assessing criteria can be that how much a generated explanation deviates from the baseline explanation produced by a human.

% missing citation
On the other side, human-level evaluation is a simplified version of an application-level assessment. The critical difference is that in human-level assessment, explanations are assessed by the consumers who are not domain-expert. It helps to reduce the experimental costs, and different interpretations are easily tested by showing it to various groups to choose the best. In contrast, the functional-level evaluation does not need any end-users. The key idea is to add some proxy or constraint to enhance the explanation quality, such as restricting the depth for a decision tree model. It establishes the interpretability, but often it comes with a cost of loss in the accuracy of a predictive model \citep{bibal2016interpretability, molnarinterpretable}.

\subsubsection{Properties of Explanations} %\subsection{}
% missing citation
%\citet{robnik2018perturbation, molnarinterpretable}

Moreover, in literature, researchers have also attempted to define some properties for evaluating the explanations. However, those properties are described in a non-mathematical way, which makes it unclear and open how to measure them \citep{robnik2018perturbation}. The most highlighted properties are accuracy, fidelity, consistency, and stability \citep{molnarinterpretable}. 

% missing citation
Accuracy strives to cover how well an explanation predict unseen data. It becomes essential when explanation method has to predict in place of the machine learning model. However, if a machine learning model already has low prediction accuracy, then having high accuracy for explanation method is not necessary. In this scenario, the explanation serves the purpose of explaining the unaccurate behaviour of a black-box model. Fidelity plays a vital part in such a situation \citep{molnarinterpretable}.

% missing citation
Fidelity tries to measure how well an explanation method approximates prediction of a black-box model. Fidelity is considered to be one of the fundamental properties for evaluating the explanation of interpretable methods. The main reason is that a low fidelity indicates the explanation method is not useful in approximating the black-box model, making the generated explanations inadequate. Moreover, fidelity and accuracy relate to each other; and high fidelity of explanation methods may imply high accuracy. Mostly, explanation methods offer local fidelity due to their limitation to provide global-level understanding for a black-box model \citep{molnarinterpretable}.

% missing citation
Consistency estimates how much an explanation differs between machine learning models that are trained on a similar dataset and give the same predictions. It is only applicable and desirable when different models share a comparable and relative structure and use similar features while making predictions. Otherwise it not helpful, and this phenomenon is also called “Rashomon Effect” \citep{molnarinterpretable}.

% missing citation
Stability appears to be one of the most desirable properties in explanations. It measures how similar are the explanations for similar instances. High stability indicates that small changes in features which do not change the prediction, should not substantially change the explanation. The causes of low stability are mostly the components in the explanations methods which are non-deterministic such as sampling of data in local surrogate methods \citep{molnarinterpretable}.

Above described properties offer pointers to better define the evaluation metrics for existing explanation methods. An evaluation metrics are essential to determine the goodness of explanations, which ultimately brings the confidence and trust of the end-users.


%\subsubsection{Evaluating Explanations Methods} %\subsection{}
%%\textit{Sub-section Scope.}
%%\begin{itemize}
%%	\item \textit{Highlight how research has been doing work on evaluating explanations}
%%	\item \textit{Write explanation evaluation schemes present in literature}
%%	\item \textit{Explain their mechanism, pros and cons.}
%%	\item \textit{- Add Table to show the survey (RQ2 Review Literature)}
%%\end{itemize}


%\citet{bibal2016interpretability, miller2018explanation, honegger2018shedding, molnarinterpretable, guidotti2018survey, murdoch2019interpretable}

\section{Methodologies}\label{sec:methodologies} %\section{Three}
This section explains the methodologies which are carried from research to measure explanations robustness. Explanations are generated using methods; Local Interpretable Model-Agnostic Explanations (LIME) \citep{ribeiro2016should}, Shapley Additive Explanations \citep{lundberg2017unified}, and DeepExplain framework \citep{ancona2017towards}. To measure the robustness of explanations, a methodology of local Lipschitz Continuity introduced in \citet{alvarez2018robustness} is implemented and tested out on UCI classification datasets and MNIST digits dataset. Moreover, an extension of a local Lipschitz estimation which is proposed in section~\ref{sec:lipschitz_estimation_as_a_separability_measure} also tested out on the similar datasets.

On a high-level, the steps work as follow. 1) Predictive models are trained to perform classification tasks; these models behave as black-box models in the process. 2) Explanation methods interpret black-box models to find the reasons behind the predictions. 3) Explanations robustness is measured using local Lipschitz estimation technique.

Let $f_c: X \to y$ as a black-box classifier who classifies $x \in X; X = {\mathbb{R}}^d$ to class $y, y \in \{c_1, \dots, c_n\}$. Now, let $z \in X$ be an instance from input space which requires an explanation for a prediction $f_c(z) = c_z$. An explanation model $f_e(z)$ outputs an explanation feature vector $ e = {\mathbb{R}}^d$ for which the estimation of robustness or stability is required. The main objective of this section is to define the notion of stability using local Lipschitz esitmation which requires a neighborhood $N_{\epsilon}(z)$ around an instance $z \in X$. Verbally, the notion of stability is the estimation of \textit{"how similar are the explanations for similar instances"} and vice versa indicates the notion of unstabiliy.

A detailed explanation for agnostic explanation methods (LIME and SHAP) is presented in Section~\ref{sec:LIME} and Section~\ref{sec:SHAP}, respectively. Section~\ref{sec:DeepExplain} describes the unified framework for model-dependent methods. Section~\ref{sec:defining_similarity} defines similarity measure and Section~\ref{sec:defining_explanation_evaluation_framework} formalizes the explanation evaluaton framework which defines the robustness measures and presents the algorithmic design used in the implementation.

%\subsection{Predictive Models}%\subsection{}
%%\textit{Sub-section Scope.}
%%\begin{itemize}
%%	\item \textit{Explain about selected training methods}
%%\end{itemize}
%
%\subsubsection{Logistic Regression} %\subsubsection{}
%%\textit{Sub-section Scope.}
%%\begin{itemize}
%%	\item \textit{Explain what is logistic regression}
%%	\item \textit{Explain how it works, learn and predict}
%%\end{itemize}
%
%\subsubsection{Random Forest} %\subsubsection{}
%%\textit{Sub-section Scope.}
%%\begin{itemize}
%%	\item \textit{Explain what is random forest}
%%	\item \textit{Explain how it works, learn and predict}
%%\end{itemize}

%\subsection{Model-Dependent Explanation Methods} %\subsection{}
%\subsection{Model-Agnostic Explanation Methods}  %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain about selected explanation methods}
%\end{itemize}

%\subsubsection{Local Surrogate (LIME)} %\subsubsection{}
\subsection{ LIME (Local Interpretable Model-Agnostic Explanations)}\label{sec:LIME} %\subsection{}
LIME (Local Interpretable Model-agnostic Explanations) is an explanation method which approximates the explanations for individual predictions of any classifier or regressor \citep{ribeiro2016should}. The motivation behind LIME is simple; imagine there is a black box model which is trained on some data and gives predictions for new data points. LIME tries to explain a specific prediction of the black-box model by fitting the interpretable model locally. It trains the local interpretable model in a selected local vicinity around a point of interest. A new training dataset which contains permuted samples and corresponding predictions by the black-box model is used to train and evaluate the local interpretable model. 

\subsubsection{LIME explanation mechanism} %\subsubsection{}
The generated explanations can be textual or visual artefacts that show the relationship between an instance's features and the model's prediction. A critical argument here is what if an explanation is not intelligible, therefore it is essential to make it faithful and understandable. Figure~\ref{fig:ribeiro2016_lime_figure1} illustrates the case where a doctor can utilise the model explanation to make a decision. Here, an explanation is a small set of symptoms with some weights associated with them; green indicates positive contribution where red shows that the symptom is contributing negatively. It helps in the decision-making where a domain expert can accept or reject the explanation based on his or her prior knowledge.

\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{ribeiro2016_lime_figure1}
	\vspace*{-10mm}
	\caption{LIME \citep{ribeiro2016should} Depicts that an explanation behind the model prediction can help a doctor to make an informed decision. LIME presents the symptoms which led the model to predict that a patient has flu. As portrayed "sneeze" and "headache" contributed positively, while "no fatigue" is against the "flu" prediction. }
	\label{fig:ribeiro2016_lime_figure1}
\end{figure}
 
Local Interpretable Model-Agnostic Explanations \citep{ribeiro2016should} explainer focuses on mainly two essential criteria while generating explanations. The first one is that any explanation should be \textbf{interpretable}, i.e., the relationship between input variables and the output prediction should be easily understandable by users. Here, defining what is interpretable varies based on the target audience, e.g., users can be layman or machine learning experts. However, the bottom line is, an explanation should be simple enough to comprehend. Regardless of the hundreds or thousands of input variables used in the black-box model, there should be a mechanism to control the complexity of explanation.

The second one is a \textbf{local fidelity} which means that local interpretable model should be faithful in the locality around a point of prediction being explained. Though local faithfulness may not show features which are important at the global level, several explanations can be shown to give a global perspective. It is due to that LIME does not generate a faithful global explanation, which remains a complex problem to solve. Now keeping these two criteria LIME \citep{ribeiro2016should} provides a mechanism to produce an explanation which is interpretable and locally faithful using interpretable model.

\subsubsection{LIME explanation framework} %\subsubsection{}
Let $g \in G$ is an explanation model where $G$ is a class of \textit{interpretable models} such as linear models or decision trees. As explanation should be simple enough to understand so the domain of  $g$ is $\{0, 1\}^{d^\prime}$, which indicates the absence/presence of the component in its interpretable representation. Note that the original representation of an instance being explained is $x \in \mathbb{R}^d$ but to make an explanation interpretable, a binary vector representation $x^\prime \in \{0, 1\}^{d^\prime}$ is used as an interpretable representation. Moreover, $\upOmega(g)$ is used as a measure to control the complexity to ensure the \textit{interpretability} of an explanation model $g \in G$. For instance, in the case of linear models, the complexity factor can be the number of non-zero weights, whereas, it may be the depth of the tree for decision trees.

Now, let $f: \mathbb{R}^d \mapsto \mathbb{R}$ denotes a model being explained and $f(x)$ is a probability function which tells that $x$ belongs to certain class. To explain the prediction locally, $\pi_x(z)$ is used as a proximity measure between an instance $z$ and $x$ to define locality around $x$. In the original work of \citet{ribeiro2016should} $\pi_x(z)$ is set to a exponential kernel, $(\exp{-D(x,z)^2/\sigma^2})$ defined on some distance measure $D$ with width $\sigma$. Finally, the explanation can be produced using Equation~\eqref{eq:lime_generate_explanaton} where $\mathcal{L}(f, g, \pi_x)$ is a measure which tells that how unfaithful $g$ is in approximating $f$ in the locality defined by $\pi_x$. The goal is to minimize the $\mathcal{L}(f, g, \pi_x)$ while keeping $\upOmega(g)$ small enough to produce understandable explanation.

\begin{equation}\label{eq:lime_generate_explanaton}
\mathsf{\upxi}(x) = \argmin_{g \in G} \mathcal{L}(f, g, \pi_x) + \upOmega(g)
\end{equation}

LIME \citep{ribeiro2016should} only implements a class of linear models $G$ as interpretable models $g(z^\prime)=w_g.z^\prime$. Equation~\eqref{eq:lime_measure_unfaithfulness} shows the quantity to be minimised. Let $\mathcal{Z}$ be a dataset used to build linear model which contains sample instances $z^\prime \in \{0, 1\}^{d^\prime}$ drawn uniformly at random from nonzero elements of $x^\prime$, weighted by $\pi_x$. The labels to these sample instances are assigned by using $f(z)$, it requires first recovering $z \in \mathbb{R}^d$ to original representation from interpretable representation  $z^\prime \in \{0, 1\}^{d^\prime}$. 

\begin{equation}\label{eq:lime_measure_unfaithfulness}
\mathcal{L}(f, g, \pi_x) = \sum_{z, z^\prime \in Z} \pi_x(z)\onespace(f(z) - g(z^\prime))^2
\end{equation}

%\begin{wrapfigure}{r}{0.6\textwidth}
%	\centering
%	\vspace*{-5mm}
%	\includegraphics[width=1.0\linewidth]{ribeiro2016_lime_figure2}
%	\vspace*{-10mm}
%	\caption{Intuitive example to present an idea for LIME  \citep{ribeiro2016should}. A complex decision function f is represented as pink/blue background, which is hard to approximate using a linear model. The instance being explained is the bold red cross, surrounded by sampled instances (portrayed here by size). A function f gets predictions for all sample instances and weighs them by the proximity to the instance of interest. A dashed line is a linear model, provides a locally faithful explanation. }
%	\label{fig:ribeiro2016_lime_figure2}
%\end{wrapfigure}

\begin{figure}[H]
	\centering
	\vspace*{-2mm}
	\includegraphics[width=1.0\linewidth]{ribeiro2016_lime_figure2}
	\vspace*{-10mm}
	\caption{Intuitive example to present an idea for LIME from \citet{ribeiro2016should}. A complex decision function $f$ is represented as pink/blue background, which is hard to approximate using a linear model. The instance being explained is the bold red cross, surrounded by sampled instances (portrayed here by size). A function $f$ gets predictions for all sample instances and weighs them by the proximity to the instance of interest. A dashed line is a linear model, provides a locally faithful explanation. }
	\label{fig:ribeiro2016_lime_figure2}
\end{figure}

Figure \ref{fig:ribeiro2016_lime_figure2} shows intuition that how LIME \citep{ribeiro2016should} optimizes \eqref{eq:lime_generate_explanaton}. The figure depicts that instances are sampled in the vicinity of $x$ (bold red cross), the instances which are near to $x$ have more weights as compared to the ones which are far from $x$. Given this locality, a linear interpretable model provides a locally faithful explanation. Though it may not explain the complex black-box model globally, it captures explanation fairly well within the vicinity of the point of interest $x$. 
%\newline

%\subsubsection{Shortcomings of LIME} %\subsubsection{}

%G$ class of interpretable models. \newline
%$g \mapsto \{0, 1\}^{d^\prime}$ \newline
%$\mathbb{R}^d$ is a original representation. \newline
%$\{0, 1\}^{d^\prime}$ is a binary vector for interpretable representation. \newline
%$\upOmega(g)$ measure complexity of explanation (as opposed to interpretability). \newline

%$f: \mathbb{R}^d \mapsto \mathbb{R}$ denotes model being explained. \newline
%$f(x)$ is a probability function indicates that $x$ belongs to certain class. \newline
%$\pi_x(z)$ is proximity measure between an instance $z$ to $x$ to define locality around $x$. \newline
%$\pi_x(z)$ = $\exp{-D(x,z)^2/\sigma^2}$ \newline
%$\mathcal{L}(f, g, \pi_x)$ be a measure that how unfaithful $g$ is in approximating $f$ in the locality defined by $\pi_x$. \newline

%\subsubsection{Shapley Values (SHAP)} %\subsubsection{}
\subsection{SHAP (SHapley Additive exPlanation)}\label{sec:SHAP} %\subsection{}
%It works based on SHapley values (Wikipedia) -- a method in the cooperative game theory which tells how much each player has contributed in the total payout. On the high-level, SHAP treats each feature value as a player and total "payout" as an actual prediction of an instance minus the average prediction for all instances. In order to build the explanation for the framework, it is necessary first to present a brief overview of SHapley values. 

In contrast to LIME (Section~\ref{sec:LIME}), SHAP (SHapley Additive exPlanation) is a unified framework to interpret and explain individual predictions \citep{lundberg2017unified}. It works on the concepts of Shapley value \citep{shapley1953value} -- a method in the cooperative game theory. In order to build the explanation for the framework, it is necessary first to present a brief overview of Shapley values. 

\subsubsection{Overivew of Shapley value} %\subsubsection{}
Shapley value is a solution concept in cooperative game theory that answers how much each player contributes in a coalition and receives some payout based on the contribution \citep{shapley1953value}. The core purpose of Shapley value is to find which player is important in the cooperative game environment. Taking the idea into machine learning and interpretability context, the goal is is to find which feature value plays the most in a specific prediction. Here prediction task becomes a game; feature values are players and feature contributions are payouts.

Assume a task of predicting the price of an apartment and one specific example which has feature values as \textit{"park-nearby", "cat-banned", "area-50" and "floor-2nd"} predicts the price of \$300k for an apartment. Now, suppose the average prediction of all the instances is \$310k, which lead to the difference of \$10k price between actual prediction and average prediction. SHapley value tries to explain the difference value \$10k  and how each feature value contributed to it. In this case, it repeats all the combinations of features values and predicts the price of an apartment and then get the average prediction price. The computational cost increases exponentially with the number of feature values. One way to minimise the computational cost is to use fewer samples to calculate the contribution in all possible combinations.

%\begin{wrapfigure}{r}{0.5\textwidth}
%	\centering
%	\vspace*{-2mm}
%	\includegraphics[width=1.0\linewidth]{lundberg2017_shap_figure1}
%	\vspace*{-10mm}
%	\caption{SHapley value: \citep{molnarinterpretable}: All 8 combinations require to compute the Shapley value for feature value = "cat-banned".}
%	\label{fig:lundberg2017_shap_figure1}
%\end{wrapfigure}

\begin{figure}[H]
	\centering
	\vspace*{-2mm}
	\includegraphics[width=1.0\linewidth]{lundberg2017_shap_figure1}
	\vspace*{-10mm}
	\caption{SHapley value: \citep{molnarinterpretable}: All 8 combinations require to compute the Shapley value for feature value = "cat-banned".}
	\label{fig:lundberg2017_shap_figure1}
\end{figure}

Figure~\ref{fig:lundberg2017_shap_figure1} intuitively elaborates the idea of calculating the feature contributions. It shows all the possible combinations which are required to calculate the average marginal contribution for feature value "cat-banned". Each of the combinations predicts the price with or without \textit{"cat-banned"} value. Then the difference between predicted prices gives the marginal contribution for feature value "cat-banned".  Averaging the marginal contribution would give the Shapley value for feature value "cat-banned".  Similarly, Shapley values for all other feature values can be estimated to get the complete distribution of the prediction (minus the average) among the feature values.


\subsubsection{SHAP explanation framework} %\subsubsection{}
Keeping the concepts of SHapley value \citep{shapley1953value}, SHAP \citep{lundberg2017unified} introduced a new class of additive feature attribution methods (Definition~\ref{def:SHAP_additive_class}), which unifies six different explanation methods. By applying game theory concept, SHAP guarantees that there is a unique solution to a new class which helps to measure the unified SHAP values, approximated by various methods. SHAP represents an additive feature attribution method, as a linear model of Equation~\eqref{eq:SHAP_additive_class}, which enables the connectivity of various explanation models, including LIME \citep{ribeiro2016should} within the SHAP framework.

\begin{definition}\label{def:SHAP_additive_class}{\textbf{Additive feature attribution methods} \textit{have an explanation model that is a linear
function of binary variables:}}
\end{definition}

\begin{equation}\label{eq:SHAP_additive_class}
	g(z^\prime) =  \phi_0 + \sum_{i=1}^{m} \phi_i z_i^\prime 
\end{equation}
where $g$ is an explanation model, $z^\prime \in \{0,1\}^{M}$ is a simplied feature vector where 0 indicates the absence of feature value and 1 indicates the presence. $M$ is the number of simplified input features and $\phi_i \in \mathbb{R}$ is the feature attribution for a feature $i$, the Shapley values. SHAP proposed a way to transform the underlying interpretable models into Equation~\eqref{eq:SHAP_additive_class} and then unifies explanation method who satisfies three desirable properties (given below) of Shapley values \citep{molnarinterpretable}.

The first desireable property is \textit{local accuracy}, it measures that how well an explanation method is approximating the output of function $f$ for a simplified input $x^\prime$. $x^\prime$ corresponds to a original instance being explained $x$ and $f$ is a black-box model which predicts an output for $x$.

\begin{properties} \label{prop:all_properties}
\item Local Accuracy:
\begin{align*}
	f(x) = g(x^\prime) =  \phi_0 + \sum_{i=1}^{m} \phi_i x_i^\prime 
\end{align*}
Here, an explanation model $g(x^\prime)$ matches the original model $f(x)$ when $x = h_x(x^\prime)$ where function $h_x$ transforms the simplified input $x^\prime$ to original instance $x$.
%%If you define $\phi_0 =  E_X(\hat{f}(X))$ and set all $x_i^\prime$ to 1, this is the Shapley efficiency property. Only with a different name and using the coalition vector.
%\begin{align*}
%	f(x) =  \phi_0 + \sum_{i=1}^{m} \phi_i x_i^\prime = E_X(\hat{f}(X)) + \sum_{i=1}^{m} \phi_i
%\end{align*}
The second desireable property is \textit{missingness}. It applies the constraints to features that when $x^\prime = 0$ then it should have no attributed impact.
\item Missingness:
\begin{align*}
	x_i^\prime = 0 \implies \phi_i = 0
\end{align*}
%Missingness says that a missing feature gets an attribution of zero. or even the contribution stays the same regardless of the other inputs,
The third property is \textit{consistency}. It states that if some changes in a model increase the input's contribution, it should not decrease the input's attribution. In the context of a Shapley value, it means that if a model changes increase the marginal contribution of a feature value, or even the marginal contribution remains the same (regardless of the other features), then the Shapley value of the feature should not decrease, it should also increase or stays the same.

\item Consistency: \par
Let $f_x(z_i^\prime) = f(h_x(z_i^\prime))$. $z^\prime_{\setminus i}$ indicate that $z^\prime_i = 0$. For any two models $f$ and $f^\prime$ that satisfy:
\begin{align*}
	f^\prime_x(z_i^\prime) - f^\prime_x(z^\prime_{\setminus i}) \geq f_x(z_i^\prime) - f_x(z^\prime_{\setminus i})
\end{align*}
for all inputs $z^\prime \in \{0,1\}^{M}$ if follows that $\phi_i(f^\prime, x) \geq \phi_i(f, x)$
\end{properties}

\subsection{DeepExplain Framework}\label{sec:DeepExplain}
% simonyan2013deep - Saliency 
% sundararajan2017axiomatic - Int-Input (Not included in experiments) 
% bach2015pixel - e-LRP
% zeiler2014visualizing - Occlusion
% shrikumar2017learning - DeepLIFT


DeepExplain Framework \citep{ancona2017towards} provides the unified way of comparing attribution methods which examine the flow of Deep Neural Networks (DNNs). It mainly studies gradient-based methods \citep{simonyan2013deep, shrikumar2016not, bach2015pixel, sundararajan2017axiomatic, shrikumar2017learning} and re-reformulates two of them \citep{bach2015pixel, shrikumar2017learning} in order to provide a simpler implementation. Moreover, it proposes an evaluation metric, called Sensitivity-n, which presents empirical comparisons among gradient-based and perturbation-based \citep{zeiler2014visualizing} attribution methods. 


%There are several attribution methods which calculate attributions or contributions of input features in Deep Neural Networks (DNNs).  However, they lack to provide comprehensive comparisons. It is mainly due to different styles of problem formulations, which makes them incompatible with a variety of DNN architectures. DeepExplain framework \citep{ancona2017towards} primarily studies these attribution methods and analyses how they assign attributions to input features. 

%It gives a unified framework which shows that several attribution methods \citep{simonyan2013deep, shrikumar2016not, zeiler2014visualizing, bach2015pixel, sundararajan2017axiomatic, shrikumar2017learning} strongly relate to each other. Thus, it reformulates the problem definitions and introduces a new evaluation metric, called Sensitivity-n, to perform empirical comparisons among them. In a theoretical perspective, the framework proves the condition of equivalence among various methods and then re-implements a few methods \citep{bach2015pixel, shrikumar2017learning}. 

On a high-level, the goal of attribution methods is to explain predictions of DNN by examing that which input features help to activate a target neuron. Consider a DNN that takes an input of $x = [x_i, ..., x_n] \in \mathbb{R}^n$ and outputs $S(x) = \left[S_{1}(x), ..., S_{c}(x)\right]$, where $C$ is a total number of output neurons. An attribution method tries to calculate the contribution $R^c = \left[R_{1}^c, ..., R_{n}^c\right] \in \mathbb{R}^n$, given a target neuron of interest. In a classification case, a target neuron of interest would be an output neuron associated with the correct class label. The attribution values assigned to input features together form the attribution maps, such as shown in Figure~\ref{fig:ancona2017_deepexplain_figure1}. They resemble the input shape of a given sample and display heatmaps, where red colour indicates input features which contribute to the activation of target neuron and blue colour indicates the opposite.

%DeepExplain framework mainly formulises two categories of attribution methods; 1) Perturbation-based methods and 2) backpropagation-based methods. The first one compute attributions directly by removing or altering input features.

\subsubsection{Perturbation-based Methods} %\subsubsection{}
Perturbation-based methods compute attributions directly by removing or altering input features. These methods run a forward pass for every new input obtained and then measure the difference with an original input. The domain of image classification has widely adopted this technique, particularly for interpreting Convolutional Neural Networks (CNN) \citep{zeiler2014visualizing}. It visualises the probability of a correct class as a function of grey patch position, occluding parts of an image. Moreover, the number of features strongly influence in these methods, due to which they perform slowly as the number of features grow \citep{zintgraf2017visualizing}, as shown in Figure~\ref{fig:ancona2017_deepexplain_figure1}.

\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{ancona2017_deepexplain_figure1}
	\vspace*{-8mm}
	\caption{DeepExplain \citep{ancona2017towards} shows the generated attributions which occlude parts of an input image with grey squared patches of different sizes. As shown, the patch size strongly influences the outcome and the bigger patch size focuses more on the main subject.}
	\label{fig:ancona2017_deepexplain_figure1}
\end{figure}

DeepExplain only includes \textit{occlusion-1} method from \citep{zeiler2014visualizing} and uses it as a comparison benchmark for perturbation-based methods. It replaces one feature at a time with a zero baseline and measures the effect on the target output \citep{ancona2017towards}. For instance, $S_{c}(x) - S_{c}(x_{[x_i=0]})$, where $x[x_i=v]$ is a sample $x \in R^n$  which replaces $v$ to $i-th$ component of a sample.

\subsubsection{Gradient-based Methods} %\subsubsection{}
In contrast to perturbation-based methods, gradient-based methods are faster because they hardly relate directly to the variation of output. It is because that they generate attributions for all input features in a single forward and backwards pass in a given DNN. DeepExplains primarily analyses following gradient-based methods, which are also used in our experiments \ref{sec:interpret_convolutional_neural_network}.

\begin{itemize}
	\item Grad*Input: It computes the attribution by taking the partial derivatives of the output concerning input and then multiplies the input with the result. It improves the sharpness of the attribution maps, shown in \citep{shrikumar2016not}.
	
	\item Integrated Gradient: Similar to Grad*Input, it calculates the attribution by taking the partial derivatives of the output for input. The main difference is that it takes an average gradient while the input varies along the linear path in the network. See \citet{sundararajan2017axiomatic} for mathematical definition. .
	
	\item e-LRP: This method estimates the relevance of each input feature for target neuron. It works in a backward fashion, starts from an output layer and assigns relevance of the target neuron equal to the output of neuron itself. The algorithm proceeds layer by layer from output to input layer, redistributing the prediction score in the process. Refer to \citet{bach2015pixel} for mathematical details.
	
	\item DeepLift: It computes the attributions with a backward pass over the network, similar to LRP. The attribution value assigned to a unit represents its activation in the network with respect to some baseline unit. Refer to \citet{shrikumar2017learning} for mathematical details.
	
	\item Saliency: It estimates the attributions by taking the absolute value of the partial derivative of a target output for the input features. It indicates those input features which perturbs for the target output. However, the absolute value prevents the detection of positive and negative evidence that might be present in the input \citet{simonyan2013deep}.
\end{itemize}

In comparison to Figure~\ref{fig:ancona2017_deepexplain_figure1}, Figure~\ref{fig:ancona2017_deepexplain_figure2} shows that all gradient-based methods capture the higher variance in input features while generating the attributions. 

\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{ancona2017_deepexplain_figure2}
	\vspace*{-8mm}
	\caption{DeepExplain: \citep{ancona2017towards}: Several gradient-based methods generating attributions for natural image classification and Inception V3 network architecture.}
	\label{fig:ancona2017_deepexplain_figure2}
\end{figure}


%Saliency \citep{simonyan2013deep, shrikumar2016not, bach2015pixel, sundararajan2017axiomatic, shrikumar2017learning}
%
%Grad*Input.
%
%e-LRP.
%
%DeepLift.


%perturbation-based methods and gradient-based methods.

% It tests and shows comparisons on several DNN architectures and datasets (image and text classification).

\subsection{Defining Similarity}\label{sec:defining_similarity} %\subsubsection{}
Now moving forward, the next step is to specify a similarity measure. It is necessary for establishing the explanation evaluation framework which can measure the explanations generated by LIME (Section~\ref{sec:LIME}), SHAP (Section~\ref{sec:SHAP}) and DeepExplain (Section~\ref{sec:DeepExplain}). One trivial way to define a similarity measure is utilising Euclidean distance measure. It is an ordinary straight line distance between two points in Euclidean space. For instance, in Euclidean n-space the distance between two points $x = \{x_1, x_2, ..., x_n\}$ and $y = \{y_1, y_2, ..., y_n\}$ is:

\begin{equation}\label{eq:euclidean_distance}
d(x,y) = d(y,x) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
\end{equation}

Using Euclidean distance, it is straightforward to define a small neighbourhood around any instance. However, estimating neighbourhood is challenging task itself, but Euclidean distance allows us to find similar points trivially. The Definition~\ref{def:1} and Definition~\ref{def:2} formalize the notion of defining a ball of radius $r$ centered at instance $a$ as a neighbourhood policy. The subtle difference between the definitions is that closed balls contain all "exterior" points as compared to open balls.

\begin{definition}\label{def:1}{Let $a \in {\mathbb{R}}^n$: Then the \textbf{Open Ball centered at $a$ with Radius $r > 0$} denoted $B_{r}(a)$ is defined to be a set of all points $x \in {\mathbb{R}}^n$ such that $||x-a|| < r$, that is, $\{x \in {\mathbb{R}}^n |\;||x - a|| < r\}.$}
\end{definition}
Now, if $n=2$ then for each $a = (a_1,a_2) \in {\mathbb{R}}^2$ and $r \in {\mathbb{R}}^2$ where $r > 0$ we have an open ball centered at $a$ with radius $r$ is:
\begin{align*}
B_{r}(a) = \{x \in {\mathbb{R}}^2\ : \sqrt{(x_1 - a_1)^2 + (x_2 - a_2)^2} < r\}
\end{align*}

%% Definition of closed ball
\begin{definition}\label{def:2}{Let $a \in {\mathbb{R}}^n$: Then the \textbf{Closed Ball centered at $a$ with Radius $r > 0$} denoted $B_{r}(a)$ is defined to be a set of all points $x \in {\mathbb{R}}^n$ such that $||x-a|| \leq r$, that is, $\{x \in {\mathbb{R}}^n |\;||x - a|| \leq r\}.$}
\end{definition}
And, if $n=2$ then for each $a = (a_1,a_2) \in {\mathbb{R}}^2$ and $r \in {\mathbb{R}}^2$ where $r > 0$ we have the closed ball centered at $a$ with radius $r$ is:
\begin{align*}
B_{r}(a) = \{x \in {\mathbb{R}}^2\ : \sqrt{(x_1 - a_1)^2 + (x_2 - a_2)^2} \leq r\}
\end{align*}

Now, a neighbourhood term can be defined as below in Equation~\eqref{eq:neighbourhood} using above definitions and distance measure of Equation~\eqref{eq:euclidean_distance}. It is useful especially relevant for a next section where neighbourhood policy is required to measure the \textit{robustness} for explanations. 

\begin{equation}\label{eq:neighbourhood}
N_{\epsilon}(x_i) = \{x_j \in X| \onespace d(x_i, x_j) \leq \epsilon\}
\end{equation}  

Here, $N_{\epsilon}(x_{i})$ is ball of radius $\epsilon$ centered at any point of interest $x_i$. We used a closed ball Definiton~\ref{def:2} because \citet{alvarez2018robustness} used the same form in the original work.

\subsection{Defining Explanation Evaluation Framework}\label{sec:defining_explanation_evaluation_framework} %\subsection{}

Now, let us define the robustness or evaluation measures using the above explanations. Intuitively, the idea is to have the measures which seek a variability in the explanations generated by interpretable methods for similar inputs. If a variation in inputs is subtle enough that it does not change the model's predictions, then it should not change explanations either. \citet{alvarez2018robustness} proposes a notion of robustness measure which forms the basis of this work.

The critical argument was that the understanding of a complex model using a single point-wise explanations might lead to a false understanding. One way to address this would be looking beyond the point of interest and covering its neighbourhood to examine the behaviour of a model. Therefore a crucial property such as \textit{robustness} is required to measure the explanations generated by the interpretability methods. It ensures that an explanation of a point is roughly constant in its vicinity regardless of the models (linear, tree-based).  In a simple form, it means that explanations for similar instances should not widely differentiate.

%\begin{wrapfigure}{r}{0.5\textwidth}
%	\centering
%	\vspace*{-5mm}
%	\includegraphics[width=1.0\linewidth]{alvarez2018_robustness_figure1}
%	\vspace*{-11mm}
%	\caption{LIME and SHAP explanations for binary classifiers \citep{alvarez2018robustness}: The heatmaps show models’ positive-class probabilities and bar charts represent explanations (attribution values where x in green and y in purple) for predictions. It is visible that both LIME and SHAP explanations are stable for linear SVM model (top) but significantly vary for a non-linear two-layer neural network (bottom).}
%	\label{fig:alvarez2018_robustness_figure1}
%\end{wrapfigure}

\begin{figure}[H]
	\centering
	\vspace*{-2mm}
	\includegraphics[width=1.0\linewidth]{alvarez2018_robustness_figure1}
	\vspace*{-10mm}
	\caption{LIME and SHAP explanations for binary classifiers from \citet{alvarez2018robustness}. The heatmaps show models’ positive-class probabilities and bar charts represent explanations (attribution values where x in green and y in purple) for predictions. It is visible that both LIME and SHAP explanations are stable for linear SVM model (top) but significantly vary for a non-linear two-layer neural network (bottom).}
	\label{fig:alvarez2018_robustness_figure1}
\end{figure}

Figure~\ref{fig:alvarez2018_robustness_figure1} presented in original work \citep{alvarez2018robustness} shows the explanations generated by LIME \citep{ribeiro2016should} and SHAP \citep{lundberg2017unified} for binary classifier predictions trained on a two-dimensional dataset. It explained that the explanations for a complex neural network model (bottom row) are often inconsistent and vary noticeably for neighbouring points. On the other hand, explanations for linear SVM's predictions (top row) are relatively stable. This instability motivated the investigation of such a phenomenon and a need for an objective tool to quantify stability.

In light of this, Lipschitz continuity as a function stability has been suggested to measure the relative change in output concerning the inputs. It measures the substantial relative deviations (global) throughout the input space. Since in the context of interpretable methods, explanations do not require uniformity for very distant inputs. Therefore, a local notion of stability (Definition~\ref{def:3}) has been proposed in the form of local Lipschitz continuity (Hein and Andriushchenko, 2017; Weng et al., 2018), which measure the relative deviations in a small neighbourhood input space.
\begin{definition}\label{def:3}{\textit{Local notion of stability}:}
	$f: X \subseteq {\mathbb{R}}^n \rightarrow {\mathbb{R}}^m$ is \textbf{\textit{locally Lipschitz}} \textit{if for every $x_0$ there exist $\delta > 0$ and $L \in {\rm I\!R}$ such that $||x - x_0|| < \delta$ implies $||f(x) - f(x_0)|| \leq L||x - x_0||$.}
\end{definition}

In the Definition~\ref{def:3}, $\delta$ and $L$ are dependent on a point of interest $x_i$. It allows quantifying the robustness of the explanations generated by a model $f$ in terms of constant $L$. Usually, this term is not known, and it requires an estimation. One straightforward way to solve it for every point of interest $x_i$ is to solve Equation~\eqref{eq:lipschitz_estimation}.

%Computing this quantity is a challenge itself because a function is not differentiable end-to-end. The computation needs a restricted evaluation budget, and luckily there are various approaches off the shelf to manage computation expenses, i.e., Bayesian Optimization  (Snoek et al. , 2012, and references therein).

\begin{equation}\label{eq:lipschitz_estimation}
L(x_i) = \operatorname*{max}_{x_j \in B_{\epsilon}(x_i)}  \frac{||f(x_i) - f(x_j)||_{2}}{||x_i - x_j||_{2}},
\end{equation}

where $B_{\epsilon}(x_{i})$ is a ball of radius $\epsilon$ centered at $x_i$. It is a similar term to the one defined in \eqref{eq:neighbourhood}. 

\subsubsection{Algorithmic Desgin}\label{sec:algorithmic_desgin} %% \subsubsection
Moving towards algorithmic functionality, Algorithm~\ref{algo:lipscitz_estimation} shows the implementation for local Lipschitz estimations used in various experimental settings. It takes 1) all the test-set points $ \in X$. 2) the explanations (attribution) vectors $f_e(X)$ generated by explanation model $f_e$  for all the test-set points $\in X$. 3) the value $\epsilon$ to define the radius. It iterates for each point of interest $x_i$ to find its neighbourhood (Line~\ref{algo:line:lipscitz_estimation_neighbourhood}). Then it estimates the maximal local Lipschitz value (Line~\ref{algo:line:lipscitz_estimation_calculation}) for the point of interest $x_i$ by solving the \eqref{eq:lipschitz_estimation} and outputs the computed value as $L(x_i)$ .

%%%% Package algorithm plus algpseudocode style
\begin{algorithm}[H]
	\caption{$LipschitzEstimations(X,\onespace f_e(X),\onespace \epsilon)$}
	\label{algo:lipscitz_estimation}
	\hspace*{\algorithmicindent} \textbf{Input}\textbf{:} $\{x_i,...,x_n\} \in X$\textbf{:} test set with all the points, $\onespace \{f_e(x_i),...,f_e(x_n)\} \in f_e(X)$\textbf{:} set of generated explanation vectors for all the points in a test set, $\onespace \epsilon$\textbf{:} radius threshold \\
	\hspace*{\algorithmicindent} \textbf{Output}\textbf{:} $L(x_i)$\textbf{:} local Lipschitz estimation value computed using \eqref{eq:lipschitz_estimation}
	\begin{algorithmic}[1]
%		\State $d$ = $||x_i||$
%		\State $\epsilon$ = $\epsilon * \sqrt{d}$ 
		\For{\texttt{$x_{i}$ in X}}
		\State \label{algo:line:lipscitz_estimation_neighbourhood} $N_\epsilon(x_i)\leftarrow \{x\in X\mid d(x,x_i)\le \epsilon\}$ \Comment {\eqref{eq:neighbourhood}}
		\State \label{algo:line:lipscitz_estimation_calculation} $L(x_i) \gets \operatorname*{max}_{x_j \in N_\epsilon(x_i)} \frac{||f_e(x_i) - f_e(x_j)||_2}{||x_i - x_j||_2}$ \Comment {\eqref{eq:lipschitz_estimation}}
		\EndFor
		\MultiLineComment[0\dimexpr\algorithmicindent]{local lipschitz estimation value for a point of interest $x_i$}
		\State \textbf{return} $L(x_i)$
		%		\EndProcedure
	\end{algorithmic}
\end{algorithm}

To be more elaborative, Line~\ref{algo:line:lipscitz_estimation_neighbourhood}
takes 1)  point of interest $x_i$, which requires a local Lipschitz estimation. 2) all the test-set points $ \in X$. 3) the value $\epsilon$ to define the radius. Given the inputs, it calculates the Euclidean distances from point $x_i$ to all points in $X$. If the distance is less than or equal to $\epsilon$ then the point $x_j \in X$ is considered to be in a neighbourhood $N_{\epsilon}(x_i)$. Once the neighrbouring points are found for any point $x_i$, a local Lipschitz estimation value can be calculated (Line~\ref{algo:line:lipscitz_estimation_calculation}) and after the calculation a value is returned.

\subsubsection{Lipschitz Estimation as a Stability Measure}\label{sec:lipschitz_estimation_as_a_stability_measure} %% \subsubsection
In the beginning, Section~\ref{sec:defining_explanation_evaluation_framework} mentions that local Lipschitz estimation as a stability function measures relative deviations inside the neighbourhood input space. To support the argument, Figure~\ref{fig:alvarez2018_robustness_figure2} from \citet{alvarez2018robustness} helps to demonstrate that how $L(x_i)$ computed by above mechanism describe the relative deviations.

The idea is to find the worst-case deviation point $x_j$ which maximise the \eqref{eq:lipschitz_estimation} for point of interest $x_i$. For instance, in Figure~\ref{fig:alvarez2018_robustness_figure2} from \citet{alvarez2018robustness} the examples from the BOSTON dataset show that the point of interests are incredibly similar, but their explanations by each method for the model’s prediction vary considerably. It also depicts that maximizing \eqref{eq:lipschitz_estimation} for $x_i$ measures relative deviation and allows us to enforce the robustness mechanism on explanations methods.
\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{alvarez2018_robustness_figure2}
	\vspace*{-10mm}
	\caption{LIME and SHAP explanations deviation examples: \citep{alvarez2018robustness}: \textbf{Top}: example $x_i$ from the BOSTON dataset and its explanations (attributions). \textbf{Bottom}: explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation}.}
	\label{fig:alvarez2018_robustness_figure2}
\end{figure}


\subsubsection{Lipschitz Estimation as a Separability Measure}\label{sec:lipschitz_estimation_as_a_separability_measure} %% \subsubsection
An example presented in the above section shows the case where generated explanations widely differ for similar inputs. Another question appears that how to identify the cases where difference in explanations can serve as a useful quantity. For example, in a classification case, multiple instances belong to different class labels. A well-trained predictive model classifies the instances correctly into corresponding class labels. Explanations methods interpret the predictive model and give explanations behind the predictions. Now, how to measure the difference in explanations for two different correctly classified instances?
%what is the way to evaluate that the instances which are classified correctly to two different classes are having a different explanations?

Similar to the idea presented in \citet{alvarez2018robustness}, which uses Lipschitz continuity as a stability function. We further utilise the concept to define Lipschitz Estimation as a Separability Measure, given as follows:

\begin{equation}\label{eq:lipschitz_estimation_as_a_separability_measure}
L(x_i) = \min_{x_j \notin B_{\epsilon}(x_i)}  \frac{||f(x_i) - f(x_j)||_{2}}{||x_i - x_j||_{2}},
\end{equation}

where $B_{\epsilon}(x_{i})$ is a ball of radius $\epsilon$ centered at $x_i$.

It similarly measures the relative deviation in explanation regarding inputs, but looks outside the neighbourhood points and then minimise the estimation. As a result, it indicates the separability in generated explanations that which explanations are relatively similar, but their inputs are widely different in feature values. Figure~\ref{fig:iris_rf_deviation_separability} from the following experiments presents a worst-deviation example to show the usefullness of separability measure.

\section{Experiments and Evaluation}\label{sec:experiments_and_evaluation} %\section{Four}
This section presents the experiments conducted to investigate the phenomenon of relative deviation in explanations regarding inputs using the robustness measure (Section~\ref{sec:methodologies}). It mainly constitutes two experimental parts. The first part uses \textit{LIME} and \textit{SHAP} to interpret random forest and logistic regression models trained on the UCI classification datasets (iris, ionosphere, glass). The second part uses explanation methods (\textit{Saliency, Grad*Input, e-LRP, Occlusion, Deeplift}) provided by \textit{DeepExplain} framework to explain the predictions of the convolutional neural network model trained on MNIST handwritten digits dataset.

The main objective is to utilise the explanation evaluation measure to quantify the instability in generated explanations. Section~\ref{sec:interpret_radnom_forest_and_logistic_regression} aims to present and explains the results of the first part. Section~\ref{sec:interpret_convolutional_neural_network} shows the results of the second part.  On a high-level, there are four components in each of the experiments pipeline 1) datasets and preparation, 2) training and evaluating predictive models, 3) building explanation models and 4) evaluating explanations. In the end, Section~\ref{sec:results_summary} summarises the results.

\subsection{Interpreting Random Forest and Logistic Regression}\label{sec:interpret_radnom_forest_and_logistic_regression}
In the first part of the experiments, the robustness of the black-box interpretability methods \textit{(LIME \& SHAP)} is calculated. Three benchmark classification datasets are used to train and interpret black-box models. For each dataset, the pipeline is 1) Split the dataset into train:test sets by 80:20 ratio, 2) Train random forest and logistic regression classifiers on them, 3) Explain the prediction using explanation methods, and 4) Calculate local robustness of explanations using measures defined in Section~\ref{sec:defining_explanation_evaluation_framework} for all the points in the test set.

\subsubsection{Datasets}\label{sec:datasets_UCI}
To perform the quantitative experiments, following three benchmark classification datasets are used.
\begin{center}
	\begin{itemize}
		\item \textit{Iris}: It is a well-known classification task of plants based on four flower leaf characteristics. It has 150 data points and three classes.
		\item \textit{Ionosphere}: It is radar data which contains the values of 16 high-frequency antennas that transmitted power on the order of 6.4 kilowatts. There are two targets values electrons in the ionosphere; "Good" or "Bad". The first indicates that there is some structure in the ionosphere, and the latter represents that there is no structure.
		\item \textit{Glass}: It is a popular classification dataset which contains the values for attributes, i.e. sodium, magnesium etc. 
		Each array of values has some specific label which represents the glass category.
	\end{itemize}
\end{center}

Table~\ref{table:datasets_UCI} shows the attributes for each dataset. Iris contains a total of 150 instances with the number of attributes equal to four and the total number of class labels equal to three. Ionoshpere contains a total of 351 instances, each of which has 34 attribute values and "Good" or "Bad" class label. Glass dataset has a total of 214 instances and a total of 7 class labels; each instance has one specific class label and a total of 9 attribute values.

\begin{table}[H]
	\caption{UCI Benchmark Classification Datasets}
	\label{table:datasets_UCI}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline	
		\textbf{Datasets} & \textbf{No. of Attributes} & \textbf{No. of instances} & \textbf{No. of Classes} \\ \hline
		
		Iris  & 4 & 150 & 3 \\ \hline
		Ionoshpere  & 34 & 351 & 2 \\ \hline
		Glass  & 9 & 214 & 7 \\ \hline
	\end{tabular}
	\end{center}
\end{table}


\subsubsection{Training and Evaluating Models}
Random forest and logistic regression classifiers are used to train predictive models. In the case of random forest, \textit{"balanced\_subsample"} mode is set for setting the \textit{class\_weights}. The \textit{"balanced\_subsample"} mode computes weights based on the bootstrap sample for every tree grown. For \textit{"n\_estimators"}, multiple values \textit{(100, 200 500, 1000)} are tried to get the best performing model.

In the case of logistic regression, \textit{"max\_iter"} (maximum iteration) is set to 100 and \textit{solver="liblinear"}. The \textit{'liblinear'} as a optimisation algorithm is a good choice in our case because of the small sizes of datasets. The \textit{"penalty"} scheme is set to $l_2$ norm and \textit{class\_weights="balanced"}. The \textit{"balanced"} mode uses the values of class to adjust the weights automatically. The weights are inversely proportional to the class frequencies in the input data as  \textit{n\_samples/(n\_classes * np.bincount(y))}. To manage multiple class case, an \textit{"ovr"} mode is used which makes a binary problem for each label and fit a model. 

Table~\ref{table:training_models_datasets_UCI} shows fixed and grid search parameters used to train the models.


\begin{table}[H]
	\caption{Settings for Training Models}
	\label{table:training_models_datasets_UCI}
	\begin{center}
		\begin{tabular}{@{}ccc@{}}
			\toprule
			Model & Fixed Parameters & Grid Search Parameters \\ \hline

			\multicolumn{1}{|c|}{Random Forest} & \multicolumn{1}{p{40mm}|}{class\_weight=\textit{'balanced \newline subsample'}} & \multicolumn{1}{p{40mm}|}{n\_estimators=\textit{100, 200, \newline 500, 1000}} \\ \hline

			\multicolumn{1}{|c|}{Logistic Regression} & \multicolumn{1}{p{40mm}|}{max\_iter=\textit{100}, \newline solver=\textit{'liblinear'}} & \multicolumn{1}{p{40mm}|}{penalty=\textit{$l_2$},
			\newline class\_weight=\textit{'balanced'},
			\newline multi\_class=\textit{'ovr'}} \\ \hline
			

		\end{tabular}
	\end{center}
\end{table}

For each dataset, a random forest and a logistic regression classifiers are trained using the above settings for training the models. It appears that a random forest classifier train well on ionoshphere and glass datasets with the \textit{F1-score} of 0.86 and 1.00, respectively. On the other side, Logistic Regression classifier fit well on iris dataset. It gives the \textit{F1-score} of 0.96 as compared to \textit{F1-score} of 0.86 given by random forest. Table~\ref{table:training_models_F1_score_datasets_UCI} shows the \textit{F1-score} of each model.

\begin{table}[H]
	\caption{F1-scores of Classification Models}
	\label{table:training_models_F1_score_datasets_UCI}
	\begin{center}
		\begin{tabular}{@{}cccc@{}}
			\toprule
			Dataset & Random Forest Model & Logistic Regression Model \\ \hline
			
			\multicolumn{1}{|c|}{Iris} & \multicolumn{1}{c|}{0.89} & \multicolumn{1}{c|}{0.96} \\ \hline
			
			\multicolumn{1}{|c|}{Ionoshpere} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.74} \\ \hline
			
			\multicolumn{1}{|c|}{Glass} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{0.92} \\ \hline
			
		\end{tabular}
	\end{center}
\end{table}

%%% Type of glass: (class attribute) -- 1 building_windows_float_processed -- 2 building_windows_non_float_processed -- 3 vehicle_windows_float_processed -- 4 vehicle_windows_non_float_processed (none in this database) -- 5 containers -- 6 tableware -- 7 headlamps

%%% BWF = building_windows_float_processed
%%% BWNF = building_windows_non_float_processed
%%% VWF = vehicle_windows_float_processed
%%% C = containers
%%% T = tableware
%%% HL = headlamps

Moreover, the classification reports are given below for each dataset and model. These reports show the precision and recall of each model.
Table~\ref{table:rf_model_iris_report} and Table~\ref{table:lr_model_iris_report} show the classification reports of random forest and Logistic Regression trained on iris dataset, respectively. After having a train:test split, a total of 30 instances were selected in the test set. Table~\ref{table:rf_model_iris_report} shows that the random forest model classifies all the instances correctly which belong to class \textit{"setosa"}. However, it misclassifies one instance of \textit{"versicolor"} class and two instances from \textit{"virginica"} class. The precision values achieved by the random forest classifier are 1.0, 0.83 and 0.86, where recall values are 1.0, 0.71 and 0.92 for all the three classes: \textit{"setosa"}, \textit{"versicolor"}, and \textit{"viginica"}, respectively.  

\begin{table}[H]
	\centering
	\caption{Iris: Classification Report of random forest}
	\label{table:rf_model_iris_report}
	\begin{tabular}{@{}ccccccc@{}}
		\toprule
		& setosa & versicolor & virginica & precision & recall & f1-score \\ \hline
		
		\multicolumn{1}{|c|}{setosa} & \multicolumn{1}{c|}{10} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
		
		\multicolumn{1}{|c|}{versicolor} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0.83} & \multicolumn{1}{c|}{0.71} & \multicolumn{1}{c|}{0.77} \\ \hline
		
		\multicolumn{1}{|c|}{virginica} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.92} & \multicolumn{1}{c|}{0.89} \\ \hline

	\end{tabular}
\end{table}

Table~\ref{table:lr_model_iris_report} shows that the logistic regression model which accurately classifies all the instances of class \textit{"setosa"} and \textit{"versicolor"}. However, it misclassifies one instance of \textit{"virginica}" class. The precision values achieved by the logistic regression classifier are 1.0, 0.10 and 0.93, where recall values are 1.0, 0.86 and 0.10 for all the three classes: \textit{"setosa"}, \textit{"versicolor"}, and \textit{"viginica"}, respectively.

\begin{table}[H]
	\centering
	\caption{Iris: Classification Report of Logistic Regression}
	\label{table:lr_model_iris_report}
	\begin{tabular}{@{}ccccccc@{}}
		\toprule
		& setosa & versicolor & virginica & precision & recall & f1-score \\ \hline
		
		\multicolumn{1}{|c|}{setosa} & \multicolumn{1}{c|}{10} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
		
		\multicolumn{1}{|c|}{versicolor} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{6} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.92} \\ \hline
		
		\multicolumn{1}{|c|}{virginica} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{13} & \multicolumn{1}{c|}{0.93} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{0.96} \\ \hline
	\end{tabular}
\end{table}


Table~\ref{table:rf_model_iono_report} shows the report of the random forest model trained on ionoshpere dataset. It wrongly classifies the five instances of \textit{"Bad"} class and gives the values of 0.80 and 0.83 for precision and recall. For a class label \textit{"Good"}, it only misclassifies the four instances out of fourty-two which gives the precision and recall values of 0.91 and 0.89, respectively.

\begin{table}[H]
	\begin{center}
		\caption{Ionoshpere: Classification Report of Random Forest}
		\label{table:rf_model_iono_report}
		\begin{tabular}{@{}cccccc@{}}
			\toprule
			& Bad & Good & precision & recall & f1-score \\ \hline
			\multicolumn{1}{|c|}{Bad} & \multicolumn{1}{c|}{20} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{0.8} & \multicolumn{1}{c|}{0.83} & \multicolumn{1}{c|}{0.82} \\ \hline
			\multicolumn{1}{|c|}{Good} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{42} & \multicolumn{1}{c|}{0.91} & \multicolumn{1}{c|}{0.89} & \multicolumn{1}{c|}{0.9} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

Table~\ref{table:lr_model_iono_report} shows the report of of Logistic Regression model trained on ionoshpere dataset. In constrast to the random forest model, it misclassifies more instances with respect to class label \textit{"Bad"}, droping the precision and recall values to 0.76 and 0.56. It also decreases the precision of class label \textit{"Good"} but it seems that a recall value increases from 0.89 to 0.91.

\begin{table}[H]
	\begin{center}
		\caption{Ionoshpere: Classification Report of Logistic Regression}
		\label{table:lr_model_iono_report}
		\begin{tabular}{@{}cccccc@{}}
			\toprule
			& Bad & Good & precision & recall & f1-score \\ \hline
			\multicolumn{1}{|c|}{Bad} & \multicolumn{1}{c|}{13} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{0.76} & \multicolumn{1}{c|}{0.54} & \multicolumn{1}{c|}{0.63} \\ \hline
			\multicolumn{1}{|c|}{Good} & \multicolumn{1}{c|}{11} & \multicolumn{1}{c|}{43} & \multicolumn{1}{c|}{0.80} & \multicolumn{1}{c|}{0.91} & \multicolumn{1}{c|}{0.85} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

Table~\ref{table:rf_model_glass_report} and Table~\ref{table:lr_model_glass_report} show the classification reports of Random forest and logistic regression classifiers trained on glass dataset. The class labels shown in the tables are the short forms of the original labels. Original labels are given below:
\begin{itemize}
	\item BWF = building\_windows\_float\_processed
	\item BWNF = building\_windows\_non\_float\_processed
	\item VWF = vehicle\_windows\_float\_processed
	\item C = containers
%	\item T = tableware
	\item HL = headlamps
\end{itemize}

Logistic Regression model misclassifies two instances of class label \textit{"BWNF"}, which decreases the precision and recall values from 1.00 to 0.91 and 0.86, respectively. Moreover, it also misclassifies one out of five instances of class label \textit{"VWF"} that drops the precision value to 0.67 and recall value to 0.8. On the other side, random forest model with the settings mentioned in Table~\ref{table:training_models_datasets_UCI} achieves 100\textit{\%} precision and recall for all the class labels. 

\begin{table}[H]
	\begin{center}
		\caption{Glass: Classification Report of Random Forest}
		\label{table:rf_model_glass_report}
		\begin{tabular}{@{}ccccccccc@{}}
			\toprule
			&  BWF & BWNF & VWF & C & HL & precision & recall & f1-score \\ \hline
			\multicolumn{1}{|c|}{BWF} & \multicolumn{1}{c|}{15} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{BWNF} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{14} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{VWF} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{C} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{HL} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
		\end{tabular}
	\end{center}
\end{table}


\begin{table}[H]
	\begin{center}
		\caption{Glass: Classification Report of Logistic Regression}
		\label{table:lr_model_glass_report}
		\begin{tabular}{@{}ccccccccc@{}}
			\toprule
			& BWF & BWNF & VWF & C & HL & precision & recall & f1-score \\ \hline
			\multicolumn{1}{|c|}{BWF} & \multicolumn{1}{c|}{15} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{BWNF} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0.92} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.89} \\ \hline
			
			\multicolumn{1}{|c|}{VWF} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0.67} & \multicolumn{1}{c|}{0.8} & \multicolumn{1}{c|}{0.73} \\ \hline
			
			\multicolumn{1}{|c|}{C} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{HL} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

In conclusion, it appears that Logistic Regression fits well on iris dataset as compared to random forest model. On the other hand, random forest is a better fit on ionoshphere and glass datasets. Now, these six predicive models can be used as black-box models in further experiments.

\subsubsection{Interpreting Models using LIME \& SHAP}
Now, having predictive models as black-boxes at our disposal, the black-box interpretability models (LIME~\ref{sec:LIME} and SHAP~\ref{sec:SHAP}) can be used to interpret predictions and generate explanations. Selected iris dataset examples and predictions are used in the following demonstrations to present the concept of interpreting the black-box model.

As explained in Section~\ref{sec:LIME}, LIME only provides the framework to explain an individual prediction, and by combining multiple explanations it gives the global perspective of a black-box model. The way LIME gives an explanation is illustrative in Figure~\ref{fig:lime_rf_iris_instance_explanation}. It shows an example of interpreting random forest's prediction for an instance of iris dataset. The class probabilities are shown and in this case the class is correctly predicted to \textit{"setosa"} with $p(setosa)=1.0$. To explain the prediction, it describes the range of values which represent the positive or negative contribution of a feature in the prediction. As a result, LIME provides an explanation vector which contains the values for each feature.
\begin{figure}[H]
	\centering
	\vspace*{5mm}
	\includegraphics[width=1.0\linewidth]{lime_rf_iris_instance_explanation.png}
	\caption{\textbf{LIME} explanation of a \textbf{Random Forest} model: The figure shows an example of explanation generated using LIME for a random forest prediction on a test instance of the \textbf{iris} dataset.}
	\label{fig:lime_rf_iris_instance_explanation}
\end{figure}

In contrast to random forest model, logistic regression model predicts the class label \textit{"setosa"} with the $p(setosa) = 0.85$ and gives $p(versicolor) = 0.15$ to class \textit{"versicolor"}. It is visible that both classifiers correctly predict class \textit{"setosa"}, but what made a Logistic Regression model to assign a probability of $p=0.15$ to class \textit{"versicolor"}? In the Figure~\ref{fig:lime_lr_iris_instance_explanation}, it appears that the classifier does it mainly due to a \textit{"petal width"} feature. Though the prediction result is same but an interpretation is different which shows that the models treat feature values in different ways. Therefore, an explanation vector generated for a Logistic Regression prediction contains different values for each feature.
\begin{figure}[H]
	\centering
	\vspace*{5mm}
	\includegraphics[width=1.0\linewidth]{lime_lr_iris_instance_explanation.png}
	\caption{\textbf{LIME} explanation of a \textbf{Logistic Regression} model: The figure shows an example of explanation generated using LIME for a Logistic Regression prediction on a test instance of the \textbf{iris} dataset}
	\label{fig:lime_lr_iris_instance_explanation}
\end{figure} 


Figure~\ref{fig:lime_iris_instance_interpretations} tries to explain the difference more explicitly. It presents the local explanations for class \textit{"versicolor"}
for both models and shows an explanation that which specific features influence Logistic Regression model to assign some probability to a wrong class. It is illusrative in Figure~\ref{fig:lime_lr_iris_instance_interpretations} that the \textit{"petal width"} feature contributes positive regarding class \textit{"versicolor"}. LIME highlights it by assigning positive value to the feature in an explanation vector.
\begin{figure}[H]
	\vspace*{0mm}
	\centering
	\subfloat[Random Forest, LIME\label{fig:lime_rf_iris_instance_interpretations}]{{\includegraphics[width=5cm,height=3.5cm]{lime_rf_iris_instance_interpret.png} }}
	\qquad
	\subfloat[Logistic Regression, LIME\label{fig:lime_lr_iris_instance_interpretations}]{{\includegraphics[width=5cm,height=3.5cm]{lime_lr_iris_instance_interpret.png} }}
	\caption{LIME explanations for class "versicolor": The figure presents the local explanations generated using LIME for both models. It shows the difference in values of explanations for class "versicolor".}%
	\label{fig:lime_iris_instance_interpretations}%
\end{figure}


Similarly, SHAP~\ref{sec:SHAP} provides a framework to interpret the black-box models and gives the explanation vectors for each instance of interest. Figure~\ref{fig:shap_iris_summary_plots} shows the summary plots generated after calculating the shapley values on the iris dataset predictions. SHAP tells that the overall feature \textit{"petal width"} is the most important in random forest predictions (Figure~\ref{fig:shap_rf_iris_summary_plots}). However, in Logistic Regression the \textit{"petal length"} feature impacts the most in making overall predictions (Figure~\ref{fig:shap_lr_iris_summary_plots}). 
\begin{figure}[H]
	\vspace*{0mm}
	\centering
	\subfloat[Random Forest, SHAP\label{fig:shap_rf_iris_summary_plots}]{{\includegraphics[width=5cm,height=3.5cm]{shap_rf_iris_summary_plot.png} }}%
	\qquad
	\subfloat[Logistic Regression, SHAP\label{fig:shap_lr_iris_summary_plots}]{{\includegraphics[width=5cm,height=3.5cm]{shap_lr_iris_summary_plot.png} }}%
	\caption{\textbf{SHAP} explanations summary plots: The figure shows the summary plots of explanations generated using SHAP for both models. It presents that SHAP gives the explanation vector for each instance of interest, and the figure contains a summary of all the explanations vectors.}%
	\label{fig:shap_iris_summary_plots}%
\end{figure}

It is apparent from the above interpretations that the explanations generated by explanation methods (LIME and SHAP) depends on a black-box model regardless of the same predictions. Moreover, the explanations vary and there is need to quantify the variations in the explanations. It helps to understand the nature of a black-box model and the importance of explanation method.

\subsubsection{Evaluating Explanations and Results}\label{sec:evaluating_explanations_and_results_part1}
As now it is established how predictive models are built and how to generated explanation vectors using LIME \citep{ribeiro2016should} and SHAP \citep{lundberg2017unified}. It is feasible to apply robustness measures defined in Section~\ref{sec:lipschitz_estimation_as_a_stability_measure} \citep{alvarez2018robustness} and Section~\ref{sec:lipschitz_estimation_as_a_separability_measure}.

Similar to the previous section, this section also uses Iris dataset examples first to present the worst-deviation case (Section~\ref{sec:defining_explanation_evaluation_framework}) and then the dataset-level estimations for each model (random forest and logistic regression) and dataset (iris, ionosphere, glass).
The main ideas presented were the need to quantify the variation occurs in the explanations concerning the inputs. 1) Local Explanation Stabiliy: If inputs do not differ widely, then the corresponding explanations should not vary largely. 2) Local Explanation Separability: If explanations do not differ widely, then the corresponding inputs should not vary largely. The defined robustness measures gives a mechanism to quantify the variations and evaluate the explanation methods.

\subsubsection*{Local Explanation Stability Measure}
In figure~\ref{fig:iris_rf_deviation_stability}, an example of finding the worst-case deviation is presented. The illustration aims to show that which similar instances are having a wide difference in explanations.  Figure~\ref{fig:iris_rf_deviation_stability_shap} shows the deviation scenario of SHAP explanations when interpreting the random forest for a point of interest $x_i$. It shows the deviation of explanation which maximises the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation}. However, the value of Lipschitz estimate ($L=0.20$) is low in this specific case, but it still captures the change of positive to a negative contribution of the feature \textit{"sepal length"}.

%https://tex.stackexchange.com/questions/42968/
%reduction-of-space-between-two-sub-figures
% https://latex.org/forum/viewtopic.php?t=28543
\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[SHAP (L=0.20)\label{fig:iris_rf_deviation_stability_shap}]{
		{\includegraphics[width=0.49\linewidth,height=7cm]{iris_rf_deviation_stability_shap.png}}}%
	\hspace*{\fill}%
	\subfloat[LIME (L=2.80)\label{fig:iris_rf_deviation_stability_lime}]{
		{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_rf_deviation_stability_lime.png}}}%
	\caption{LIME and SHAP explanations deviation examples: \textbf{Top}: example $x_i$ from the IRIS dataset and its explanations (attributions). \textbf{Bottom}: Explanations generated for the \textbf{Random Forest} model that maximising the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation}.}%
	\label{fig:iris_rf_deviation_stability}%
\end{figure}

On the other side, Figure~\ref{fig:iris_rf_deviation_stability_lime} shows the deviation scenario of LIME explanations when interpreting the random forest for a point of interest $x_i$. In contrast, LIME explanations show a wide deviation. It maximises the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation} and gives the value of $L=2.80$. It seems that even a slight difference in feature values of \textit{"petal length"} and \textit{"petal width"} has a high impact on LIME explanations.

%\begin{center}
%	\begin{figure}[H]
%	\includegraphics[width=1.0\columnwidth]{iris_rf_deviation_stability.png}
%		\vspace*{-5mm}
%		\caption{LIME and SHAP explanations deviation examples on interpreting random forest model: \textbf{Top}: example $x_i$ from the \textit{iris} dataset and its explanations (attributions). \textbf{Bottom}: explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%		\label{fig:iris_rf_deviation_stability}
%	\end{figure}
%\end{center}

Similarly, the deviation scenarios in explanations generated for the logistic regression model are presented in Figure~\ref{fig:iris_lr_deviation_stability}. The first noticeable thing is that even for the same point of interest $x_i$, LIME and SHAP provides different explanations when treating logistic regression as a black-box model (top row). In comparison to Figure~\ref{fig:iris_rf_deviation_stability}, here SHAP and LIME treat \textit{"petal width"} feature as a negative contributor. LIME explanation also shows that \textit{"sepal length"} feature contributes negatively.

Figure~\ref{fig:iris_lr_deviation_stability} also illustrates both deviation cases of LIME and SHAP explanations for logistic regression model. In Figure~\ref{fig:iris_lr_deviation_stability_shap}, an explanation generated using SHAP maximises the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation} and gives the value $L=0.157$. It shows that the deviated instance explanation (bottom left) has \textit{"sepal width"} feature as a negative contributor instead of positive. Now, Figure~\ref{fig:iris_lr_deviation_stability_lime} shows the deviation case of LIME explanations. In contrast to SHAP (Figure~\ref{fig:iris_lr_deviation_stability_lime}), LIME explanations deviate largely and maximise the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation} with the value of $L=2.03$. 

\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[SHAP (L=0.157)\label{fig:iris_lr_deviation_stability_shap}]{{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_lr_deviation_stability_shap.png}}}%
	\hspace*{\fill}%
	\subfloat[LIME (L=2.03)\label{fig:iris_lr_deviation_stability_lime}]{{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_lr_deviation_stability_lime.png} }}%
	\caption{LIME and SHAP explanations deviation examples: \textbf{Top}: example $x_i$ from the IRIS dataset and its explanations (attributions). \textbf{Bottom}: Explanations generated for the \textbf{Logistic Regression} model that maximising the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation}.}%
	\label{fig:iris_lr_deviation_stability}%
\end{figure}

%\begin{center}
%	\begin{figure}[H]
%		\includegraphics[width=1.0\columnwidth]{iris_lr_deviation_stability.png}
%		\vspace*{-5mm}
%		\caption{LIME and SHAP explanations deviation examples on interpreting logistic regression model: \textbf{Top}: example $x_i$ from the \textit{iris} dataset and its explanations (attributions). \textbf{Bottom}: explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%		\label{fig:iris_lr_deviation_stability}
%	\end{figure}
%\end{center}


Likewise, the local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation} are calculated for each test point of UCI datasets (iris, ionosphere, glass) used in experiments. Figure~\ref{fig:UCI_datasets_stability_estimates} illustrates the estimations performed for both predictive models on test points of datasets using LIME and SHAP. It seems that SHAP has low deviations values in explanations generated on test points of iris dataset as compared to LIME. However, the difference in the mean is more significant when the random forest (Figure~\ref{fig:UCI_datasets_rf_stability_estimates}) model is used as a black-box instead of logistic regression (Figure~\ref{fig:UCI_datasets_lr_stability_estimates}).

Moreover, it appears that SHAP provides less deviated explanations also on test points of ionosphere dataset. In this case, explanations generated for random forest (Figure~\ref{fig:UCI_datasets_rf_stability_estimates}) model also gives low values of local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation} as compared to logistic regression (Figure~\ref{fig:UCI_datasets_lr_stability_estimates}). However, LIME seems to provide slightly more stable explanations on test points of glass datasets. In this case, it looks that the logistic regression (Figure~\ref{fig:UCI_datasets_lr_stability_estimates}) model gives low values of local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation} as compared to random forest (Figure~\ref{fig:UCI_datasets_rf_stability_estimates}), though the difference in the mean values of the estimations seems equal for both models.

\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[Random forest as black-box \& estimations as per \eqref{eq:lipschitz_estimation}\label{fig:UCI_datasets_rf_stability_estimates}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{UCI_datasets_rf_stability_estimates.png}}}%
	\hspace*{\fill}%
	\subfloat[Logistic Regression as black-box \& estimations as per Equation~\eqref{eq:lipschitz_estimation}\label{fig:UCI_datasets_lr_stability_estimates}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{UCI_datasets_lr_stability_estimates.png} }}%
	\caption{Dataset-level local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation} computed for two predictive models on test points of various UCI classification datasets.}%
	\label{fig:UCI_datasets_stability_estimates}%
\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=1.0\linewidth]{iris_rf_robustness_lipschitz_estimates.png}
%	\vspace*{-5mm}
%	\caption{LIME and SHAP dataset level explanations on interpreting \textit{Random Forest} models: Explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%	\label{fig:iris_rf_robustness_lipschitz_estimates}
%\end{figure} 

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=1.0\linewidth]{iris_lr_robustness_lipschitz_estimates.png}
%	\vspace*{-5mm}
%	\caption{LIME and SHAP dataset level explanations on interpreting \textit{Logistic Regression} models: Explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%	\label{fig:iris_lr_robustness_lipschitz_estimates}
%\end{figure} 

%The aggregated or mean values of local Lipschitz estimates in the above multiple scenarios also depict the evaluation for explanation methods (LIME and SHAP). Table~\ref{table:lipschitz_estimation_aggregated_datasets_UCI} contains these mean values of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation} computed in figure~\ref{fig:UCI_datasets_stability_estimates}.


\subsubsection*{Local Explanation Separability Measure}
Applying a local explanation separability measure reveals important results. Figure~\ref{fig:iris_rf_deviation_separability} shows an example of minimising the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure} for a point of interest $x_i$, i.e., which similar explanations are having a large difference in inputs.

Figure~\ref{fig:iris_rf_deviation_separability} shows the results when the random forest model is interpreted using SHAP method. A random forest model classifies both inputs correctly and assigns probability values of above 99\% to correct class labels, \textit{p("versicolor")}=0.996 (top left) and \textit{p("setosa")}=1.0 (bottom left). It appears that a model easily differentiates the difference in feature values while making predictions. However, it seems that SHAP does not provide different explanations for different class labels. It gives an almost similar explanation for these two different predictions. 

%https://tex.stackexchange.com/questions/42968/
%reduction-of-space-between-two-sub-figures
% https://latex.org/forum/viewtopic.php?t=28543
\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[SHAP (L=0.014)\label{fig:iris_rf_deviation_separability_shap}]{
		{\includegraphics[width=0.49\linewidth,height=7cm]{iris_rf_deviation_separability_shap.png}}}%
	\hspace*{\fill}%
	\subfloat[LIME (L=0.058)\label{fig:iris_rf_deviation_separability_lime}]{
		{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_rf_deviation_separability_lime.png}}}%
	\caption{LIME and SHAP explanations separability deviation examples: \textbf{Top}: example $x_i$ from the IRIS dataset and its explanations (attributions). \textbf{Bottom}: Explanations generated for the \textbf{Random Forest} model that minimising the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure}.}%
	\label{fig:iris_rf_deviation_separability}%
\end{figure}


On the other side, Figure~\ref{fig:iris_rf_deviation_separability_lime} shows the plot when LIME explanations are used to minimise the Lipschitz estimate for a point of interest $x_i$ as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure}. In contrast to SHAP explanations, LIME provides better separability results. The first reason is that for LIME explanations, a deviated example is also classified to the same class label \textit{"versicolor"} as a point of interest $x_i$. The other reason is that LIME seems to capture the difference in feature values, i.e. a \textit{"sepal length"} contributes negatively in deviated instance explanation and that drops the prediction probability value from \textit{p("versicolor")}=0.996 (top right) to \textit{p("versicolor")}=0.85 (bottom right).


Similarly, Figure~\ref{fig:iris_lr_deviation_separability} shows an example of applying separability measure \eqref{eq:lipschitz_estimation_as_a_separability_measure} on a setting where logistic regression model is used as a black-box model. Replacing a predictive model to logistic regression shows the drastic change in the worst-case deviation results. It seems that in this setting, both SHAP and LIME lacks a real separability in producing different explanations for inputs which are widely different in feature values.
%https://tex.stackexchange.com/questions/42968/
%reduction-of-space-between-two-sub-figures
% https://latex.org/forum/viewtopic.php?t=28543
\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[SHAP (L=0.046)\label{fig:iris_lr_deviation_separability_shap}]{
		{\includegraphics[width=0.49\linewidth,height=7cm]{iris_lr_deviation_separability_shap.png}}}%
	\hspace*{\fill}%
	\subfloat[LIME (L=0.086)\label{fig:iris_lr_deviation_separability_lime}]{
		{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_lr_deviation_separability_lime.png}}}%
	\caption{LIME and SHAP explanations separability deviation examples: \textbf{Top}: example $x_i$ from the IRIS dataset and its explanations (attributions). \textbf{Bottom}: Explanations generated for the \textbf{Random Forest} model that minimising the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure}.}%
	\label{fig:iris_lr_deviation_separability}%
\end{figure}

It is illustrative in Figure~\ref{fig:iris_lr_deviation_separability_shap} that even being correctly predicted to two different class labels and having a significant shift in feature values, SHAP provides a similar explanation for both inputs. Figure~\ref{fig:iris_lr_deviation_separability_lime} gives slightly better value but produces similar results. It also gives similar explanations for inputs which differ widely in feature values and belong to separate class labels.

Finally, Figure~\ref{fig:UCI_datasets_separability_estimates} presents the calculated local Lipschitz estimates as a separability measure for each test point of UCI classification dataset (iris, ionosphere, glass) using LIME and SHAP. In Figure~\ref{fig:UCI_datasets_rf_separability_estimates}, it appears that SHAP explanations for random forest model and iris have low estimated separability \eqref{eq:lipschitz_estimation_as_a_separability_measure} mean value as compared to LIME explanations. However, the case is opposite if a model is a logistic regression. It is illustrative in Figure~\ref{fig:UCI_datasets_lr_separability_estimates} that SHAP explanations have high estimated separability \eqref{eq:lipschitz_estimation_as_a_separability_measure} mean value compared to LIME explanations.

In the case of ionosphere dataset, the SHAP explanations for both predictive models result in low estimated separability \eqref{eq:lipschitz_estimation_as_a_separability_measure} mean values as compare to LIME explanations. Though if we make here a small comparison between both models, then explanations generated for logistic regression model follow more separability than the explanations for the random forest model.

\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[Random forest as black-box \& estimations as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure}\label{fig:UCI_datasets_rf_separability_estimates}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{UCI_datasets_rf_separability_estimates.png}}}%
	\hspace*{\fill}%
	\subfloat[Logistic Regression as black-box \& estimations as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure}\label{fig:UCI_datasets_lr_separability_estimates}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{UCI_datasets_lr_separability_estimates.png} }}%
	\caption{Dataset-level local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure} computed for two predictive models on test points of various UCI classification datasets.}%
	\label{fig:UCI_datasets_separability_estimates}%
\end{figure}

In the case for glass dataset, it looks that when a model is random forest, then the SHAP explanations seem to follow slightly more separability \eqref{eq:lipschitz_estimation_as_a_separability_measure}  as compared to LIME explanations (Figure~\ref{fig:UCI_datasets_rf_separability_estimates}). However, the results are opposite when a black-box model is logistic regression. In Figure~\ref{fig:UCI_datasets_lr_separability_estimates}, LIME explanations have high estimated separability \eqref{eq:lipschitz_estimation_as_a_separability_measure}  mean value compared to SHAP explanations.

\subsection{Interpreting Convolutional Neural Network (CNN)}\label{sec:interpret_convolutional_neural_network}
In the second part of the experiments, a Convolutional Neural Network (CNN) trained on MNIST handwritten digits dataset is used as a black-box model. Overall the experiments pipeline is the same as used in Section 4.1, but there is a slight difference. At the first step, 1) Prepare the training and test set 2) Train a Convolutional Neural Network (CNN) with the best possible accuracy. 3) Predictions are explained using DeepExplain framework, which comprises various neural network interpretability models (\textit{Saliency, Grad*Input, e-LRP, Occlusion, Deeplift}). 4) Calculate the local robustness of explanations using measures defined in Section~\ref{sec:defining_explanation_evaluation_framework} for test points. 

\subsubsection{Dataset}\label{sec:dataset_MNIST}
The MNIST handwritten digits dataset is primarily used in performing all the experiments in the following sections. However, apart from original dataset samples, Section~\ref{sec:CNN_evaluating_explnations_and_resuls} uses a small artificial dataset created by adding Gaussian noise to showcase the results.

Table~\ref{table:datasets_MNIST} shows the attributes for MNIST dataset of handwritten digits. It contains 60,000 training examples and a test set of 10,000 examples. The total number of columns or attributes are 784, excluding the class label. Each image has a height of 28 pixels and a width of 28 pixels, which gets in a total of 784 pixels. Pixels are row-wise organised, and the pixel-value is an integer between 0 and 255, 0 means background (white), 255 means foreground (black). There are a total of 10 class labels in training examples, and each image has one label value range from  0 to 9, which represents the class of an image.

\begin{table}[H]
	\caption{MNIST Handwritten Digits Dataset }
	\label{table:datasets_MNIST}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline	
			\textbf{Attributes} & \textbf{Training examples} & \textbf{Testing examples} & \textbf{Classes} \\ \hline
			784 & 60,000 & 10,000 & 10 \\ \hline
		\end{tabular}
	\end{center}
\end{table}

As mentioned above, a new small dataset is also created for demonstrating the evaluation of explanations results. It draws a random example from a training example and samples new images by adding Gaussian noise. The noise ratio slightly gets increase during the sampling, and a total of 5 to 7 instances are finally drawn at the end of sampling.

\subsubsection{Training and Evaluating CNN}
To proceed further in experiments, a CNN trained on MNIST dataset used as a black-box model. Several models with different properties were tried out in order to get the best accuracy value of 0.99 on the test set. The selected model contains a total of five layers, two \textit{Con2D}, one \textit{Maxpooling2D} and two \textit{Dense} layers.

In the first Con2D layer, a\textit{ kernel\_size=(3,3)} and an \textit{activation='relu'} with \textit{input\_shape=(28, 28, 1)} are used along with 32 output filters. The second Con2D layer also contained a \textit{kernel\_size=(3,3)} and an \textit{activation='relu'} with a value of 64 for filters. A Maxpooling2D layer with \textit{pooling=(2,2)} and a \textit{dropout=0.25} was applied on top of the second layer to reduce the calculations of weights. To perform the classfication, a final output was flattend in order to feed it to the feed forward network who contains the last two \textit{Dense} fully-connected layers. Finally, an activation='softmax' in the output layer helps to make the classification for images.

As visible in Figure~\ref{fig:cnn_training_plots}, a total of 10 epochs are used to train the network. A \textit{'rmsprop'} value is used for an \textit{opitmizer} and \textit{batch\_size} was set to 128 instances. Moreover, the figure shows the loss, accuracy and error patterns throughout the training process.

\begin{figure}[H]
	%	\vspace*{0mm}
	\centering
	\subfloat[Graph of Log-loss\label{fig:cnn_log_loss_plot}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{cnn_log_loss_plot.png}}}%
	\hspace*{\fill}%
	\subfloat[Graph of Accuracy\label{fig:cnn_accuracy_plot}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{cnn_accuracy_plot.png} }}%
	\qquad
	\subfloat[Graph of Mean sqaured error\label{fig:cnn_mse_plot}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{cnn_mse_plot.png} }}%
	\caption{Training and evaluation of Convolutional Neural Network (CNN) on MNIST handwritten digits dataset.}%
	\label{fig:cnn_training_plots}%
\end{figure}

\subsubsection{Interpreting CNN using DeepExplain Framework}
Now, having a CNN trained model which can make accurate predictions, it can serve as a black-box model to DeepExplain framework (Section~\ref{sec:DeepExplain}). The framework contains attribution methods which help to interpret the predictions and generate attribution maps. Figure~\ref{fig:interpreting_CNN_using_DeepExplain} demonstrates these attribution maps generated for randomly selected instances from the test-set.

It is visible from Figure~\ref{fig:interpreting_CNN_using_DeepExplain} that multiple attribution methods are used to generate the heatmaps. These heatmaps depict the positive and negative contributions of features in the activation of a target output. The red highlighted colour shows the positive influence of features over an activation, whereas blue indicates that these features have a suppressing effect on the activation of a target neuron. Moreover, attribution maps excluding 'saliency' look very similar to each other, but it is hard to tell the similarity or difference among them. 

\begin{figure}[H]
	\vspace*{0mm}
	\centering
	\includegraphics[width=1.0\linewidth]{digit_0_interpret.png}
%	\caption{digit 0 interpret}%
%	\label{fig:digit_0_interpret}%
\end{figure}

\begin{figure}[H]
	\vspace*{-10 mm}
	\centering
	\includegraphics[width=1.0\linewidth]{digit_1_interpret.png}
	\caption{Interpreting CNN prediction by generating attribution maps using DeepExplain framework.}%
	\label{fig:interpreting_CNN_using_DeepExplain}%
\end{figure}
 
Explanation robustness measures (Section~\ref{sec:defining_explanation_evaluation_framework}) help to such measure the variation in the attribution maps. Measuring the variation not only helps to understand the workflow of the CNN model but also evaluates the attribution methods. It tells that which attribution method best describes the right activation of neurons for a target output.

\subsubsection{Evaluating Explanations and Results}\label{sec:CNN_evaluating_explnations_and_resuls}
This section shows the evaluation of explanations generated by DeepExplain framework and presents the results. Similar to Section~\ref{sec:evaluating_explanations_and_results_part1}, the local explanation robustness measures (Section~\ref{sec:defining_explanation_evaluation_framework}) are applied to extract the valuable results. In this scenario, a trained convolutional neural network model serves as a black box and attribution methods provided by DeepExplain produce attribution maps or explanation vectors. The robustness measures used in the following experiments are 1) Local Explanation Stability: If inputs do not differ widely, then the corresponding explanations should not vary largely. 2) Local Explanation Separability: If explanations do not differ widely, then the corresponding inputs should not vary largely.

In the beginning, the experiments use an artificial dataset (section~\ref{sec:dataset_MNIST}) to generate the explanations and estimate the local explanation robustness.  Next, the experiments use the test-set and selected instances to demonstrate the results for local explanation stability and local explanation separability measures. In the end, box-plots present the dataset-level estimations of robustness measures.

Following Figure~\ref{fig:digit3_noise_ratio} shows the result of applying attribution methods on a small artificial dataset. A randomly selected instance from the test set is used to generate the six versions of perturbed instances which creates the small dataset. The noise ratio varies for each perturbed instance, as shown in the first column $\sigma$ value represents the noise variation, and $p(3)$ represents the predicted class probabilities. As it is illustrative, the top row shows the original image and its corresponding attribution maps, whereas bottom rows present the attribution maps for each perturbed instance. The ampersand ($\&$) sign in the bottom rows represents the ratio $||f(x) - f(x\prime)||_2/||x - x\prime||_2$ for the perturbed instance. By maximising the ratio as per Equation~\eqref{eq:lipschitz_estimation} gives the worst-deviation case of local explanation stability measure for this small dataset.

\begin{figure}[H]
	\vspace*{0mm}
	\centering
	\includegraphics[width=1.0\linewidth]{lc_ratio_noise_digit_3.png}
	\caption{Explanations of randomly selected MNIST digit instance (top row) and six variations of it with guassian noise. The perturbed instances are labeled with probability and noise $\sigma$ value. The ampersand ($\&$) sign represents a ratio $||f(x) - f(x\prime)||_2/||x - x\prime||_2$ for perturbed instance.}%
	\label{fig:digit3_noise_ratio}%
\end{figure}

The worst-deviation case is visible in Figure~\ref{fig:digit3_noise_deviations}, which shows that inputs are similar but lead to different explanations. It is observed earlier that all perturbed instances are predicted correctly with 1.0 probability which tells that a CNN model easily differentiates the noise. However, it is not the case for attribution methods applied to generate the explanations. Notably, an attribution map extracted using \textit{Saliency} method appears to have a wide difference from the corresponding original map. Apart from \textit{Saliency} method, all other attribution methods seem to produce similar results, having a close range of estimated Lipschitz values $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation}.

\begin{figure}[H]
%	\vspace*{-5mm}
	\centering
	\subfloat[Saliency]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_Saliency.png} }}%
	\qquad
	\subfloat[Grad*input]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_Grad*input.png} }}%
	\qquad
	\subfloat[e-LRP]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_e-LRP.png} }}%
	\qquad
	\subfloat[Occlusion]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_Occlusion.png} }}%
	\qquad
	\subfloat[Deeplift]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_Deeplift.png} }}%
	\caption{Noise Dataset: Worst-case deviations for the maximizing of the Lipschitz estimation as a stability measure as per \eqref{eq:lipschitz_estimation}.}%
	\label{fig:digit3_noise_deviations}%
\end{figure}

%%% ['Saliency', 'Grad*input', 'e-LRP', 'Occlusion', 'Deeplift']
\begin{figure}[H]
	\vspace*{-5mm}
	\centering
	\subfloat[Saliency]{{\includegraphics[width=3.5cm]{digit_3_deviation_Saliency.png} }}%
	\qquad
	\subfloat[Grad*input]{{\includegraphics[width=3.5cm]{digit_3_deviation_Grad*input.png} }}%
	\qquad
	\subfloat[e-LRP]{{\includegraphics[width=3.5cm]{digit_3_deviation_e-LRP.png} }}%
	\qquad
	\subfloat[Occlusion]{{\includegraphics[width=3.5cm]{digit_3_deviation_Occlusion.png} }}%
	\qquad
	\subfloat[Deeplift]{{\includegraphics[width=3.5cm]{digit_3_deviation_Deeplift.png} }}%
	\caption{Worst-case deviations for the maximizing of the Lipschitz estimation as a stability measure $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}%
	\label{fig:digit3_deviations}%
\end{figure}

Similarly, Figure~\ref{fig:digit3_deviations} shows the worst-case deviations, which maximise the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation}. However, a neighbourhood in this scenario is all the images which have a similar label; for example, all digit images with label three. As shown in the Figure~\ref{fig:digit3_deviations}, \textit{Saliency} method gives highest $L(x_i)$ value as compared to other methods. It indicates that the Saliency method is more likely to produce unstable explanations. On the other side, \textit{DeepLift} gives the minimum $L(x_i)$ value which makes it the most stable method as compare to others. \textit{Grad*Input} and \textit{e-LRP} methods output almost the similar values, whereas Occlusion estimates a slighly higher value.

The above results are produced as the \citet{alvarez2018robustness} showed how to apply such robustness measure \eqref{eq:lipschitz_estimation} on explanations. However, in the original work, the image with label seven was used to demonstrate the results. As described earlier, extending the \citet{alvarez2018robustness}'s method, we introduced a new robustness measure \eqref{eq:lipschitz_estimation_as_a_separability_measure}. Likewise, we applied it on explanations and results are visible in the following Figure~\ref{fig:digit3_deviations_separability}.

Interestingly, Figure~\ref{fig:digit3_deviations_separability} shows different aspects regarding evaluating the explanations. In this scenario, it minimises the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure} therefore the neighbourhood was all the other images with a non-similar label, e.g., all images which do not have label three. Intuitively, the point of interest image is close to the image which should have a label eight; however, it is not the case. 

\begin{figure}[H]
	\vspace*{-5mm}
	\centering
	\subfloat[Saliency]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_Saliency.png} }}%
	\qquad
	\subfloat[Grad*input]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_Grad*input.png} }}%
	\qquad
	\subfloat[e-LRP]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_e-LRP.png} }}%
	\qquad
	\subfloat[Occlusion]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_Occlusion.png} }}%
	\qquad
	\subfloat[Deeplift]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_Deeplift.png} }}%
	\caption{Worst-case deviations for the minimising of the Lipschitz estimation as a separability measure $L(x_i)$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}.}%
	\label{fig:digit3_deviations_separability}%
\end{figure}

In the Figure~\ref{fig:digit3_deviations_separability}, the CNN model accurately differentiates and classifies the deviation images, but explanations generated by attribution methods do not have such a difference. As it is visible in the Figure~\ref{fig:digit3_deviations_separability}, \textit{Saliency} and \textit{Occlusion} confuse the explanation of the point of interest image with an explanation of an image with label two, whereas, the explanations generated through \textit{Grad*Input}, \textit{e-LRP} and \textit{Deeplift} tend to confuse it with an image of label zero.

%%% mnist_robustness_lipschitz_estimates_old 
%\begin{figure}[H]
%	\includegraphics[width=1.0\linewidth]{mnist_robustness_lipschitz_estimates_old.png}
%	\vspace*{-5mm}
%	\caption{Explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%	\label{fig:mnist_robustness_lipschitz_estimates}
%\end{figure} 


%%% CommentDigit 7 experiemnts
%\begin{figure}[H]
%	\vspace*{-5mm}
%	\centering
%	\subfloat[Saliency]{{\includegraphics[width=3cm]{digit_7_deviation_Saliency.png} }}%
%	\qquad
%	\subfloat[Grad*input]{{\includegraphics[width=3cm]{digit_7_deviation_Grad*input.png} }}%
%	\qquad
%	\subfloat[e-LRP]{{\includegraphics[width=3cm]{digit_7_deviation_e-LRP.png} }}%
%	\qquad
%	\subfloat[Occlusion]{{\includegraphics[width=3cm]{digit_7_deviation_Occlusion.png} }}%
%	\qquad
%	\subfloat[Deeplift]{{\includegraphics[width=3cm]{digit_7_deviation_Deeplift.png} }}%
%	\caption{Figures side by side}%
%	\label{fig:digit7_deviations}%
%\end{figure}


\section{Discussion and Conclusion}\label{sec:conclusion} %\Section{}
This last section first summarises the results gathered from the previous section. After that, it discusses the limitations and future work and then finally concludes the research. 

\subsection{Results Summary}\label{sec:results_summary} %\subsection{}
In section~\ref{sec:experiments_and_evaluation}, the experiments  showed the way of applying the robustness measures on the explanation methods for several classification settings. The first experimental setting uses logistic regression and random forest as classifiers trained on iris, ionosphere and glass datasets. The explanations are generated using interpretability methods, LIME and SHAP. After applying the robustness measures to such setting, it appears that both of them lack overall stability and separability in generating explanations.

However, in some cases, it looks SHAP performs better than LIME and vice-versa. In our specified scenario, SHAP generally explains tree-based black-box such as random forest better as compared to LIME, whereas, LIME explain logistic regression predictions slightly better then SHAP. Though, it can not be generalised since results showed such preference only when applying stability measure on the explanations. As a summary, table~\ref{table:lipschitz_estimation_aggregated_datasets_UCI} contains the mean values of local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation} computed in Figure~\ref{fig:UCI_datasets_stability_estimates}.

\begin{table}[H]
	\caption{Mean values of local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation} computed in Figure~\ref{fig:UCI_datasets_stability_estimates}}
	\label{table:lipschitz_estimation_aggregated_datasets_UCI}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Dataset} & \textbf{Model} & \textbf{LIME} & \textbf{SHAP} \\ \hline
			
			Iris & Random Forest & 0.94 & 0.22  \\ \hline
			& Logistic Regression & 0.70 & 0.31 \\ \hline
			
			Ionosphere & Random Forest & 0.10 & 0.08 \\ \hline
			& Logistic Regression & 0.33 & 0.12 \\ \hline
			
			Glass & Random Forest & 0.09 & 0.10 \\ \hline
			& Logistic Regression & 0.03 & 0.04 \\ \hline
			
		\end{tabular}
	\end{center}
\end{table}

It shows that a random forest model trained on iris and ionosphere datasets and explained by explanation method SHAP gives the lowest mean value of local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation}. It means that these combinations produce more stable explanations among comparison with other. However, for glass dataset, a combination of the logistic regression model and LIME gives the lowest mean value of local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation}.


In the case of applying separability measure on explanations, overall LIME showed slightly better results. Earlier in section, it is showed with a demonstration where iris dataset inputs are widely different, and SHAP gives an approximately exact explanation. In contrast, LIME outputs slightly different explanations for inputs having different feature values. Table~\ref{table:lipschitz_estimation_separability_aggregated_datasets_UCI} presents the mean values of local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure} for all the cases shown in Figure~\ref{fig:UCI_datasets_separability_estimates}. According to Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure}, the goal was to minimise the value in order to see which experimental settings produce more different explanations for different instances.

\begin{table}[H]
	\caption{Mean values of local Lipschitz estimates as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure} computed in Figure~\ref{fig:UCI_datasets_separability_estimates}}
	\label{table:lipschitz_estimation_separability_aggregated_datasets_UCI}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Dataset} & \textbf{Model} & \textbf{LIME} & \textbf{SHAP} \\ \hline
			
			Iris & Random Forest & 0.022 & 0.016  \\ \hline
			& Logistic Regression & 0.039 & 0.067 \\ \hline
			
			Ionosphere & Random Forest & 0.025 & 0.017 \\ \hline
			& Logistic Regression & 0.085 & 0.057 \\ \hline
			
			Glass & Random Forest & 0.0012 & 0.0013 \\ \hline
			& Logistic Regression & 0.0013 & 0.0012 \\ \hline
			
		\end{tabular}
	\end{center}
\end{table}

It appears that for Iris dataset a combination of logistic regression interpreted by SHAP outputs explanations which represent more separability compared to other combinations. For ionosphere dataset, LIME explanations follow more separability when a predictive model is logistic regression. For glass dataset, a random forest model with SHAP explainer and a LIME explainer with a rogistic regression model gives the same mean values and on average outputs the same explanations regarding separability measure.

Finally, the second experimental setting which uses convolutional neural network trained on MNIST digits as classifier and DeepExplain framework to provide explanations. Several attribution methods from DeepExplain were compared (as presented in the above experiments) and the following Figure~\ref{fig:mnist_robustness_lipschitz_estimates} and Figure~\ref{fig:mnist_robustness_lipschitz_estimates_separability} demonstrate dataset-level summary of robustness measures when applied to 100 randomly selected MNIST digits images.

In the Figure~\ref{fig:mnist_robustness_lipschitz_estimates}, it looks as expected that \textit{Saliency} method gives the highest average value for maximising the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation}, making it the most unstable method among others.  On the other side, \textit{Deeplift} method gives the lowest average value, whereas the other three methods (\textit{Grad*Input}, \textit{e-LRP}, \textit{Occlusion}) lie close to each other.

On the other side, Figure~\ref{fig:mnist_robustness_lipschitz_estimates_separability} shows the values for minimising the Lipschitz estimate $L(x_i)$ as per Equation~\eqref{eq:lipschitz_estimation_as_a_separability_measure}. In this case, \textit{Deeplift} method gives the minimum average value, which indicates that among all other methods, it lacks the most to having a separability in explanations. Similar to the previous case, \textit{Grad*Input}, \textit{e-LRP} and \textit{Occlusion} methods output values which are close to each other. \textit{Saliency} method shows the most separability in explanations by giving the highest average value.

\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{mnist_robustness_lipschitz_estimates.png}
	\vspace*{-5mm}
	\caption{Local Lipschitz estimates $L(x_i)$ as a stability measure \eqref{eq:lipschitz_estimation} computed on randomly selected 100 MNIST digits explanations.}
	\label{fig:mnist_robustness_lipschitz_estimates}
\end{figure}


\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{mnist_robustness_lipschitz_estimates_separability.png}
	\vspace*{-5mm}
	\caption{Local Lipschitz estimates $L(x_i)$ as a separability measure \eqref{eq:lipschitz_estimation_as_a_separability_measure} computed on randomly selected 100 MNIST digits explanations.}
	\label{fig:mnist_robustness_lipschitz_estimates_separability}
\end{figure} 
\subsection{Limitations and Future Work} %\subsection{}
Enhancing interpretability in machine learning by evaluating explanations is an ongoing research challenge. In this work, mainly an approach proposed by \citet{alvarez2018robustness} is studied and further extended. Following are the limitations presented in two aspects: limitation of an approach, and limitation of developed explanation evaluation framework. 

\citet{alvarez2018robustness} utilises the Lipschitz continuity approach and apply it as an approximation function to measure the relative change in explanations with respect to inputs, named as a stability measure. One of the main limitations of this approach is that it is not straight forward to use it for measuring the fidelity of an explanation. The main reason is Lipschitz continuity technique always requires a continuous function after calculating the distance among two or more vectors (Section~\ref{sec:defining_explanation_evaluation_framework}); whereas measuring fidelity is a measure for a single input vector \citep{molnarinterpretable}. Moreover, according to \citet{molnarinterpretable}’s definition of consistency, it is also a measure of a single input which makes it difficult to calculate it as a Lipschitz estimate.

Secondly, in our work, we proposed a new robust measure, as a separability measure and tested in on various benchmark datasets (Section~\ref{sec:datasets_UCI} \&~\ref{sec:dataset_MNIST}). 
It is shown that measuring the relative change of input with regard to explanations also lead to interesting results (Section~\ref{sec:CNN_evaluating_explnations_and_resuls}). However, applying this measure on the more extensive settings, e.g., textual or natural images dataset, might potentially reveal surprising outcomes. In this work, we used LIME \citep{ribeiro2016model}, SHAP \citep{lundberg2017unified} and DeepExplain framework \citep{ancona2017towards} for explanations methods. In the future, new explanations methods can be used in the comparison.

\subsection{Conclusion} %\subsection{}
In this work, we started with an aim to design a robust explanation evaluation framework which contributes to current ongoing research. In this first step, we set to review existing literature that what it means to have interpretability in machine learning. After that, we moved to explore currently popular explanation methods which provide the way to generate explanations for black-box predictions.We selected model-agnostic LIME \citep{ribeiro2016model} and SHAP \citep{lundberg2017unified} methods and model-dependent DeepExplain framework \citep{ancona2017towards}. 

We divided our experiments into two types: the first setting for model-agnostic methods and the second for model-dependent methods (Section~\ref{sec:explanation_methods}). To evaluate the robustness for generated explanations generated in both settings, we defined explanation evaluation framework by using existing measure (Section~\ref{sec:lipschitz_estimation_as_a_stability_measure}) and by proposing new robustness measure (Section~\ref{sec:lipschitz_estimation_as_a_separability_measure}). Our results showed that separability measure uncovers that behaviour which can be useful in conjunction with other evaluation metrics. 

Our results illustrated that both agnostic perturbation based methods and dependent gradient-based methods may lack stability. We used examples with minimal differences in feature values which do not affect black-box predictions, yet they have a significant impact with respect to explanations. Also, our proposed separability measure reveals that explanation methods produce similar explanations for inputs which belong to different classes and have a wide difference in feature values. It exhibits the unstable behaviour of explanations methods and put them into under question.

Particularly, if the goal of explanation methods is to provide an understanding of predictive models, then the explanations should be stable and less variant to small differences in inputs (Section~\ref{sec:lipschitz_estimation_as_a_stability_measure}). Moreover, explanations should also differentiate when there is a wide difference in feature values (Section~\ref{sec:lipschitz_estimation_as_a_separability_measure}). We applied these robustness measures on the settings where benchmark classification datasets (Section~\ref{sec:datasets_UCI} \&~\ref{sec:dataset_MNIST}) and relatively simple black-box models (Section~\ref{sec:interpret_convolutional_neural_network} \&~\ref{sec:interpret_convolutional_neural_network}) are used. However, it inspires further study where more complex predictive models trained on textual and natural images datasets can be used in exploration. 

\bibliographystyle{apalike} % alphas
%\bibliographystyle{babplain-lf}
%\bibliographystyle{plain}
%\bibliographystyle{abbrv} % numbers
\bibliography{ref}

% --- Appendices ---

% uncomment the following

% \newpage
% \appendix
% 
% \section{Example appendix}


%\subsection{mix introduction}

%\begin{itemize}
%	
%\item This starts from quite high-flying premise. :) As a general comment, it is good to review the background, but then you should in the end explicitly state what is the topic here. The topic of this thesis is not AI or ML in general, but more specifically predictive models (i.e., supervised learning) and explanations of it.
%
%\item Someone might argue that rule-based reasoning does not necessarily involve only hard-coded rules. See, e.g., https://mitpress.mit.edu/books/constraint-based-reasoning
%
%\item Again, in this thesis, is the topic generic ML algorithms inclusive of recommender systems, or "only" supervised learning?
%\end{itemize} 

%Lipton 1018 further differentiate two types of interpretability: global interpretability, and local interpretability. Global interpretability means that users can understand how the model works globally by inspecting the structures and parameters of a complex model. Global interpretability could illuminate the inner working mechanisms of machine learning models and thus can increase their transparency.  In contrast, local interpretability locally examines an individual prediction of a model, trying to figure out why the model makes the decision it makes. Local interpretability will help uncover the causal relations between a specific input and its corresponding model prediction. 
%That two help users trust a model and trust a prediction, respectively.


%\subsection{Rough content} %\subsection{}
%In explainable machine learning, explanations established interpretability and transparency of a model by answering {\itshape "why question" } behind the predictions \citep{honegger2018shedding}. \citet{honegger2018shedding} further categorizes interpretability into three types: global model interpretability, algorithm interpretability, and local model interpretability. On the other hand, \citet{lipton2016mythos} suggests establishing interpretability by breaking explanations into two pieces, transparency and post-hoc explanations. First covers {\itshape "how the model works?"} and, the latter {\itshape "how much more model can tell about the predictions?"}.
%
%These questions focus on finding possible ways to enhance user's trust in model and trust in predictions. Indeed, the answers to these questions vary and depend on the application domain. However, several explanation methods have been proposed which partially gives explanations for the model predictions. Though, due to a lack of consensus on {\itshape "what is a good explanation?"} solving these questions are challenging.
%
%[ IMPROVEMENT REQUIRED To provide solutions to open {\itshape Black Box} models, researchers have suggested multiple frameworks and terminologies \citep{guidotti2018survey}.  \citep{lipton2016mythos} also categories the solution into two primary ways "transparency" and "post-hoc" explanations, first one covers the question of {\itshape "how the model works"} and, the latter one try to address {\itshape "how much more model can tell about the predictions."} 
%One important thing is also discussing the difference between {\itshape interpretability } and {\itshape explanations.} \citep{honegger2018shedding} provides this differentiation by stating that explanations established interpretability of a model by answering {\itshape "why question" } behind the predictions.
%
%For many years, researchers have been actively working and proposing various methods to make {\itshape black box } models interpretable \citep{guidotti2018survey}. They further organize these methods into two classes: 1) Model Dependent Explanation Methods, which make black box model interpretable but tightly coupled with specific model architecture 2) Model Agnostic Explanation Methods; these methods are generalizable, thus, applicable to multiple black box models which deal with tabular, images, textual data.
%
%Local Interpretable Model-Agnostic Explanations (LIME) \citep{ribeiro2016should} and Shapely Additive Explanations (SHAP) \citep{lundberg2017unified} are few of the most prominent agnostic models identified in the literature \citep{guidotti2018survey,honegger2018shedding}, mainly provides the framework for local model interpretability, that is, producing a human-understandable explanation for a particular prediction. These explanation methods can open complex models such as DNN and work with several kinds of datasets.\newline IMPROVEMENT REQUIRED ]
%
%
%\textbf{Citations}
%\par
%\citet{honegger2018shedding} shedding light - master thesis
%\par
%\citet{beillevaire2018inside} inside black-box- master thesis KTH
%\newline \par
%
%
%\citet{doshi2017accountability} Accountability of AI under the law: The role of explanation - journal
%\par
%\citet{goodman2017european} European Union regulations on algorithmic decision-making. - journal
%\citet{bibal2016interpretability} Interpretability of Machine Learning Models - journal
%\newline \par
%
%\citet{guidotti2018survey} A survey of methods for explaining black box models - survey 2018
%\par
%\citet{du2018techniques} Techniques for interpretable machine learning - survey 2019
%\par
%\citet{murdoch2019interpretable} Interpretable machine learning: definitions, methods, and applications - survey 2019
%\par
%\citet{zhang2019machine} Machine Learning Testing: Survey - survey 2019
%\par
%\citet{mittelstadt2019explaining} Explaining explanations in AI - survey 2019
%\newline \par 
%
%
%\citet{cathy2017weapons} Weapons of Math Destruction - book
%\par
%\citet{molnarinterpretable} Interpretable machine learning - book
%\newline \par
%
%\citet{hastie09elementsofstatisticallearning} Linear models - book
%\par
%\citet{hastie09elementsofstatisticallearning} Decision tree - book
%\newline \par

%Why a right to explanation of automated decision-making does not exist in the general data protection regulation. \newline
%Counterfactual explanations without opening the black box \newline
%Improved use of continuous attributes in c4. 5 - decision tree

%- In machine learning it generally conveys that to present the end-users with understandable explanations regarding model working and predictions. 
%
%- Growing concerns of social, ethical, machine learning debugging and testing in machine learing raised the need of interpretablilty.

%- There are multiple ways of achieving the intpertablilty
%- Intrinsic interpretable models
%- Global explantion
%	Model-agnostic
%	Model-specific
%	DNN
%- Local explanation
%	Model-agnostic
%	Model-specific
%	DNN
%
%- It is hard to define interpretability since it is audience or applicaiton dependent
%- Properties which are helpful to define good explanation
%- It is hard to evaluate explanation methods and their explanations
%- Work has been done in proposing evaluation framework
%- Filtering the scope of work and explaining the baselines for next chapter 

%%%%Introduction

%LIME (Local Interpretable Model-Agnostic Explanations) \citep{ribeiro2016should}, and SHAP (Shapely Additive Explanations) \citep{lundberg2017unified} are few among the prominent explanation methods. These methods can interpret and debug any classifier model trained on any dataset such as tabular, text or image. In simple words; these methods can explain the models' predictions in a human-interpretable way. Despite, they lack to explain the overall behaviour of a predictive model, which remains a complex problem to solve. One proposed solution to explain the model's functionality is explaining multiple instances which capture the global perspective of a predictive model \citep{ribeiro2016should}.

%The recent need of making machine learning interpretable and debuggable has established a new sub-field, Explainable Machine Learning \citep{guidotti2018survey}. It primarily focuses on finding the answers to research questions, i.e., \textit{How to define a human-interpretable explanation? How to define the evaluation measures for explanations?} \citep{adhikari2018example, honegger2018shedding}. In the light of this, \citet{alvarez2018robustness} proposed a robustness measure to evaluate the stability of explanations generated by explanation methods. This work strives to replicate the desired functionality introduced in \citep{alvarez2018robustness} and evaluates explanation methods (LIME and SHAP) on various classification models and datasets.


%The main concern remains the same as  "if the users can trust the predictions and models". It is useful to highlight two aspects of trust here 1) \textit{"trust in the prediction"} that whether users can trust the prediction and decide based on it. 2) \textit{"trust in the model"} that whether users can trust and understand the overall behaviour of the model, not assuming it as some black-box.
%
%In addition to trust concerns, it is worthwhile to measure the model's behaviour before deploying it in the wild. Often, the accuracy metric is used to evaluate the model on unseen data, but it may not indicates the product's goal every time. Inspecting multiple instances from large datasets and their explanations can aid to reduce the blind faith on accuracy measure. Local Interpretable Model-agnostic Explanations (LIME) provides an individual explanation as a solution to \textit{"trust in the prediction"} and giving multiple such explanations as an approximate answer to \textit{"trust in the model"} problem.


%Over a hundred years ago, scientists believed in a dream of having a computer programme who can think and learn like humans. Back then visualizing it would have been a mythical desire, but it is no longer an unrealistic aspiration. Artificial intelligence (AI) is now a growing arena, impacting on every aspect of research and business. Users do not often realize, but a simple Google search involves complex AI algorithms running behind the scenes. From intelligent software to automate routine labor or to make diagnoses in medicine, AI has a wide range of active experimentation topics and practical plans.
%
%Though most of the services and products are incorporating AI enhancements, often the core definitions of AI, machine learning (ML), and deep learning (DL) conceive some confusion. \citet{Goodfellow-et-al-2016} provides thorough explanations which describe the relationship among these fields. A logical inference by a machine, based on a list of formal mathematical rules expresses an early concept of AI. In contrast to hard-coded rules, the capability of solving intuitive problems which involve an ability to learn from the knowledge, and make decisions is machine learning (ML). A simple machine learning algorithm can recommend a book based on user history, or it can filter an email as spam or legitimate.
%
%However, in real-world applications, algorithms require an in-depth representation of data to learn and decide. For instance, speech or image detection needs a sophisticated, approximately human-level recognition. Deep learning (DL) undertakes these problematic tasks; it builds a complex representation by combining simple concepts in a hierarchy, such as combining the corners and edges of a car image \citep{Goodfellow-et-al-2016}. Deep Learning also expands the horizons to solve complicated, real-time learning problems, e.g., autonomous cars, language translations, and chatbots. Such deep models are becoming more accurate in learning, though the explanation behind the predictive performance gets compromised \citep{lipton2016mythos, ribeiro2016should, lundberg2017unified}.
%
%Moreover, providing a consistent, human understandable explanation is nearly impossible when an algorithm complexity is substantial. Such models become black boxes which are hard to interpret, affecting user's trust, thus, raising many concerns about transparency and interpretability. Explainable machine learning addresses these problems and aims to make machine learning interpretable and understandable to everyday users.

%[ IMPROVEMENT REQUIRED However, it is difficult in some cases where an algorithm complexity is substantial and providing a consistent, human understandable explanation is nearly impossible. Such models become black boxes which are hard to interpret and more vulnerable especially in high-security systems. An increasing amount of black box models have raised many concerns about their interpretability. Explainable machine learning (XML) addresses these problems and aim to make machine learning interpretability and understandable. IMPROVEMENT REQUIRED ]




%\subsection{Background} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain the background of a problem.}
%\end{itemize}

% [ IMPROVEMENT REQUIRED AIqballabs.ai has supported this thesis project, which adds valuable work to provides a new explanation evaluation scheme for state-of-art explanation methods. AIqballabs.ai is a digital initiative and platform about Artificial Intelligence and its ethical values. It aims to contribute and increase awareness of implementing explainable machine learning on a practical scale. It invites professionals and students from diverse backgrounds to participate and learn about how interpretability and explainability in Artificial Intelligence impact positively on our society. IMPROVEMENT REQUIRED ]

%%%%Motivation

%An increasing amount of real-time intelligent services have established the user needs for trust and transparency. In simple cases, users simply put trust on a system and do not feel the urgency of receiving an explanation behind predictions, e.g. email spam filtering. However, in advanced applications such as recommendation systems, users desire to know the rational logic behind the predictions.
%
%On the other side, services which use machine learning models aim to provide explanations to non-technical users, but it is a challenging task. Due to lack of transparency, users perceive unclarity, which directly impacts on the trust factor and services. This research project aims to explore one of the piece of this challenging problem. It focuses on how existing explanations methods interpret machine learning classification models and how to evaluate and compare them.


%%%Explanation in machine learning

%[ IMPROVEMENT REQUIRED Increasing amount of these {\itshape Black Box} models are also raising societal and ethical concerns among communities. Intelligent machine learning models learn from the digital traces of people which serve as data features. Thus, in a case when a person needs to know a reason behind a specific decision taken by a system, which is his/her right as according to General Data Protection Regulation (GDPR) \citep{doshi2017accountability}, how these {\itshape Black Box} models contribute? Most of the these are incapable of providing an answer because they are hard to interpret.

%Problems which are significantly essential to address are most often challenging to solve. Unfortunately, in March 2018 a self-driving car hit and killed the pedestrian in Tempe, Arizona \citep{uberselfdrivingcar}. Interpreting such Deep Neural Network (DNN) models can be very tricky due to the complex architectures, e.g. architecture for a self-driving car, probably manipulates millions of neurons and their activations which are difficult to interpret. This particular example is imperative because it also highlights the urgency of solving it. \newline IMPROVEMENT REQUIRED ]

%\subsection{Explanations in the context of Machine Learning} %\subsection{}
%In explainable machine learning, explanations established interpretability and transparency of a model by answering {\itshape "why question" } behind the predictions \citep{honegger2018shedding}. \citet{honegger2018shedding} further categorizes interpretability into three types: global model interpretability, algorithm interpretability, and local model interpretability. On the other hand, \citet{lipton2016mythos} suggests establishing interpretability by breaking explanations into two pieces, transparency and post-hoc explanations. First covers {\itshape "how the model works?"} and, the latter {\itshape "how much more model can tell about the predictions?"}.
%
%These questions focus on finding possible ways to enhance user's trust in model and trust in predictions. Indeed, the answers to these questions vary and depend on the application domain. However, several explanation methods have been proposed which partially gives explanations for the model predictions. Though, due to a lack of consensus on {\itshape "what is a good explanation?"} solving these questions are challenging.

%[ IMPROVEMENT REQUIRED To provide solutions to open {\itshape Black Box} models, researchers have suggested multiple frameworks and terminologies \citep{guidotti2018survey}.  \citep{lipton2016mythos} also categories the solution into two primary ways "transparency" and "post-hoc" explanations, first one covers the question of {\itshape "how the model works"} and, the latter one try to address {\itshape "how much more model can tell about the predictions."} 
%One important thing is also discussing the difference between {\itshape interpretability } and {\itshape explanations.} \citep{honegger2018shedding} provides this differentiation by stating that explanations established interpretability of a model by answering {\itshape "why question" } behind the predictions.
%
%Moreover, interpretable machine learning models are which can provide human-understandable explanations for predictions, thus can be called as {\itshape interpretable } models in contrast to {\itshape black box} models. \newline
%For many years, researchers have been actively working and proposing various methods to mwake {\itshape black box } models interpretable \citep{guidotti2018survey}. They further organize these methods into two classes: 1) Model Dependent Explanation Methods, which make black box model interpretable but tightly coupled with specific model architecture 2) Model Agnostic Explanation Methods; these methods are generalizable, thus, applicable to multiple black box models which deal with tabular, images, textual data.
%
%Local Interpretable Model-Agnostic Explanations (LIME) \citep{ribeiro2016should} and Shapely Additive Explanations (SHAP) \citep{lundberg2017unified} are few of the most prominent agnostic models identified in the literature \citep{guidotti2018survey,honegger2018shedding}, mainly provides the framework for local model interpretability, that is, producing a human-understandable explanation for a particular prediction. These explanation methods can open complex models such as DNN and work with several kinds of datasets.\newline IMPROVEMENT REQUIRED ]


%\subsection{Explanation Methods}

%The interpretability of a model could be promoted by incorporating interpretability constraints. Some representative examples include enforcing sparsity terms or imposing semantic monotonicity constraints in classification models [14]. Here sparsity means that a model is encouraged to use relatively fewer features for prediction, while monotonicity enables the features to have monotonic relations with the prediction. Similarly, decision trees are pruned by replacing subtrees with leaves to encourage long and deep trees rather than wide and more balanced trees [29]. These constraints make a model simpler and could increase the model’s comprehensibility by users.
%
%However, there are often trade-os between prediction accuracy and interpretability when constraints are directly incorporated into models. The more interpretable models may result in reduced prediction accuracy comparing the less interpretable ones.
%
%
%Interpretable models explain a machine learning model predictions. A desideratum which lists the main elements of interpretable models includes Interpretability, Accuracy, and Fidelity. 
%
%Interpretability: to which extent the model and or the prediction are hu-
%man understandable. The most addressed discussion is related to how the interpretability can be measured. A component for measuring the interpretability is the complexity of the predictive model in terms of the model size. According to the literature, we refer to interpretability also with the name comprehensibility [27].
%
%Accuracy: to what extent the model accurately predict unseen instances.
%The accuracy of a model can be measured using various evaluation measures like the accuracy score, the F1-score [95]. Producing an interpretable model maintaining competitive levels of accuracy is the most common target among the papers in the literature.
%
%Fidelity: to which extent the model is able to imitate a black-box predictor accurately. The Fidelity captures how much is good an interpretable model in the mimic of the behaviour of a black-box. Similarly to the accuracy, the Fidelity is measured in terms of accuracy score, F1-score. However, for the outcome of the black box, which is considered as an oracle.



%Tree-based ensemble models, such as gradient boosting machines, random forests and XGBoost [7], are typically inscrutable to humans. There are several ways to measure the contribution of each feature. The first approach is to calculate the accuracy gain when a feature is used in tree branches. The rationale behind is that without adding a new split to a branch for a feature, there may be some misclassified elements, while after adding the new branch, there are two branches and each one is more accurate. The second approach measures the feature coverage,i.e., calculating the relative quantity of observations related to a feature. The third approach is to count the number of times that a feature is used to split the data.
%
%However, there are often trade-offs between prediction accuracy and interpretability when constraints are directly incorporated into models. The more interpretable models may result in reduced prediction accuracy comparing the less interpretable ones.


%Post-hoc global explanation, Mud 2019
%Machine learning models automatically learn useful patterns from a considerable amount of training data and retain the learned knowledge into model structures and parameters. Post-hoc global explanation aims to provide a global understanding about what knowledge has been acquired by these pre-trained models and illuminate the parameters or learned representations intuitively to humans. We classify existing models into two categories: traditional machine learning and deep learning pipelines (see Figure 2) since we are capable of extracting some similar explanation paradigms from each category. We introduce below how to explain these two types of pipelines.
%
%Post-hoc local explanation, Mud 2019
%After understanding the model globally, we zoom in to the local behaviour of the model and provide local explanations for individual predictions. Local explanations target to identify the contributions of each feature in the input towards a specific model prediction. As local methods usually attribute a model’s decision to its input features, they are also called attribution methods. In this section, we first, introduce model-agnostic attribution methods and then discuss attribution methods specific to DNN-based predictions.
%
%Molnar 2018
%Separating the explanations from the machine learning model (= model-agnostic interpretation methods) has some advantages (Ribeiro, Singh, and Guestrin 201626). The great advantage of model-agnostic interpretation methods over model-specific ones is their flexibility. Machine learning developers are free to use any machine learning model they like when the interpretation methods can be applied to any model. Anything that builds on an interpretation of a machine learning model, such as a graphic or user interface, also becomes independent of the underlying machine learning model. Typically, not just one, but many types of machine learning models are evaluated to solve a task, When comparing models in terms of interpretability, it is easier to work with model-agnostic explanations, because the same method can be used for any model.
%
%An alternative to model-agnostic interpretation methods is to use only interpretable models, which often has the significant disadvantage that predictive performance is lost compared to other machine learning models, and it limits to one type of model. The other alternative is to use model-specific interpretation methods. The disadvantage of this is that it also binds to one model type, and it will not be straightforward to switch to something else.
%
%Deep learning has been very successful, especially in tasks that involve images and texts such as image classification and language translation. The success story of deep neural networks began in 2012 when a deep learning approach won the ImageNet image classification challenge 65. Since then, we have witnessed a Cambrian explosion of deep neural network architectures, with a trend towards deeper networks with more and more weight parameters.
%
%To make predictions with a neural network, the data input is passed through many layers of multiplication with the learned weights and through non-linear transformations. A single prediction can involve millions of mathematical operations depending on the architecture of the neural network. There is no chance that we humans can follow the exact mapping from data input to prediction. We would have to consider millions of weights that interact in a complicated way to understand a prediction by a neural network. To interpret the behaviour and predictions of neural networks, we need specific interpretation methods. 
%
%We can certainly use model-agnostic methods, such as local models or partial dependence plots. However, there are two reasons why it makes sense to consider interpretation methods explicitly developed for neural networks: First, neural networks learn features and concepts in their hidden layers, and we need special tools to uncover them. Second, the gradient can be utilised to implement interpretation methods that are more computationally efficient than model-agnostic methods that look at the model “from the outside”. Also, most other methods in this book are intended for the interpretation of models for tabular data. Image and text data require different methods.

%\subsubsection{Lipschitz Estimation as a Faithfullness Measure}\label{sec:lipschitz_estimation_as_a_faithfullness_measure} %% \subsubsection
%Utilising the concepts provided in \citep{alvarez2018robustness}, this work proposed a new \textit{faithfullness} measure which evaluates that \textit{"What features are truly relevant?}. Let $h$ be a function which takes $x_i$ and removes or obscure features. The idea is to remove a feature of point $x_i$ to get a new point $x_i\prime$ and then calculate the lipzchitz estimate. Applying this measure shows that which explanation methods are considering which features as more relevant. 


%Quick comment on Alg. 1: it is not clear from the algorithm what is being computed there. Presumably for each $x_{i}$ in X you want to compute maximal |f(x)-f($x_{i}$)|/[x-$x_{i}$| where $x \in N(x_{i})?$ You could also explain what the numbers in Tab. 1 are. The level of detail here is such that I could replicate your experiment, if I had the data and if I wanted to. E.g., you need to be more explicit in the algorithm and also define what you mean by “aggregation” in Tab. 1.



%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explaining and defining Explanation Evaluation Framework (RQ3)}
%\end{itemize}

%\begin{definition}{Stability:}
%	If data points, \(\left\{x_{i},...,x_{n}\right\} \in X,i \in \left\{1,...,N\right\}\) are classified among predicted labels \(\left\{l_{j},...,l_{k}\right\} \in L, j \in \left\{1,...,K\right\}\), then corresponding explanations \(\left\{e_{i},...,e_{n}\right\} \in E\) should form the $K$ clusters  \(\left\{c_{j},...,c_{k}\right\} \in C\) which, follow the similar classification as predicted labels.
%	\begin{align}
%	\forall i,j,k \in N \;\;\; f\left(x_{i}\right) = f\left(x_{j}\right), \;\; f\left(x_{i}\right) \neq f\left(x_{k}\right) \Rightarrow \\
%	sim(g(f(x_{i})), g(f(x_{j}))) < sim(g(f(x_{i})), g(f(x_{k}))) 
%	 \\ symbols new line in maths
%	 \; symbols space in maths
%	\end{align}
%\end{definition}

%\subsubsection{Stability Test} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what stability means in literature}
%	\item \textit{Explain a stability evaluation test algorithm, which is developed in this research.}
%\end{itemize}

%\begin{algorithm}
%	\caption{Training Predictive Models Procedure}
%	\begin{algorithmic}[1]
%		\State Split dataset into training-testing sets.
%		\State Train predictive model \textit{'f'} using training set.
%		\State Predict labels for testing set using predictive model \textit{'f'}
%%		\State Show results
%	\end{algorithmic}
%\end{algorithm} \newline



%\begin{table}[H]
%	\caption{Aggregated Local Lipschitz Continuity}
%	\label{tab:quantitative_experimentation_iris}
%	\begin{center}
%		\begin{tabular}{@{}ccccc@{}}
%			\toprule
%			& $\epsilon$ & LIME & SHAP \\ \midrule
%			
%			% Logistic Regression, Iris
%			\multicolumn{1}{|c|}{Logistic Regression, Iris} &  \multicolumn{1}{c|}{\textit{0.75}} &  \multicolumn{1}{c|}{0.72} & \multicolumn{1}{c|}{0.33} \\ 
%			\midrule
%			
%			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.00}} &  \multicolumn{1}{c|}{0.73} & \multicolumn{1}{c|}{0.34} \\ 
%			\midrule 
%			
%			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.25}} & \multicolumn{1}{c|}{0.74} & \multicolumn{1}{c|}{0.35} \\ 
%			\midrule 		
%			
%			% Random Forest, Iris
%			\multicolumn{1}{|c|}{Random Forest, Iris} &  \multicolumn{1}{c|}{\textit{0.75}} &  \multicolumn{1}{c|}{0.91} & \multicolumn{1}{c|}{0.21} \\ 
%			\midrule
%			
%			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.00}} &  \multicolumn{1}{c|}{0.94} & \multicolumn{1}{c|}{0.22} \\ 
%			\midrule 
%			
%			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.25}} & \multicolumn{1}{c|}{0.94} & \multicolumn{1}{c|}{0.22} \\ 
%			\midrule 		
%		
%		\end{tabular}
%	\end{center}
%\end{table}

\end{document}
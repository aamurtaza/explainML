% --- Template for thesis / report with tktltiki2 class ---
% 
% last updated 2015/02/03 for tkltiki2 v1.03

\documentclass[english]{tktltiki2}

% tktltiki2 automatically loads babel, so you can simply
% give the language parameter (e.g. finnish, swedish, english, british) as
% a parameter for the class: \documentclass[finnish]{tktltiki2}.
% The information on title and abstract is generated automatically depending on
% the language, see below if you need to change any of these manually.
% 
% Class options:
% - grading                 -- Print labels for grading information on the front page.
% - disablelastpagecounter  -- Disables the automatic generation of page number information
%                              in the abstract. See also \numberofpagesinformation{} command below.
%
% The class also respects the following options of article class:
%   10pt, 11pt, 12pt, final, draft, oneside, twoside,
%   openright, openany, onecolumn, twocolumn, leqno, fleqn
%
% The default font size is 11pt. The paper size used is A4, other sizes are not supported.
%
% rubber: module pdftex

% Line spacing 
%
% Use \linespread{1.3} for "one and a half" line spacing, and
% \linespread{1.6} for "double" line spacing. Normally the lines
% are not spread, so the default line spread factor is 1.
\linespread{1.3}

% --- General packages ---

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}
\usepackage{multirow}
% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% --- Language-related settings ---
%
% these should be modified according to your language

% babelbib for non-english bibliography using bibtex
\usepackage[fixlanguage]{babelbib}
\usepackage{apalike}
\iflanguage{finnish}{\selectbiblanguage{finnish}}{}

% add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}
\iflanguage{finnish}{\settocbibname{LÃ¤hteet}}{}

% --- Theorem environment definitions ---

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

% --- tktltiki2 options ---
%
% The following commands define the information used to generate title and
% abstract pages. The following entries should be always specified:

\title{Interpreting "Black Box" models with Explanations Methods}
\author{Adnan Ali Murtaza}
\date{\today}
\level{Thesis}
\abstract{
%The challenge is that machine learning models become complicated black boxes when applied on a large scale to solve business problems. Such models are not explainable to end consumers, which raises trust, transparency and privacy concerns. Interpretability and explainability try to solve this challenge; researchers have developed explanation methods which approximate the inner working of the black box model by taking model and testing instances as an input. However, a new challenge has appeared to evaluate explanations produced by explanation methods. In this research, we present a new statistical approach based on similarity measures to evaluate explanations. We tested our framework mainly on classification (binary, multiple, multi-class) problems, including numerical, categorical and textual datasets (iris, titanic, 20-newsgroup).
}


% The following can be used to specify keywords and classification of the paper:

\keywords{Black Box, Interpretability, Agnostic Explanation Models}

% classification according to ACM Computing Classification System (http://www.acm.org/about/class/)
% This is probably mostly relevant for computer scientists
% uncomment the following; contents of \classification will be printed under the abstract with a title
% "ACM Computing Classification System (CCS):"
% \classification{}

% If the automatic page number counting is not working as desired in your case,
% uncomment the following to manually set the number of pages displayed in the abstract page:
%
% \numberofpagesinformation{16 pages + 10 appendix pages}
%
% If you are not a computer scientist, you will want to uncomment the following by hand and specify
% your department, faculty and subject by hand:
%
% \faculty{Faculty of Science}
% \department{Department of Computer Science}
% \subject{Computer Science}
%
% If you are not from the University of Helsinki, then you will most likely want to set these also:
%
% \university{University of Helsinki}
% \universitylong{HELSINGIN YLIOPISTO --- HELSINGFORS UNIVERSITET --- UNIVERSITY OF HELSINKI} % displayed on the top of the abstract page
% \city{Helsinki}
%

%%%%%%%%%%%%% Added Packages
%% package hyperref
\usepackage{hyperref}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
%\usepackage{lipsum}

%% package algorithm plus algorithmic style
%\usepackage{algorithm}
%\usepackage{algorithmic}
%
% package algorithm plus algpseudocode style
\usepackage{algorithm}
%\usepackage{arevmath}     % For math symbols
\usepackage[noend]{algpseudocode}

%% package algorithm2e style
%\usepackage{xcolor}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

%% package citations
%\usepackage{cite}
\usepackage{natbib}

%% package up greek math symbols
%\usepackage[upgreek, LGRgreek]{mathastext}
%\usepackage{bm}
\usepackage{newtxtext}
\usepackage[slantedGreek]{newtxmath}

%% package for adding properties
\usepackage{enumitem}
\newlist{properties}{enumerate}{2}
%\setlist[properties]{label=\textbf{Property} \arabic*.,itemindent=*}
\setlist[properties]{label=\textbf{Property} \arabic*, wide, labelwidth=!,labelindent=0pt}

%% package for adding sub figures side by side
\usepackage{subfig}

%\usepackage{subfigure}

%% NEW COMMANDS
% Algorithm definitions
\newcommand{\commentsymbol}{//}% or \% or $\triangleright$
\newcommand{\multicommentsymbolstart}{/*}
\newcommand{\multicommentsymbolend}{*/}
\algrenewcommand\algorithmiccomment[1]{\hfill \commentsymbol{} #1}
\makeatletter
\newcommand{\LineComment}[2][\algorithmicindent]{\Statex \hspace{#1}\commentsymbol{} #2}
\newcommand{\MultiLineComment}[2][\algorithmicindent]{\Statex \hspace{#1}\multicommentsymbolstart{} #2 \multicommentsymbolend{}}
\newcommand{\onespace}{\;}
\newcommand{\twospaces}{\;\;}
% {\rm I\!R}^n = {\mathbb{R}}^n

%% Declare math alphabets or operators
\DeclareMathAlphabet{\mathsf}{OT1}{\sfdefault}{m}{n} % newtxmath has T1
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\graphicspath{{./plots/}}

\begin{document}

% --- Front matter ---

\frontmatter      % roman page numbering for front matter

\maketitle        % title page
\makeabstract     % abstract page

\tableofcontents  % table of contents

% --- Main matter ---
\mainmatter       % clear page, start arabic page numbering
\section{Introduction} %\section{One}
%\textit{Section Scope.}
%\begin{itemize}
%	\item \textit{Start introduction.}
%	\item \textit{Go through artificial intelligence, machine learning and the need of explainable machine-learning.}
%	\item - \textit{Add figure or diagram to show the relation among these fields.}
%\end{itemize} 

%\begin{itemize}
%	
%\item This starts from quite high-flying premise. :) As a general comment, it is good to review the background, but then you should in the end explicitly state what is the topic here. The topic of this thesis is not AI or ML in general, but more specifically predictive models (i.e., supervised learning) and explanations of it.
%
%\item Someone might argue that rule-based reasoning does not necessarily involve only hard-coded rules. See, e.g., https://mitpress.mit.edu/books/constraint-based-reasoning
%
%\item Again, in this thesis, is the topic generic ML algorithms inclusive of recommender systems, or "only" supervised learning?
%\end{itemize} 

The machine learning is a core element in the advancement of science and technology. However, in comparison to the technological progress in the field, the social aspects and trust concerns are neglected. Today, a wide range of predictive machine learning models are being used in digital products and services. The main concern of the users remains the same as \textit{"can they trust the machine learning models and their predictions"}? It is essential to have trust and transparency on models because it impacts on users decisions and enhances services quality. Though providing a transparent service which gives an understandable explanation behind the predictions and models is a complicated task  \citep{lipton2016mythos}.

Such unexplainable models become black boxes to the users, affecting user's trust and raise concerns about predictive models interpretability.
On the other side, experts who build models to solve complex problems, mainly utilise standard accuracy measures to evaluate the models. These measures may not indicate the product's goal every time, and having blind faith in such measures can harm the service quality \citep{lipton2016mythos, guidotti2018survey}. Explanation methods strive to solve these issues and aim to make machine learning interpretable and debuggable \citep{guidotti2018survey}.

LIME (Local Interpretable Model-Agnostic Explanations) \citep{ribeiro2016should}, and SHAP (Shapely Additive Explanations) \citep{lundberg2017unified} are few among the prominent explanation methods. These methods can interpret and debug any classifier model trained on any dataset such as tabular, text or image. In simple words; these methods can explain the models' predictions in a human-interpretable way. Despite, they lack to explain the overall behaviour of a predictive model, which remains a complex problem to solve. One proposed solution to explain the model's functionality is explaining multiple instances which capture the global perspective of a predictive model \citep{ribeiro2016should}.


Needs mentioned above for making predictive models interpretable has established a new sub-field, Explainable Machine Learning \citep{guidotti2018survey}. It primarily focuses on finding the answers to research questions, i.e., \textit{How to define a human-interpretable explanation? How to define the evaluation measures for explanations?} \citep{adhikari2018example, honegger2018shedding}. In the light of this, \citet{alvarez2018robustness} proposed a robustness measure to evaluate the stability of explanations generated by explanation methods. This work strives to replicate the desired functionality introduced in \citep{alvarez2018robustness} and evaluates explanation methods (LIME and SHAP) on various classification models and datasets.

%The main concern remains the same as  "if the users can trust the predictions and models". It is useful to highlight two aspects of trust here 1) \textit{"trust in the prediction"} that whether users can trust the prediction and decide based on it. 2) \textit{"trust in the model"} that whether users can trust and understand the overall behaviour of the model, not assuming it as some black-box.
%
%In addition to trust concerns, it is worthwhile to measure the model's behaviour before deploying it in the wild. Often, the accuracy metric is used to evaluate the model on unseen data, but it may not indicates the product's goal every time. Inspecting multiple instances from large datasets and their explanations can aid to reduce the blind faith on accuracy measure. Local Interpretable Model-agnostic Explanations (LIME) provides an individual explanation as a solution to \textit{"trust in the prediction"} and giving multiple such explanations as an approximate answer to \textit{"trust in the model"} problem.


%Over a hundred years ago, scientists believed in a dream of having a computer programme who can think and learn like humans. Back then visualizing it would have been a mythical desire, but it is no longer an unrealistic aspiration. Artificial intelligence (AI) is now a growing arena, impacting on every aspect of research and business. Users do not often realize, but a simple Google search involves complex AI algorithms running behind the scenes. From intelligent software to automate routine labor or to make diagnoses in medicine, AI has a wide range of active experimentation topics and practical plans.
%
%Though most of the services and products are incorporating AI enhancements, often the core definitions of AI, machine learning (ML), and deep learning (DL) conceive some confusion. \citet{Goodfellow-et-al-2016} provides thorough explanations which describe the relationship among these fields. A logical inference by a machine, based on a list of formal mathematical rules expresses an early concept of AI. In contrast to hard-coded rules, the capability of solving intuitive problems which involve an ability to learn from the knowledge, and make decisions is machine learning (ML). A simple machine learning algorithm can recommend a book based on user history, or it can filter an email as spam or legitimate.
%
%However, in real-world applications, algorithms require an in-depth representation of data to learn and decide. For instance, speech or image detection needs a sophisticated, approximately human-level recognition. Deep learning (DL) undertakes these problematic tasks; it builds a complex representation by combining simple concepts in a hierarchy, such as combining the corners and edges of a car image \citep{Goodfellow-et-al-2016}. Deep Learning also expands the horizons to solve complicated, real-time learning problems, e.g., autonomous cars, language translations, and chatbots. Such deep models are becoming more accurate in learning, though the explanation behind the predictive performance gets compromised \citep{lipton2016mythos, ribeiro2016should, lundberg2017unified}.
%
%Moreover, providing a consistent, human understandable explanation is nearly impossible when an algorithm complexity is substantial. Such models become black boxes which are hard to interpret, affecting user's trust, thus, raising many concerns about transparency and interpretability. Explainable machine learning addresses these problems and aims to make machine learning interpretable and understandable to everyday users.

%[ IMPROVEMENT REQUIRED However, it is difficult in some cases where an algorithm complexity is substantial and providing a consistent, human understandable explanation is nearly impossible. Such models become black boxes which are hard to interpret and more vulnerable especially in high-security systems. An increasing amount of black box models have raised many concerns about their interpretability. Explainable machine learning (XML) addresses these problems and aim to make machine learning interpretability and understandable. IMPROVEMENT REQUIRED ]

%\subsection{Background} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain the background of a problem.}
%\end{itemize}

% [ IMPROVEMENT REQUIRED AIqballabs.ai has supported this thesis project, which adds valuable work to provides a new explanation evaluation scheme for state-of-art explanation methods. AIqballabs.ai is a digital initiative and platform about Artificial Intelligence and its ethical values. It aims to contribute and increase awareness of implementing explainable machine learning on a practical scale. It invites professionals and students from diverse backgrounds to participate and learn about how interpretability and explainability in Artificial Intelligence impact positively on our society. IMPROVEMENT REQUIRED ]

\subsection{Motivation} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Write motivation for selecting the problem.}
%	\item \textit{Write why its important to solve the issue.}
%	\item \textit{Write high-level definition of relevant sub-problem.}
%\end{itemize}

%An increasing amount of real-time intelligent services have established the user needs for trust and transparency. In simple cases, users simply put trust on a system and do not feel the urgency of receiving an explanation behind predictions, e.g. email spam filtering. However, in advanced applications such as recommendation systems, users desire to know the rational logic behind the predictions.
%
%On the other side, services which use machine learning models aim to provide explanations to non-technical users, but it is a challenging task. Due to lack of transparency, users perceive unclarity, which directly impacts on the trust factor and services. This research project aims to explore one of the piece of this challenging problem. It focuses on how existing explanations methods interpret machine learning classification models and how to evaluate and compare them.

An increasing amount of real-time intelligent services have established the user needs for trust and transparency. In simple cases, users trust a system, e.g. email spam filtering, but in large practical cases, users feel the urgency of receiving an explanation, i.e., loan rejection \citep{guidotti2018survey}. On the other side, services which aim to provide explanations to non-technical users need stable and consistent explanation methods. Failing to provide transparency can increase users concerns, which directly impacts the users' trust and services. Hence, a requirement is to make predictive models interpretable using explanation methods \cite{guidotti2018survey}. This research is motivated to explore the mechanism of comparing the explanation methods. It helps to define a framework of selecting a suitable explanation method which produces a stable explanation when showing it to users.

\subsection{Social and Ethical Concerns} %\subsection{}
By its very nature, black box models are more vulnerable especially in high-security systems it is mandatory to place a debuggable and interpretable model. Moreover, after data regulations users demands for explanations has increased, pushing companies to provide transparency. Particularly in sectors such as insurance, health or banking it is crucial to interpret predictive models which assists in decision-making. It is also important to emphasize that companies who often receive personal data consent from users appear suspicious whenever they operate opposite to users consent \citep{lipton2016mythos}. For example, annoying advertisements on Facebook based on browsing history, rejection of a bank loan applications by a system often ignite the user's critical reasoning \citep{honegger2018shedding}.

In such cases where a user needs to know the reason behind a specific decision. The ethical aspect of machine learning gives a user right to ask about an explanation. It is essential to provide and accommodate user when he/she has some discrimination concerns. European union general data protection regulation (GDPR) empowers data subject in such situations and place regulations restrictions on businesses to provide necessary explanations \citep{doshi2017accountability, goodman2017european} . Article 13 and 14 in GDPR asserts that a data subject which is included in algorithmic decision-making has the right to ask âmeaningful information about the logic involvedâ \citep{goodman2017european}.


%[ IMPROVEMENT REQUIRED Increasing amount of these {\itshape Black Box} models are also raising societal and ethical concerns among communities. Intelligent machine learning models learn from the digital traces of people which serve as data features. Thus, in a case when a person needs to know a reason behind a specific decision taken by a system, which is his/her right as according to General Data Protection Regulation (GDPR) \citep{doshi2017accountability}, how these {\itshape Black Box} models contribute? Most of the these are incapable of providing an answer because they are hard to interpret.

%Problems which are significantly essential to address are most often challenging to solve. Unfortunately, in March 2018 a self-driving car hit and killed the pedestrian in Tempe, Arizona \citep{uberselfdrivingcar}. Interpreting such Deep Neural Network (DNN) models can be very tricky due to the complex architectures, e.g. architecture for a self-driving car, probably manipulates millions of neurons and their activations which are difficult to interpret. This particular example is imperative because it also highlights the urgency of solving it. \newline IMPROVEMENT REQUIRED ]

\subsection{Explanations in the context of Machine Learning} %\subsection{}
%In explainable machine learning, explanations established interpretability and transparency of a model by answering {\itshape "why question" } behind the predictions \citep{honegger2018shedding}. \citet{honegger2018shedding} further categorizes interpretability into three types: global model interpretability, algorithm interpretability, and local model interpretability. On the other hand, \citet{lipton2016mythos} suggests establishing interpretability by breaking explanations into two pieces, transparency and post-hoc explanations. First covers {\itshape "how the model works?"} and, the latter {\itshape "how much more model can tell about the predictions?"}.
%
%These questions focus on finding possible ways to enhance user's trust in model and trust in predictions. Indeed, the answers to these questions vary and depend on the application domain. However, several explanation methods have been proposed which partially gives explanations for the model predictions. Though, due to a lack of consensus on {\itshape "what is a good explanation?"} solving these questions are challenging.

%[ IMPROVEMENT REQUIRED To provide solutions to open {\itshape Black Box} models, researchers have suggested multiple frameworks and terminologies \citep{guidotti2018survey}.  \citep{lipton2016mythos} also categories the solution into two primary ways "transparency" and "post-hoc" explanations, first one covers the question of {\itshape "how the model works"} and, the latter one try to address {\itshape "how much more model can tell about the predictions."} 
%One important thing is also discussing the difference between {\itshape interpretability } and {\itshape explanations.} \citep{honegger2018shedding} provides this differentiation by stating that explanations established interpretability of a model by answering {\itshape "why question" } behind the predictions.
%
%Moreover, interpretable machine learning models are which can provide human-understandable explanations for predictions, thus can be called as {\itshape interpretable } models in contrast to {\itshape black box} models. \newline
%For many years, researchers have been actively working and proposing various methods to make {\itshape black box } models interpretable \citep{guidotti2018survey}. They further organize these methods into two classes: 1) Model Dependent Explanation Methods, which make black box model interpretable but tightly coupled with specific model architecture 2) Model Agnostic Explanation Methods; these methods are generalizable, thus, applicable to multiple black box models which deal with tabular, images, textual data.
%
%Local Interpretable Model-Agnostic Explanations (LIME) \citep{ribeiro2016should} and Shapely Additive Explanations (SHAP) \citep{lundberg2017unified} are few of the most prominent agnostic models identified in the literature \citep{guidotti2018survey,honegger2018shedding}, mainly provides the framework for local model interpretability, that is, producing a human-understandable explanation for a particular prediction. These explanation methods can open complex models such as DNN and work with several kinds of datasets.\newline IMPROVEMENT REQUIRED ]

\subsection{Problem Statements} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Define the problem statement.}
%	\item \textit{Explain why the above problem worth exploration and solution.}
%	\item \textit{Define sub-problems}
%\end{itemize}
%The challenge is that machine learning models become complicated black boxes when applied on a large scale to solve business problems. Such models are not explainable to end consumers, which raises trust, transparency and privacy concerns. Interpretability and explainability try to solve this challenge; researchers have developed explanation methods which approximate the inner working of the black box model by taking model and testing instances as an input. However, a new challenge has appeared to evaluate explanations produced by explanation methods. In this research, we present a new statistical approach based on similarity measures to evaluate explanations. We tested our framework mainly on classification (binary, multiple, multi-class) problems, including numerical, categorical and textual datasets (iris, titanic, 20-newsgroup).

\subsubsection{Research Questions} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Formulate research questions using problem definitions.}
%	\item \textit{Formulate sub-research questions to define the scope.}
%\end{itemize}

%\begin{enumerate}
%	\item RQ: How can explanation evaluation framework enhance the interpretability in machine learning?
%	
%	\begin{enumerate}\addtocounter{enumii}{0}
%		\item RQ1: What are the current approaches to interpret machine learning models?
%		\begin{enumerate}
%			\item What are the explanation methods and what are their types?
%			\item How to interpret predictive models and extract explanations using  explanations methods?
%		\end{enumerate}
%	\end{enumerate}
%	
%	\begin{enumerate}\addtocounter{enumii}{1}
%		\item RQ2: What are the existing ways of evaluating and comparing explanations methods?
%		\begin{enumerate}
%			\item What are the limitations in evaluating explanations?
%		\end{enumerate}
%	\end{enumerate}
%	
%	\begin{enumerate}\addtocounter{enumii}{2}
%		\item RQ3: How can we perform stability test among explanations methods?
%		\begin{enumerate}
%			\item What it means to have stability in explanations?
%			\item How to define an explanation evaluation framework which measures stability?
%			\item How to evaluate the quality of an explanation evaluation framework?
%		\end{enumerate}
%	\end{enumerate}
%\end{enumerate}

\subsubsection{Research Methodology} %\subsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Write about research methodology you have used to conduct the research.}
%	\item \textit{Write the usefulness of selected methodology.}
%	\item \textit{Formulate the main steps of research methodology.}
%	\item \textit{- Design the diagram to show concept.}
%\end{itemize}

\subsection{Thesis Outline} %\subsection{}
%The thesis outline is as; Section~\ref{sec:interpretability_in_ML} gives an overview on specific properties of explanations, types of explanation methods and existing work on evaluating explanations. Section~\ref{sec:methodologies}  explains the explanation methods chosen for this research and defines the approach to evaluate the explanations generated by interpretable methods. Section~\ref{sec:experiments_and_results} defines our experimental setup and shows our experiments and results. An the end, Section~\ref{sec:conclusion} concludes the results, discusses limitations and future work.

%Section (5) tests our framework scalability, complexity by defining another experimental setup and running experiments on multi-class textual classification problem.

\section{Interpretability in Machine Learning}\label{sec:interpretability_in_ML} %\section{Two} 
%\textit{Section Scope.}
%\begin{itemize}
%	\item \textit{Explain about interpretability in machine learning.}
%	\item \textit{Highlight its importance and value.}
%	\item \textit{Explain high-level about its works and how it has been evolving in research and practice}
%\end{itemize}

\subsection{Interpretable models vs Black Box Models} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Write on interpretable models}
%	\item \textit{Write about what are black box models}
%\end{itemize}

\subsubsection{Linear models}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain linear models}
%	\item \textit{Explain how they interpret black box models}
%\end{itemize}

\subsubsection{Tree models}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain Tree models and how they work}
%	\item \textit{Brief how they interpret black box models}
%\end{itemize}

\subsection{Explanation Methods in Machine Learning} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what are explanations methods in machine learning}
%	\item \textit{Write about their research aspect and value.}
%	\item \textit{What are primary types of explanation methods}
%\end{itemize}


\subsubsection{Model-Dependent Explanation Methods} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what are model-dependent methods}
%	\item \textit{Explain what are popular methods present in literature}
%	\item \textit{Write on high-level about their working, pros and cons}
%	\item \textit{- Add Table to show the survey (RQ1 Review Literature)}
%\end{itemize}

\subsubsection{Model-Agnostic Explanation Methods} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what are model-agnostic methods}
%	\item \textit{Explain what are popular methods present in literature}
%	\item \textit{Write on high-level about their working, pros and cons}
%	\item \textit{- Add Table to show the survey (RQ1 Review Literature)}
%\end{itemize}

\subsection{Properties of Explanations} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain the challenge to quantify the explanations}
%	\item \textit{Explain the properties of good explanation present in literature}
%	\item \textit{Write on high-level about their working, pros and cons}
%	\item \textit{- Add Table to show the survey (RQ2 Review Literature)}
%\end{itemize}

\subsection{Evaluating Explanations} %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Highlight how research has been doing work on evaluating explanations}
%	\item \textit{Write explanation evaluation schemes present in literature}
%	\item \textit{Explain their mechanism, pros and cons.}
%	\item \textit{- Add Table to show the survey (RQ2 Review Literature)}
%\end{itemize}

\section{Methodologies}\label{sec:methodologies} %\section{Three}
%\textit{Section Scope.}
%\begin{itemize}
%	\item \textit{Communicate about primary methodologies which are selected}
%\end{itemize}
This section explains the methodologies which are carried from research to measure explanations robustness. Explanations are generated using methods; Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP) \citep{ribeiro2016should,lundberg2017unified}. To measure the robustness of explanations, local Lipschitz Continuity technique \citep{alvarez2018robustness} is implemented and tested out on UCI classification datasets.

On a high-level, the steps work as follow. 1) Predictive models are trained to perform classification tasks; these models behave as black-box models in the process. 2) Explanation methods interpret black-box models to find the reasons behind the predictions. 3) Explanations robustness is measured using local Lipschitz estimation technique \citep{alvarez2018robustness}.

Let $f_c: X \mapsto y$ as a black-box classifier who classifies $x \in X; X = {\mathbb{R}}^d$ to class $y, y \in \{c_1, \dots, c_n\}$. Now, let $z \in X$ be an instance from input space which requires an explanation for a prediction $f_c(z) = c_z$. An explanation model $f_e(z)$ outputs an explanation feature vector $ e = {\mathbb{R}}^d$ for which the estimation of robustness or stability is required. The main objective of this section is to define the notion of stability using local Lipschitz esitmation which requires a neighborhood $N_{\epsilon}(z)$ around an instance $z \in X$. Verbally, the notion of stability is the estimation of \textit{"how similar are the explanations for similar instances"} and vice versa indicates the notion of unstabiliy.

A detailed explanation for each explanation methods (LIME and SHAP) is presented in Section~\ref{sec:LIME} and Section~\ref{sec:SHAP}, respectively. Section~\ref{sec:defining_similarity} defines similarity measure and Section~\ref{sec:defining_explanation_evaluation_framework} formalizes the explanation evaluaton framework which defines the robustness measures and presents the algorithmic design used in the implementation.

%\subsection{Predictive Models}%\subsection{}
%%\textit{Sub-section Scope.}
%%\begin{itemize}
%%	\item \textit{Explain about selected training methods}
%%\end{itemize}
%
%\subsubsection{Logistic Regression} %\subsubsection{}
%%\textit{Sub-section Scope.}
%%\begin{itemize}
%%	\item \textit{Explain what is logistic regression}
%%	\item \textit{Explain how it works, learn and predict}
%%\end{itemize}
%
%\subsubsection{Random Forest} %\subsubsection{}
%%\textit{Sub-section Scope.}
%%\begin{itemize}
%%	\item \textit{Explain what is random forest}
%%	\item \textit{Explain how it works, learn and predict}
%%\end{itemize}

%\subsection{Model-Dependent Explanation Methods} %\subsection{}
%\subsection{Model-Agnostic Explanation Methods}  %\subsection{}
%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain about selected explanation methods}
%\end{itemize}

%\subsubsection{Local Surrogate (LIME)} %\subsubsection{}
\subsection{ LIME (Local Interpretable Model-Agnostic Explanations)}\label{sec:LIME} %\subsection{}
LIME (Local Interpretable Model-agnostic Explanations) is an explanation method which approximates the explanations for individual predictions of any classifier or regressor \citep{ribeiro2016should}. The motivation behind LIME is simple; imagine there is a black box model which is trained on some data and gives predictions for new data points. LIME tries to explain a specific prediction of the black-box model by fitting the interpretable model locally. It trains the local interpretable model in a selected local vicinity around a point of interest. A new training dataset which contains permuted samples and corresponding predictions by the black-box model is used to train and evaluate the local interpretable model. 

\subsubsection{LIME explanation mechanism} %\subsubsection{}
The generated explanations can be textual or visual artefacts that show the relationship between an instance's features and the model's prediction. A critical argument here is what if an explanation is not intelligible, therefore it is essential to make it faithful and understandable. figure~\ref{fig:ribeiro2016_lime_figure1} illustrates the case where a doctor can utilise the model explanation to make a decision. Here, an explanation is a small set of symptoms with some weights associated with them; green indicates positive contribution where red shows that the symptom is contributing negatively. It helps in the decision-making where a domain expert can accept or reject the explanation based on his prior knowledge.

\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{ribeiro2016_lime_figure1}
	\vspace*{-10mm}
	\caption{LIME: \citep{ribeiro2016should}: Depicts that an explanation behind the model prediction can help a doctor to make an informed decision. LIME presents the symptoms which led the model to predict that a patient has flu. As portrayed "sneeze" and "headache" contributed positively, while "no fatigue" is against the "flu" prediction. }
	\label{fig:ribeiro2016_lime_figure1}
\end{figure}

LIME \citep{ribeiro2016should} explainer focuses on mainly two essential criteria while generating explanations. The first one is that any explanation should be \textbf{interpretable}, i.e., the relationship between input variables and the output prediction should be easily understandable by users. Here, defining what is interpretable varies based on the target audience, e.g., users can be layman or machine learning experts. However, the bottom line is, an explanation should be simple enough to comprehend. Regardless of the hundreds or thousands of input variables used in the black-box model, there should be a mechanism to control the complexity of explanation.

The second one is a \textbf{local fidelity} which means that local interpretable model should be faithful in the locality around a point of prediction being explained. Though local faithfulness may not show features which are important at the global level, several explanations can be shown to give a global perspective. It is due to that LIME does not generate the faithful global explanation, which remains a complex problem to solve. Now keeping these two criteria LIME \citep{ribeiro2016should} provides a mechanism to produce an explanation which is interpretable and locally faithful using interpretable model.

\subsubsection{LIME explanation framework} %\subsubsection{}
Let $g \in G$ is an explanation model where $G$ is a class of \textit{interpretable models} such as linear models or decision trees. As explanation should be simple enough to understand so the domain of  $g$ is $\{0, 1\}^{d^\prime}$, which indicates the absence/presence of the component in its interpretable representation. Note that the original representation of an instance being explained is $x \in \mathbb{R}^d$ but to make an explanation interpretable, a binary vector representation $x^\prime \in \{0, 1\}^{d^\prime}$ is used as an interpretable representation. Moreover, $\upOmega(g)$ is used as a measure to control the complexity to ensure the \textit{interpretability} of an explanation model $g \in G$. For instance, in the case of linear models, the complexity factor can be the number of non-zero weights, whereas, it may be the depth of the tree for decision trees.

Now, let $f: \mathbb{R}^d \mapsto \mathbb{R}$ denotes a model being explained and $f(x)$ is a probability function which tells that $x$ belongs to certain class. To explain the prediction locally, $\pi_x(z)$ is used as a proximity measure between an instance $z$ and $x$ to define locality around $x$. In the original work \citep{ribeiro2016should} $\pi_x(z)$ is set to a exponential kernel, $\exp{-D(x,z)^2/\sigma^2}$ defined on some distance measure $D$ with width $\sigma$.
Finally, the explanation can be produced using \eqref{eq:lime_generate_explanaton} where $\mathcal{L}(f, g, \pi_x)$ is a measure which tells that how unfaithful $g$ is in approximating $f$ in the locality defined by $\pi_x$. The goal is to minmize the $\mathcal{L}(f, g, \pi_x)$ and keeping $\upOmega(g)$ small enough to produce understandable explanation.

\begin{equation}\label{eq:lime_generate_explanaton}
\mathsf{\upxi}(x) = \argmin_{g \in G} \mathcal{L}(f, g, \pi_x) + \upOmega(g)
\end{equation}

Since LIME \citep{ribeiro2016should} only implements a class of linear models $G$ as interpretable models $g(z^\prime)=w_g.z^\prime$, so \eqref{eq:lime_measure_unfaithfulness} shows the quantity to be minimised. Let $\mathcal{Z}$ be a dataset used to build linear model which contains sample instances $z^\prime \in \{0, 1\}^{d^\prime}$ drawn uniformly at random from nonzero elements of $x^\prime$, weighted by $\pi_x$. The labels to these sample instances are assigned by using $f(z)$, it requires first recovering $z \in \mathbb{R}^d$ to original representation from interpretable representation  $z^\prime \in \{0, 1\}^{d^\prime}$. 

\begin{equation}\label{eq:lime_measure_unfaithfulness}
\mathcal{L}(f, g, \pi_x) = \sum_{z, z^\prime \in Z} \pi_x(z)\onespace(f(z) - g(z^\prime))^2
\end{equation}

%\begin{wrapfigure}{r}{0.6\textwidth}
%	\centering
%	\vspace*{-5mm}
%	\includegraphics[width=1.0\linewidth]{ribeiro2016_lime_figure2}
%	\vspace*{-10mm}
%	\caption{Intuitive example to present an idea for LIME  \citep{ribeiro2016should}. A complex decision function f is represented as pink/blue background, which is hard to approximate using a linear model. The instance being explained is the bold red cross, surrounded by sampled instances (portrayed here by size). A function f gets predictions for all sample instances and weighs them by the proximity to the instance of interest. A dashed line is a linear model, provides a locally faithful explanation. }
%	\label{fig:ribeiro2016_lime_figure2}
%\end{wrapfigure}

\begin{figure}[H]
	\centering
	\vspace*{-2mm}
	\includegraphics[width=1.0\linewidth]{ribeiro2016_lime_figure2}
	\vspace*{-10mm}
	\caption{Intuitive example to present an idea for LIME  \citep{ribeiro2016should}. A complex decision function $f$ is represented as pink/blue background, which is hard to approximate using a linear model. The instance being explained is the bold red cross, surrounded by sampled instances (portrayed here by size). A function $f$ gets predictions for all sample instances and weighs them by the proximity to the instance of interest. A dashed line is a linear model, provides a locally faithful explanation. }
	\label{fig:ribeiro2016_lime_figure2}
\end{figure}

Figure \ref{fig:ribeiro2016_lime_figure2} shows intuition that how LIME \citep{ribeiro2016should} optimize the \eqref{eq:lime_generate_explanaton}. It depicts that instances are sampled in the vicinity of $x$ (bold red cross), the instances which are near to $x$ have more weights as compared to the ones which are far from $x$. Given this locality, a linear interpretable model provides a locally faithful explanation. Though it may not explain the complex black-box model globally, it captures explanation fairly well within the vicinity of the point of interest $x$. 
%\newline

%\subsubsection{Shortcomings of LIME} %\subsubsection{}

%G$ class of interpretable models. \newline
%$g \mapsto \{0, 1\}^{d^\prime}$ \newline
%$\mathbb{R}^d$ is a original representation. \newline
%$\{0, 1\}^{d^\prime}$ is a binary vector for interpretable representation. \newline
%$\upOmega(g)$ measure complexity of explanation (as opposed to interpretability). \newline

%$f: \mathbb{R}^d \mapsto \mathbb{R}$ denotes model being explained. \newline
%$f(x)$ is a probability function indicates that $x$ belongs to certain class. \newline
%$\pi_x(z)$ is proximity measure between an instance $z$ to $x$ to define locality around $x$. \newline
%$\pi_x(z)$ = $\exp{-D(x,z)^2/\sigma^2}$ \newline
%$\mathcal{L}(f, g, \pi_x)$ be a measure that how unfaithful $g$ is in approximating $f$ in the locality defined by $\pi_x$. \newline

%\subsubsection{Shapley Values (SHAP)} %\subsubsection{}
\subsection{SHAP (SHapley Additive exPlanation)}\label{sec:SHAP} %\subsection{}
%It works based on SHapley values (Wikipedia) -- a method in the cooperative game theory which tells how much each player has contributed in the total payout. On the high-level, SHAP treats each feature value as a player and total "payout" as an actual prediction of an instance minus the average prediction for all instances. In order to build the explanation for the framework, it is necessary first to present a brief overview of SHapley values. 

In contrast to LIME (Section~\ref{sec:LIME}), SHAP (SHapley Additive exPlanation) is a unified framework to interpret and explain individual predictions \citep{lundberg2017unified}. It works on the concepts of SHapley value \citep{shapley1953value} -- a method in the cooperative game theory. In order to build the explanation for the framework, it is necessary first to present a brief overview of SHapley values. 

\subsubsection{Overivew of SHapley value} %\subsubsection{}
SHapley value -- is a solution concept in cooperative game theory that answers how much each player contributes in a coalition and receives some payout based on the contribution \citep{shapley1953value}. The core purpose of SHapley value is to find which player is important in the cooperative game environment. Taking the idea into machine learning and interpretability context, the goal is is to find which feature value plays the most in a specific prediction. Here prediction task becomes a game; feature values are players and feature contributions are payouts.

Assume a task of predicting the price of an apartment and one specific example which has feature values as \textit{"park-nearby", "cat-banned", "area-50" and "floor-2nd"} predicts the price of \$300k for an apartment. Now, suppose the average prediction of all the instances is \$310k, which lead to the difference of \$10k price between actual prediction and average prediction. SHapley value tries to explain the difference value \$10k  and how each feature value contributed to it. In this case, it repeats all the combinations of features values and predicts the price of an apartment and then get the average prediction price. The computational cost increases exponentially with the number of feature values. One way to minimise the computational cost is to use fewer samples to calculate the contribution in all possible combinations.

\begin{wrapfigure}{r}{0.5\textwidth}
	\centering
	\vspace*{-2mm}
	\includegraphics[width=1.0\linewidth]{lundberg2017_shap_figure1}
	\vspace*{-10mm}
	\caption{SHapley value: \citep{molnarinterpretable}: All 8 combinations require to compute the Shapley value for feature value = "cat-banned".}
	\label{fig:lundberg2017_shap_figure1}
\end{wrapfigure}

%\begin{figure}[H]
%	\centering
%	\vspace*{-2mm}
%	\includegraphics[width=1.0\linewidth]{lundberg2017_shap_figure1}
%	\vspace*{-10mm}
%	\caption{SHapley value: \citep{molnarinterpretable}: All 8 combinations require to compute the Shapley value for feature value = "cat-banned".}
%	\label{fig:lundberg2017_shap_figure1}
%\end{figure}

Figure~\ref{fig:lundberg2017_shap_figure1} intuitively elaborates the idea of calculating the feature contributions. It shows all the possible combinations which are required to calculate the average marginal contribution for feature value "cat-banned". Each of the combinations predicts the price with or without \textit{"cat-banned"} value. Then the difference between predicted prices gives the marginal contribution for feature value "cat-banned".  Averaging the marginal contribution would give the Shapley value for feature value "cat-banned".  Similarly, Shapley values for all other feature values can be estimated to get the complete distribution of the prediction (minus the average) among the feature values.


\subsubsection{SHAP explanation framework} %\subsubsection{}
Keeping the concepts of SHapley value \citep{shapley1953value}, SHAP \citep{lundberg2017unified} introduced a new class of additive feature attribution methods (Definition~\ref{def:SHAP_additive_class}), which unifies six different explanation methods. By applying game theory concept, SHAP guarantees that there is a unique solution to a new class which helps to measure the unified SHAP values, approximated by various methods. SHAP represents an additive feature attribution method, as a linear model \eqref{eq:SHAP_additive_class}, which enables the connectivity of various explanation models, including LIME \citep{ribeiro2016should} within the SHAP framework.

\begin{definition}\label{def:SHAP_additive_class}{\textbf{Additive feature attribution methods} \textit{have an explanation model that is a linear
function of binary variables:}}
\end{definition}

\begin{equation}\label{eq:SHAP_additive_class}
	g(z^\prime) =  \phi_0 + \sum_{i=1}^{m} \phi_i z_i^\prime 
\end{equation}
where $g$ is an explanation model, $z^\prime \in \{0,1\}^{M}$ is a simplied feature vector where 0 indicates the absence of feature value and 1 indicates the presence. $M$ is the number of simplified input features and $\phi_i \in \mathbb{R}$ is the feature attribution for a feature $i$, the Shapley values. SHAP proposed a way to transform the underlying interpretable models into ~\eqref{eq:SHAP_additive_class} and then unifies explanation method who satisfies three desirable properties (given below) of Shapley values \citep{molnarinterpretable}.

The first desireable property is \textit{local accuracy}, it measures that how well an explanation method is approximating the output of function $f$ for a simplified input $x^\prime$. $x^\prime$ corresponds to a orignal instance being explained $x$ and $f$ is a black-box model which predicts an output for $x$.

\begin{properties} \label{prop:all_properties}
\item Local Accuracy:
\begin{align*}
	f(x) = g(x^\prime) =  \phi_0 + \sum_{i=1}^{m} \phi_i x_i^\prime 
\end{align*}
Here, an explanation model $g(x^\prime)$ matches the original model $f(x)$ when $x = h_x(x^\prime)$ where function $h_x$ transforms the simplified input $x^\prime$ to original instance $x$.
%%If you define $\phi_0 =  E_X(\hat{f}(X))$ and set all $x_i^\prime$ to 1, this is the Shapley efficiency property. Only with a different name and using the coalition vector.
%\begin{align*}
%	f(x) =  \phi_0 + \sum_{i=1}^{m} \phi_i x_i^\prime = E_X(\hat{f}(X)) + \sum_{i=1}^{m} \phi_i
%\end{align*}
The second desireable property is \textit{missingness}. It applies the constraints to features that when $x^\prime = 0$ then it should have no attributed impact.
\item Missingness
\begin{align*}
	x_i^\prime = 0 \implies \phi_i = 0
\end{align*}
%Missingness says that a missing feature gets an attribution of zero. or even the contribution stays the same regardless of the other inputs,
The third property is \textit{consistency}. It states that if some changes in a model increase the input's contribution, it should not decrease the input's attribution. In the context of a Shapley value, it means that if a model changes increase the marginal contribution of a feature value, or even the marginal contribution remains the same (regardless of the other features), then the Shapley value of the feature should not decrease, infact it should also increase or atleast stays the same.

\item Consistency
Let $f_x(z_i^\prime) = f(h_x(z_i^\prime))$, and $z^\prime_{\setminus i}$ indicates that $z^\prime_i = 0$. For any two models $f$ and $f^\prime$ that satisfy:
\begin{align*}
	f^\prime_x(z_i^\prime) - f^\prime_x(z^\prime_{\setminus i}) \geq f_x(z_i^\prime) - f_x(z^\prime_{\setminus i})
\end{align*}
for all inputs $z^\prime \in \{0,1\}^{M}$, then $\phi_i(f^\prime, x) \geq \phi_i(f, x)$
\end{properties}

\subsection{DeepExplain Framework}\label{sec:DeepExplain}
% simonyan2013deep - Saliency 
% sundararajan2017axiomatic - Int-Input (Not included in experiments) 
% bach2015pixel - e-LRP
% zeiler2014visualizing - Occlusion
% shrikumar2017learning - DeepLIFT


DeepExplain Framework \citep{ancona2017towards} provides the unified way of comparing attribution methods which examine the flow of Deep Neural Networks (DNNs). It mainly studies gradient-based methods \citep{simonyan2013deep, shrikumar2016not, bach2015pixel, sundararajan2017axiomatic, shrikumar2017learning} and re-reformulates two of them \citep{bach2015pixel, shrikumar2017learning} in order to provide a simpler implementation. Moreover, it proposes an evaluation metric, called Sensitivity-n, which presents empirical comparisons among gradient-based and perturbation-based \citep{zeiler2014visualizing} attribution methods. 


%There are several attribution methods which calculate attributions or contributions of input features in Deep Neural Networks (DNNs).  However, they lack to provide comprehensive comparisons. It is mainly due to different styles of problem formulations, which makes them incompatible with a variety of DNN architectures. DeepExplain framework \citep{ancona2017towards} primarily studies these attribution methods and analyses how they assign attributions to input features. 

%It gives a unified framework which shows that several attribution methods \citep{simonyan2013deep, shrikumar2016not, zeiler2014visualizing, bach2015pixel, sundararajan2017axiomatic, shrikumar2017learning} strongly relate to each other. Thus, it reformulates the problem definitions and introduces a new evaluation metric, called Sensitivity-n, to perform empirical comparisons among them. In a theoretical perspective, the framework proves the condition of equivalence among various methods and then re-implements a few methods \citep{bach2015pixel, shrikumar2017learning}. 

On a high-level, the goal of attribution methods is to explain predictions of DNN by examing that which input features help to activate a target neuron. Consider a DNN that takes an input of $x = [x_i, ..., x_n] \in \mathbb{R}^n$ and outputs $S(x) = \left[S_{1}(x), ..., S_{c}(x)\right]$, where $C$ is a total number of output neurons. An attribution method tries to calculate the contribution $R^c = \left[R_{1}^c, ..., R_{n}^c\right] \in \mathbb{R}^n$, given a target neuron of interest. In a classification case, a target neuron of interest would be an output neuron associated with the correct class label. The attribution values assigned to input features together form the attribution maps, such as shown in figure~\ref{fig:ancona2017_deepexplain_figure1}. They resemble the input shape of a given sample and display heatmaps, where red colour indicates input features which contribute to the activation of target neuron and blue colour indicates the opposite.

%DeepExplain framework mainly formulises two categories of attribution methods; 1) Perturbation-based methods and 2) backpropagation-based methods. The first one compute attributions directly by removing or altering input features.

\subsubsection{Perturbation-based Methods} %\subsubsection{}
Perturbation-based methods compute attributions directly by removing or altering input features. These methods run a forward pass for every new input obtained and then measure the difference with an original input. The domain of image classification has widely adopted this technique, particularly for interpreting Convolutional Neural Networks (CNN) \citep{zeiler2014visualizing}. It visualises the probability of a correct class as a function of grey patch position, occluding parts of an image. Moreover, the number of features strongly influence in these methods, due to which they perform slowly as the number of features grow \citep{zintgraf2017visualizing}, as shown in figure~\ref{fig:ancona2017_deepexplain_figure1}.

\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{ancona2017_deepexplain_figure1}
	\vspace*{-8mm}
	\caption{DeepExplain: \citep{ancona2017towards}: It shows the generated attributions which occlude parts of an input image with grey squared patches of different sizes. As shown, the patch size strongly influences the outcome and the bigger patch size focuses more on the main subject.}
	\label{fig:ancona2017_deepexplain_figure1}
\end{figure}

DeepExplain only includes \textit{occlusion-1} method from \citep{zeiler2014visualizing} and uses it as a comparison benchmark for perturbation-based methods. It replaces one feature at a time with a zero baseline (it is consistent in the literature) and measures the effect on the target output \citep{ancona2017towards}. For instance, $S_{c}(x) - S_{c}(x_{[x_i=0]})$, where $x[x_i=v]$ is a sample $x \in R^n$  which replaces $v$ to $i-th$ component of a sample.

\subsubsection{Gradient-based Methods} %\subsubsection{}
In contrast to perturbation-based methods, gradient-based methods are faster because they hardly relate directly to the variation of output. It is because that they generate attributions for all input features in a single forward and backwards pass in a given DNN. DeepExplains primarily analyses following gradient-based methods, which are also used in our experiments \ref{sec:interpret_convolutional_neural_network}.

\begin{itemize}
	\item Grad*Input: It computes the attribution by taking the partial derivatives of the output concerning input and then multiplies the input with the result. It improves the sharpness of the attribution maps, shown in \citep{shrikumar2016not}.
	
	\item Integrated Gradient: Similar to Grad*Input, it calculates the attribution by taking the partial derivatives of the output for input. The main difference is that it takes an average gradient while the input varies along the linear path in the network. See \citep{sundararajan2017axiomatic} for mathematical definition. .
	
	\item e-LRP: This method estimates the relevance of each input feature for target neuron. It works in a backward fashion, starts from an output layer and assigns relevance of the target neuron equal to the output of neuron itself. The algorithm proceeds layer by layer from output to input layer, redistributing the prediction score in the process. Refer to \citep{bach2015pixel} for mathematical details.
	
	\item DeepLift: It computes the attributions with a backward pass over the network, similar to LRP. The attribution value assigned to a unit represents its activation in the network with respect to some baseline unit. Refer to \citep{shrikumar2017learning} for mathematical details.
	
	\item Saliency: It estimates the attributions by taking the absolute value of the partial derivative of a target output for the input features. It indicates those input features which perturbs for the target output. However, the absolute value prevents the detection of positive and negative evidence that might be present in the input \citep{simonyan2013deep}.
\end{itemize}

In comparison to figure~\ref{fig:ancona2017_deepexplain_figure1}, figure~\ref{fig:ancona2017_deepexplain_figure2} shows that all gradient-based methods capture the higher variance in input features while generating the attributions. 

\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{ancona2017_deepexplain_figure2}
	\vspace*{-8mm}
	\caption{DeepExplain: \citep{ancona2017towards}: Several gradient-based methods generating attributions for natural image classification and Inception V3 network architecture.}
	\label{fig:ancona2017_deepexplain_figure2}
\end{figure}


%Saliency \citep{simonyan2013deep, shrikumar2016not, bach2015pixel, sundararajan2017axiomatic, shrikumar2017learning}
%
%Grad*Input.
%
%e-LRP.
%
%DeepLift.


%perturbation-based methods and gradient-based methods.

% It tests and shows comparisons on several DNN architectures and datasets (image and text classification).

\subsection{Defining Similarity}\label{sec:defining_similarity} %\subsubsection{}
Now moving forward, the next step is to specify a similarity measure. It is necessary for establishing the explanation evaluation framework which can measure the explanations generated by LIME (Section~\ref{sec:LIME}), SHAP (Section~\ref{sec:SHAP}) and DeepExplain (Section~\ref{sec:DeepExplain}). One trivial way to define a similarity measure is utilising euclidean distance measure. It is an ordinary straight line distance between two points in euclidean space. For instance, in euclidean n-space the distance between two points $x = \{x_1, x_2, ..., x_n\}$ and $y = \{y_1, y_2, ..., y_n\}$ is:

\begin{equation}\label{eq:euclidean_distance}
d(x,y) = d(y,x) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
\end{equation}

Using euclidean distance, it is straightforward to define a small neighbourhood around any instance. However, estimating neighbourhood is challenging task itself, but euclidean distance allows us to find similar points trivially. The Definition~\ref{def:1} and Definition~\ref{def:2} formalize the notion of defining a ball of radius $r$ centered at instance $a$ as a neighbourhood policy. The subtle difference between the definitions is that closed balls contain all "exterior" points as compared to open balls.

\begin{definition}\label{def:1}{Let $a \in {\mathbb{R}}^n$: Then the \textbf{Open Ball centered at $a$ with Radius $r > 0$} denoted $B_{r}(a)$ is defined to be a set of all points $x \in {\mathbb{R}}^n$ such that $||x-a|| < r$, that is, $\{x \in {\mathbb{R}}^n |\;||x - a|| < r\}.$}
\end{definition}
Now, if n=2 then for each $a = (a_1,a_2) \in {\mathbb{R}}^2$ and $r \in {\mathbb{R}}^2$ where $r > 0$ we have an open ball centered at $a$ with radius $r$ is:
\begin{align*}
B_{r}(a) = \{x \in {\mathbb{R}}^2\ : \sqrt{(x_1 - a_1)^2 + (x_2 - a_2)^2} < r\}
\end{align*}

%% Definition of closed ball
\begin{definition}\label{def:2}{Let $a \in {\mathbb{R}}^n$: Then the \textbf{Closed Ball centered at $a$ with Radius $r > 0$} denoted $B_{r}(a)$ is defined to be a set of all points $x \in {\mathbb{R}}^n$ such that $||x-a|| \leq r$, that is, $\{x \in {\mathbb{R}}^n |\;||x - a|| \leq r\}.$}
\end{definition}
And, if n=2 then for each $a = (a_1,a_2) \in {\mathbb{R}}^2$ and $r \in {\mathbb{R}}^2$ where $r > 0$ we have the closed ball centered at $a$ with radius $r$ is:
\begin{align*}
B_{r}(a) = \{x \in {\mathbb{R}}^2\ : \sqrt{(x_1 - a_1)^2 + (x_2 - a_2)^2} \leq r\}
\end{align*}

Now, a neighbouhood term can be defined as below \eqref{eq:neighbourhood} using above definitions and distance measure \eqref{eq:euclidean_distance}. It is useful especially relevant for a next section where neighbourhood policy is required to measure the \textit{robustness} for explanations. 

\begin{equation}\label{eq:neighbourhood}
N_{\epsilon}(x_i) = \{x_j \in X| \onespace d(x_i, x_j) \leq \epsilon\}
\end{equation}  

Here, $N_{\epsilon}(x_{i})$ is ball of radius $\epsilon$ centered at any point of interest $x_i$. It is noticeable that we used a closed ball Definiton~\ref{def:2} because \citep{alvarez2018robustness} used the same form in the original work.

\subsection{Defining Explanation Evaluation Framework}\label{sec:defining_explanation_evaluation_framework} %\subsection{}

Now, let us define the robustness or evaluation measures using the above explanations. Intuitively, the idea is to have the measures which seek a variability in the explanations generated by interpretable methods for similar inputs. If a variation in inputs is subtle enough that it does not change the model's predictions, then it should not change explanations either. \citet{alvarez2018robustness} proposes a notion of robustness measure which forms the basis of this work.

The critical argument was that the understanding of a complex model using a single point-wise explanations might lead to a false understanding. One way to address this would be looking beyond the point of interest and covering its neighbourhood to examine the behaviour of a model. Therefore a crucial property such as \textit{robustness} is required to measure the explanations generated by the interpretability methods. It ensures that an explanation of a point is roughly constant in its vicinity regardless of the models (linear, tree-based).  In a simple form, it means that explanations for similar instances should not widely differentiate.

%\begin{wrapfigure}{r}{0.5\textwidth}
%	\centering
%	\vspace*{-5mm}
%	\includegraphics[width=1.0\linewidth]{alvarez2018_robustness_figure1}
%	\vspace*{-11mm}
%	\caption{LIME and SHAP explanations for binary classifiers \citep{alvarez2018robustness}: The heatmaps show modelsâ positive-class probabilities and bar charts represent explanations (attribution values where x in green and y in purple) for predictions. It is visible that both LIME and SHAP explanations are stable for linear SVM model (top) but significantly vary for a non-linear two-layer neural network (bottom).}
%	\label{fig:alvarez2018_robustness_figure1}
%\end{wrapfigure}

\begin{figure}[H]
	\centering
	\vspace*{-2mm}
	\includegraphics[width=1.0\linewidth]{alvarez2018_robustness_figure1}
	\vspace*{-10mm}
	\caption{LIME and SHAP explanations for binary classifiers \citep{alvarez2018robustness}: The heatmaps show modelsâ positive-class probabilities and bar charts represent explanations (attribution values where x in green and y in purple) for predictions. It is visible that both LIME and SHAP explanations are stable for linear SVM model (top) but significantly vary for a non-linear two-layer neural network (bottom).}
	\label{fig:alvarez2018_robustness_figure1}
\end{figure}

Figure~\ref{fig:alvarez2018_robustness_figure1} presented in original work \citep{alvarez2018robustness} shows the explanations generated by LIME \citep{ribeiro2016should} and SHAP \citep{lundberg2017unified} for binary classifier predictions trained on a two-dimensional dataset. It explained that the explanations for a complex neural network model (bottom row) are often inconsistent and vary noticeably for neighbouring points. On the other side, explanations for linear SVM's predictions (top row) are relatively stable. This instability motivated the investigation of such a phenomenon and a need for an objective tool to quantify stability.

In light of this, Lipschitz continuity as a function stability has been suggested to measure the relative change in output concerning the inputs. It measures the substantial relative deviations (global) throughout the input space. Since in the context of interpretable methods, explanations do not require uniformity for very distant inputs. Therefore, a local notion of stability (Definition~\ref{def:3}) has been proposed in the form of local Lipschitz continuity (Hein and Andriushchenko, 2017; Weng et al., 2018), which measure the relative deviations in a small neighbourhood input space.
\begin{definition}\label{def:3}{\textit{local notion of stability}:}
	$f: X \subseteq {\mathbb{R}}^n \rightarrow {\mathbb{R}}^m$ is \textbf{\textit{locally Lipschitz}} \textit{if for every $x_0$ there exist $\delta > 0$ and $L \in {\rm I\!R}$ such that $||x - x_0|| < \delta$ implies $||f(x) - f(x_0)|| \leq L||x - x_0||$.}
\end{definition}

In the Definition~\ref{def:3}, $\delta$ and $L$ are dependent on a point of interest $x_i$. It allows quantifying the robustness of the explanations generated by a model $f$ in terms of constant $L$. Usually, this term is not known, and it requires an estimation. One straightforward way to solve it for every point of interest $x_i$ is to do as an optimization problem \eqref{eq:lipschitz_estimation}.

%Computing this quantity is a challenge itself because a function is not differentiable end-to-end. The computation needs a restricted evaluation budget, and luckily there are various approaches off the shelf to manage computation expenses, i.e., Bayesian Optimization  (Snoek et al. , 2012, and references therein).

\begin{equation}\label{eq:lipschitz_estimation}
L(x_i) = \operatorname*{max}_{x_j \in B_{\epsilon}(x_i)}  \frac{||f(x_i) - f(x_j)||_{2}}{||x_i - x_j||_{2}}
\end{equation}

Where $B_{\epsilon}(x_{i})$ is a ball of radius $\epsilon$ centered at $x_i$. It is a similar term to the one defined in \eqref{eq:neighbourhood}. 

\subsubsection{Algorithmic Desgin}\label{sec:algorithmic_desgin} %% \subsubsection
Moving towards algorithmic functionality, Algorithm~\ref{algo:lipscitz_estimation} shows the implementation for local Lipschitz estimations used in various experimental settings. It takes 1) all the test-set points $ \in X$. 2) the explanations (attribution) vectors $f_e(X)$ generated by explanation model $f_e$  for all the test-set points $\in X$. 3) the value $\epsilon$ to define the radius. It follows the intuitive strategy, iterates for each point of interest $x_i$ to find its neighbourhood (Line~\ref{algo:line:lipscitz_estimation_neighbourhood}). Then it estimates the maximal local Lipschitz value (Line~\ref{algo:line:lipscitz_estimation_calculation}) for the point of interest $x_i$ by solving the \eqref{eq:lipschitz_estimation} and outputs the computed value as $L(x_i)$ .

%%%% Package algorithm plus algpseudocode style
\begin{algorithm}[H]
	\caption{$LipschitzEstimations(X,\onespace f_e(X),\onespace \epsilon)$}
	\label{algo:lipscitz_estimation}
	\hspace*{\algorithmicindent} \textbf{Input}\textbf{:} $\{x_i,...,x_n\} \in X$\textbf{:} test set with all the points, $\onespace \{f_e(x_i),...,f_e(x_n)\} \in f_e(X)$\textbf{:} set of generated explanation vectors for all the points in a test set, $\onespace \epsilon$\textbf{:} radius threshold \\
	\hspace*{\algorithmicindent} \textbf{Output}\textbf{:} $L(x_i)$\textbf{:} local Lipschitz estimation value computed using \eqref{eq:lipschitz_estimation}
	\begin{algorithmic}[1]
%		\State $d$ = $||x_i||$
%		\State $\epsilon$ = $\epsilon * \sqrt{d}$ 
		\For{\texttt{$x_{i}$ in X}}
		\State \label{algo:line:lipscitz_estimation_neighbourhood} $N_\epsilon(x_i)\leftarrow \{x\in X\mid d(x,x_i)\le \epsilon\}$ \Comment {\eqref{eq:neighbourhood}}
		\State \label{algo:line:lipscitz_estimation_calculation} $L(x_i) \gets \operatorname*{max}_{x_j \in N_\epsilon(x_i)} \frac{||f_e(x_i) - f_e(x_j)||_2}{||x_i - x_j||_2}$ \Comment {\eqref{eq:lipschitz_estimation}}
		\EndFor
		\MultiLineComment[0\dimexpr\algorithmicindent]{local lipschitz estimation value for a point of interest $x_i$}
		\State \textbf{return} $L(x_i)$
		%		\EndProcedure
	\end{algorithmic}
\end{algorithm}

To be more elaborative, Line~\ref{algo:line:lipscitz_estimation_neighbourhood}
takes 1)  point of interest $x_i$, which requires a local Lipschitz estimation. 2) all the test-set points $ \in X$. 3) the value $\epsilon$ to define the radius. Given the inputs, it calculates the Euclidean distances from point $x_i$ to all points in $X$. If the distance is less than or equal to $\epsilon$ then the point $x_j \in X$ is considered to be in a neighbourhood $N_{\epsilon}(x_i)$. Once the neighrbouring points are found for any point $x_i$, a local Lipschitz estimation value can be calculated (Line~\ref{algo:line:lipscitz_estimation_calculation}) and after the calculation a value is returned.

\subsubsection{Lipschitz Estimation as a Stability Measure}\label{sec:lipschitz_estimation_as_a_stability_measure} %% \subsubsection
In the beginning, Section~\ref{sec:defining_explanation_evaluation_framework} mentions that local Lipschitz estimation as a stability function measures relative deviations inside the neighbourhood input space. To support the argument, figure~\ref{fig:alvarez2018_robustness_figure2} from \citep{alvarez2018robustness} helps to demonstrate that how $L(x_i)$ computed by above mechanism describe the relative deviations.

The idea is to find the worst-case deviation point $x_j$ which maximise the \eqref{eq:lipschitz_estimation} for point of interest $x_i$. For instance, in figure~\ref{fig:alvarez2018_robustness_figure2} from \citep{alvarez2018robustness} the examples from the BOSTON dataset show that the point of interests are incredibly similar, but their explanations by each method for the modelâs prediction vary considerably. It also depicts that maximizing \eqref{eq:lipschitz_estimation} for $x_i$ measures relative deviation and allows us to enforce the robustness mechanism on explanations methods.
\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{alvarez2018_robustness_figure2}
	\vspace*{-10mm}
	\caption{LIME and SHAP explanations deviation examples: \citep{alvarez2018robustness}: \textbf{Top}: example $x_i$ from the BOSTON dataset and its explanations (attributions). \textbf{Bottom}: explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
	\label{fig:alvarez2018_robustness_figure2}
\end{figure}


\subsubsection{Lipschitz Estimation as a Separability Measure}\label{sec:lipschitz_estimation_as_a_separability_measure} %% \subsubsection
An example presented in the above section shows the case where generated explanations widely differ for similar inputs. Another question appears that how to identify the cases where difference in explanations can serve as a useful quantity. For example, in a classification case, multiple instances belong to different class labels. A well-trained predictive model classifies the instances correctly into corresponding class labels. Explanations methods interpret the predictive model and give explanations behind the predictions. Now, how to measure the difference in explanations for two different correctly classified instances?
%what is the way to evaluate that the instances which are classified correctly to two different classes are having a different explanations?

Similar to the idea presented in \citep{alvarez2018robustness}, which uses Lipschitz continuity as a stability function. We further utilise the concept to define Lipschitz Estimation as a Separability Measure; given below:

\begin{equation}\label{eq:lipschitz_estimation_as_a_separability_measure}
L(x_i) = \min_{x_j \notin B_{\epsilon}(x_i)}  \frac{||f(x_i) - f(x_j)||_{2}}{||x_i - x_j||_{2}}
\end{equation}

Where $B_{\epsilon}(x_{i})$ is a ball of radius $\epsilon$ centered at $x_i$.

It similarly measures the relative deviation in explanation regarding inputs, but looks outside the neighbourhood points and then minimise the estimation. As a result, it indicates the separability in generated explanations that which explanations are relatively similar, but their inputs are widely different in feature values. Figure~\ref{fig:iris_rf_deviation_separability} from the following experiments presents a worst-deviation example to show the usefullness of separability measure.

\section{Experiments and Results}\label{sec:experiments_and_results} %\section{Four}
This section presents the experiments conducted to investigate the phenomenon of relative deviation in explanations regarding inputs using the robustness measure (Section~\ref{sec:methodologies}). It mainly constitutes two experimental parts. The first part uses \textit{LIME} and \textit{SHAP} to interpret random forest and logistic regression models trained on the UCI classification datasets (iris, ionosphere, glass). The second part uses explanation methods (\textit{Saliency, Grad*Input, e-LRP, Occlusion, Deeplift}) provided by \textit{DeepExplain} framework to explain the predictions of the convolutional neural network model trained on MNIST handwritten digits dataset.

The main objective is to utilise the explanation evaluation measure to quantify the instability in generated explanations. Section~\ref{sec:interpret_radnom_forest_and_logistic_regression} aims to present and explains the results of the first part. Section~\ref{sec:interpret_convolutional_neural_network} shows the results of the second part.  On a high-level, there are four components in each of the experiments pipeline 1) datasets and preparation, 2) training and evaluating predictive models, 3) building explanation models and 4) evaluating explanations. In the end, Section~\ref{sec:results_summary} summarises the results.

\subsection{Interpreting Random Forest and Logistic Regression}\label{sec:interpret_radnom_forest_and_logistic_regression}
In the first part of the experiments, the robustness of the black-box interpretability methods \textit{(LIME \& SHAP)} is calculated. Three benchmark classification datasets are used to train and interpret black-box models. For each dataset, the pipeline is 1) split the dataset into train:test sets by 80:20 ratio, 2) train random forest and logistic regression classifiers on them, 3) explain the prediction using explanation methods, and 4) calculate local robustness of explanations using measures defined in Section~\ref{sec:defining_explanation_evaluation_framework} for all the points in the test set.

\subsubsection{Datasets}
To perform the quantitative experiments, following three benchmark classification datasets are used.
\begin{center}
	\begin{itemize}
		\item \textit{Iris}: It is a well-known classification task of plants based on four flower leaf characteristics. It has 150 data points and three classes.
		\item \textit{Ionosphere}: It is radar data which contains the values of 16 high-frequency antennas that transmitted power on the order of 6.4 kilowatts. There are two targets values electrons in the ionosphere; "Good" or "Bad". The first indicates that there is some structure in the ionosphere, and the latter represents that there is no structure.
		\item \textit{Glass}: It is a popular classification dataset which contains the values for attributes, i.e. sodium, magnesium etc. 
		Each array of values has some specific label which represents the glass category.
	\end{itemize}
\end{center}

Table~\ref{table:datasets_UCI} shows the attributes for each dataset. Iris contains a total of 150 instances with the number of attributes equal to four and the total number of class labels equal to three. Ionoshpere contains a total of 351 instances, each of which has 34 attribute values and "Good" or "Bad" class label. Glass dataset has a total of 214 instances and a total of 7 class labels; each instance has one specific class label and a total of 9 attribute values.

\begin{table}[H]
	\caption{UCI Benchmark Classification Datasets}
	\label{table:datasets_UCI}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline	
		\textbf{Datasets} & \textbf{No. of Attributes} & \textbf{No. of instances} & \textbf{No. of Classes} \\ \hline
		
		Iris  & 4 & 150 & 3 \\ \hline
		Ionoshpere  & 34 & 351 & 2 \\ \hline
		Glass  & 9 & 214 & 7 \\ \hline
	\end{tabular}
	\end{center}
\end{table}


\subsubsection{Training and Evaluating Models}
Random forest and Logistic Regression classifiers are used to train predictive models. In the case of Random Forest, \textit{"balanced\_subsample"} mode is set for setting the \textit{class\_weights}. The \textit{"balanced\_subsample"} mode computes weights based on the bootstrap sample for every tree grown. For \textit{"n\_estimators"}, multiple values \textit{(100, 200 500, 1000)} are tried to get the best performing model.

In the case of Logistic Regression, \textit{"max\_iter"} (maximum iteration) is set to 100 and \textit{solver="liblinear"}. The \textit{'liblinear'} as a optimisation algorithm is a good choice in our case because of the small sizes of datasets. The \textit{"penalty"} scheme is set to $l_2$ norm and \textit{class\_weights="balanced"}. The \textit{"balanced"} mode uses the values of class to adjust the weights automatically. The weights are inversely proportional to the class frequencies in the input data as  \textit{n\_samples/(n\_classes * np.bincount(y))}. To manage multiple class case, an \textit{"ovr"} mode is used which makes a binary problem for each label and fit a model. 

Table~\ref{table:training_models_datasets_UCI} shows fixed and grid search parameters used to train the models.


\begin{table}[H]
	\caption{Settings for Training Models}
	\label{table:training_models_datasets_UCI}
	\begin{center}
		\begin{tabular}{@{}ccc@{}}
			\toprule
			Model & Fixed Parameters & Grid Search Parameters \\ \hline

			\multicolumn{1}{|c|}{Random Forest} & \multicolumn{1}{p{40mm}|}{class\_weight=\textit{'balanced \newline subsample'}} & \multicolumn{1}{p{40mm}|}{n\_estimators=\textit{100, 200, \newline 500, 1000}} \\ \hline

			\multicolumn{1}{|c|}{Logistic Regression} & \multicolumn{1}{p{40mm}|}{max\_iter=\textit{100}, \newline solver=\textit{'liblinear'}} & \multicolumn{1}{p{40mm}|}{penalty=\textit{$l_2$},
			\newline class\_weight=\textit{'balanced'},
			\newline multi\_class=\textit{'ovr'}} \\ \hline
			

		\end{tabular}
	\end{center}
\end{table}

For each dataset, a Random Forest and a Logistic Regression classifiers are trained using the above settings for training the models. It appears that a Random Forest classifier train well on ionoshphere and glass datasets with the \textit{F1-score} of 0.86 and 1.00, respectively. On the other side, Logistic Regression classifier fit well on iris dataset. It gives the \textit{F1-score} of 0.96 as compared to \textit{F1-score} of 0.86 given by Random Forest. Table~\ref{table:training_models_F1_score_datasets_UCI} shows the \textit{F1-score} of each model.

\begin{table}[H]
	\caption{F1-scores of Classification Models}
	\label{table:training_models_F1_score_datasets_UCI}
	\begin{center}
		\begin{tabular}{@{}cccc@{}}
			\toprule
			Dataset & Random Forest Model & Logistic Regression Model \\ \hline
			
			\multicolumn{1}{|c|}{Iris} & \multicolumn{1}{c|}{0.89} & \multicolumn{1}{c|}{0.96} \\ \hline
			
			\multicolumn{1}{|c|}{Ionoshpere} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.74} \\ \hline
			
			\multicolumn{1}{|c|}{Glass} & \multicolumn{1}{c|}{1.00} & \multicolumn{1}{c|}{0.92} \\ \hline
			
		\end{tabular}
	\end{center}
\end{table}

%%% Type of glass: (class attribute) -- 1 building_windows_float_processed -- 2 building_windows_non_float_processed -- 3 vehicle_windows_float_processed -- 4 vehicle_windows_non_float_processed (none in this database) -- 5 containers -- 6 tableware -- 7 headlamps

%%% BWF = building_windows_float_processed
%%% BWNF = building_windows_non_float_processed
%%% VWF = vehicle_windows_float_processed
%%% C = containers
%%% T = tableware
%%% HL = headlamps

Moreover, the classification reports are given below for each dataset and model. These reports show the precision and recall of each model.
Table~\ref{table:rf_model_iris_report} and Table~\ref{table:lr_model_iris_report} show the classification reports of Random Forest and Logistic Regression trained on iris dataset, respectively. After having a train:test split, a total of 30 instances were selected in the test set. Table~\ref{table:rf_model_iris_report} shows that the Random Forest model classifies all the instances correctly which belong to class \textit{"setosa"}. However, it misclassifies one instance of \textit{"versicolor"} class and two instances from \textit{"virginica"} class. The precision values achieved by the Random Forest classifier are 1.0, 0.83 and 0.86, where recall values are 1.0, 0.71 and 0.92 for all the three classes: \textit{"setosa"}, \textit{"versicolor"}, and \textit{"viginica"}, respectively.  

\begin{table}[H]
	\centering
	\caption{Iris: Classification Report of Random Forest}
	\label{table:rf_model_iris_report}
	\begin{tabular}{@{}ccccccc@{}}
		\toprule
		& setosa & versicolor & virginica & precision & recall & f1-score \\ \hline
		
		\multicolumn{1}{|c|}{setosa} & \multicolumn{1}{c|}{10} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
		
		\multicolumn{1}{|c|}{versicolor} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0.83} & \multicolumn{1}{c|}{0.71} & \multicolumn{1}{c|}{0.77} \\ \hline
		
		\multicolumn{1}{|c|}{virginica} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.92} & \multicolumn{1}{c|}{0.89} \\ \hline

	\end{tabular}
\end{table}

Table~\ref{table:lr_model_iris_report} shows that the Logistic Regression model which accurately classifies all the instances of class \textit{"setosa"} and \textit{"versicolor"}. However, it misclassifies one instance of \textit{"virginica}" class. The precision values achieved by the Logistic Regression classifier are 1.0, 0.10 and 0.93, where recall values are 1.0, 0.86 and 0.10 for all the three classes: \textit{"setosa"}, \textit{"versicolor"}, and \textit{"viginica"}, respectively.

\begin{table}[H]
	\centering
	\caption{Iris: Classification Report of Logistic Regression}
	\label{table:lr_model_iris_report}
	\begin{tabular}{@{}ccccccc@{}}
		\toprule
		& setosa & versicolor & virginica & precision & recall & f1-score \\ \hline
		
		\multicolumn{1}{|c|}{setosa} & \multicolumn{1}{c|}{10} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
		
		\multicolumn{1}{|c|}{versicolor} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{6} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.92} \\ \hline
		
		\multicolumn{1}{|c|}{virginica} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{13} & \multicolumn{1}{c|}{0.93} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{0.96} \\ \hline
	\end{tabular}
\end{table}


Table~\ref{table:rf_model_iono_report} shows the report of the Random Forest model trained on ionoshpere dataset. It wrongly classifies the five instances of \textit{"Bad"} class and gives the values of 0.80 and 0.83 for precision and recall. For a class label \textit{"Good"}, it only misclassifies the four instances out of fourty-two which gives the precision and recall values of 0.91 and 0.89, respectively.

\begin{table}[H]
	\begin{center}
		\caption{Ionoshpere: Classification Report of Random Forest}
		\label{table:rf_model_iono_report}
		\begin{tabular}{@{}cccccc@{}}
			\toprule
			& Bad & Good & precision & recall & f1-score \\ \hline
			\multicolumn{1}{|c|}{Bad} & \multicolumn{1}{c|}{20} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{0.8} & \multicolumn{1}{c|}{0.83} & \multicolumn{1}{c|}{0.82} \\ \hline
			\multicolumn{1}{|c|}{Good} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{42} & \multicolumn{1}{c|}{0.91} & \multicolumn{1}{c|}{0.89} & \multicolumn{1}{c|}{0.9} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

Table~\ref{table:lr_model_iono_report} shows the report of of Logistic Regression model trained on ionoshpere dataset. In constrast to the Random Forest model, it misclassifies more instances with respect to class label \textit{"Bad"}, droping the precision and recall values to 0.76 and 0.56. It also decreases the precision of class label \textit{"Good"} but it seems that a recall value increases from 0.89 to 0.91.

\begin{table}[H]
	\begin{center}
		\caption{Ionoshpere: Classification Report of Logistic Regression}
		\label{table:lr_model_iono_report}
		\begin{tabular}{@{}cccccc@{}}
			\toprule
			& Bad & Good & precision & recall & f1-score \\ \hline
			\multicolumn{1}{|c|}{Bad} & \multicolumn{1}{c|}{13} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{0.76} & \multicolumn{1}{c|}{0.54} & \multicolumn{1}{c|}{0.63} \\ \hline
			\multicolumn{1}{|c|}{Good} & \multicolumn{1}{c|}{11} & \multicolumn{1}{c|}{43} & \multicolumn{1}{c|}{0.80} & \multicolumn{1}{c|}{0.91} & \multicolumn{1}{c|}{0.85} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

Table~\ref{table:rf_model_glass_report} and Table~\ref{table:lr_model_glass_report} show the classification reports of Random Forest and Logistic Regression classifiers trained on glass dataset. The class labels shown in the tables are the short forms of the original labels. Original labels are given below:
\begin{itemize}
	\item BWF = building\_windows\_float\_processed
	\item BWNF = building\_windows\_non\_float\_processed
	\item VWF = vehicle\_windows\_float\_processed
	\item C = containers
%	\item T = tableware
	\item HL = headlamps
\end{itemize}

Logistic Regression model misclassifies two instances of class label \textit{"BWNF"}, which decreases the precision and recall values from 1.00 to 0.91 and 0.86, respectively. Moreover, it also misclassifies one out of five instances of class label \textit{"VWF"} that drops the precision value to 0.67 and recall value to 0.8. On the other side, Random Forest model with the settings mentioned in Table~\ref{table:training_models_datasets_UCI} achieves 100\textit{\%} precision and recall for all the class labels. 

\begin{table}[H]
	\begin{center}
		\caption{Glass: Classification Report of Random Forest}
		\label{table:rf_model_glass_report}
		\begin{tabular}{@{}ccccccccc@{}}
			\toprule
			&  BWF & BWNF & VWF & C & HL & precision & recall & f1-score \\ \hline
			\multicolumn{1}{|c|}{BWF} & \multicolumn{1}{c|}{15} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{BWNF} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{14} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{VWF} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{C} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{HL} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
		\end{tabular}
	\end{center}
\end{table}


\begin{table}[H]
	\begin{center}
		\caption{Glass: Classification Report of Logistic Regression}
		\label{table:lr_model_glass_report}
		\begin{tabular}{@{}ccccccccc@{}}
			\toprule
			& BWF & BWNF & VWF & C & HL & precision & recall & f1-score \\ \hline
			\multicolumn{1}{|c|}{BWF} & \multicolumn{1}{c|}{15} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{BWNF} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0.92} & \multicolumn{1}{c|}{0.86} & \multicolumn{1}{c|}{0.89} \\ \hline
			
			\multicolumn{1}{|c|}{VWF} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0.67} & \multicolumn{1}{c|}{0.8} & \multicolumn{1}{c|}{0.73} \\ \hline
			
			\multicolumn{1}{|c|}{C} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
			
			\multicolumn{1}{|c|}{HL} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{5} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} & \multicolumn{1}{c|}{1.0} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

In conclusion, it appears that Logistic Regression fits well on iris dataset as compared to Random Forest model. On the other hand, Random Forest is a better fit on ionoshphere and glass datasets. Now, these six predicive models can be used as black-box models in further experiments.

\subsubsection{Interpreting Models using LIME \& SHAP}
Now, having predictive models as black-boxes at our disposal, the black-box interpretability models (LIME~\ref{sec:LIME} and SHAP~\ref{sec:SHAP}) can be used to interpret predictions and generate explanations. Selected iris dataset examples and predictions are used in the following demonstrations to present the concept of interpreting the black-box model.

As explained in Section~\ref{sec:LIME}, LIME only provides the framework to explain an individual prediction, and by combining multiple explanations it gives the global perspective of a black-box model. The way LIME gives an explanation is illustrative in figure~\ref{fig:lime_rf_iris_instance_explanation}. It shows an example of interpreting Random Forest's prediction for an instance of iris dataset. The class probabilities are shown and in this case the class is correctly predicted to \textit{"setosa"} with $p(setosa)=1.0$. To explain the prediction, it describes the range of values which represent the positive or negative contribution of a feature in the prediction. As a result, LIME provides an explanation vector which contains the values for each feature.
\begin{figure}[H]
	\centering
	\vspace*{5mm}
	\includegraphics[width=1.0\linewidth]{lime_rf_iris_instance_explanation.png}
	\caption{\textbf{LIME} explanation of a \textbf{Random Forest} model: The figure shows an example of explanation generated using LIME for a Random Forest prediction on a test instance of the \textbf{iris} dataset.}
	\label{fig:lime_rf_iris_instance_explanation}
\end{figure}

In contrast To Random Forest model, Logistic Regression model predicts the class label \textit{"setosa"} with the $p(setosa) = 0.85$ and gives $p(versicolor) = 0.15$ to class \textit{"versicolor"}. It is visible that both classifiers correctly predict class \textit{"setosa"}, but what made a Logistic Regression model to assign a probability of $p=0.15$ to class \textit{"versicolor"}? In the figure~\ref{fig:lime_lr_iris_instance_explanation}, it appears that the classifier does it mainly due to a \textit{"petal width"} feature. Though the prediction result is same but an interpretation is different which shows that the models treat feature values in different ways. Therefore, an explanation vector generated for a Logistic Regression prediction contains different values for each feature.
\begin{figure}[H]
	\centering
	\vspace*{5mm}
	\includegraphics[width=1.0\linewidth]{lime_lr_iris_instance_explanation.png}
	\caption{\textbf{LIME} explanation of a \textbf{Logistic Regression} model: The figure shows an example of explanation generated using LIME for a Logistic Regression prediction on a test instance of the \textbf{iris} dataset}
	\label{fig:lime_lr_iris_instance_explanation}
\end{figure} 


Figure~\ref{fig:lime_iris_instance_interpretations} tries to explain the difference more explicitly. It presents the local explanations for class \textit{"versicolor"}
for both models and shows an explanation that which specific features influence Logistic Regression model to assign some probability to a wrong class. It is illusrative in figure~\ref{fig:lime_lr_iris_instance_interpretations} that the \textit{"petal width"} feature contributes positive regarding class \textit{"versicolor"}. LIME highlights it by assigning positive value to the feature in an explanation vector.
\begin{figure}[H]
	\vspace*{0mm}
	\centering
	\subfloat[Random Forest, LIME\label{fig:lime_rf_iris_instance_interpretations}]{{\includegraphics[width=5cm,height=3.5cm]{lime_rf_iris_instance_interpret.png} }}
	\qquad
	\subfloat[Logistic Regression, LIME\label{fig:lime_lr_iris_instance_interpretations}]{{\includegraphics[width=5cm,height=3.5cm]{lime_lr_iris_instance_interpret.png} }}
	\caption{LIME explanations for class "versicolor": The figure presents the local explanations generated using LIME for both models. It shows the difference in values of explanations for class "versicolor".}%
	\label{fig:lime_iris_instance_interpretations}%
\end{figure}


Similarly, SHAP~\ref{sec:SHAP} provides a framework to interpret the black-box models and gives the explanation vectors for each instance of interest. Figure~\ref{fig:shap_iris_summary_plots} shows the summary plots generated after calculating the shapley values on the iris dataset predictions. SHAP tells that the overall feature \textit{"petal width"} is the most important in Random Forest predictions (figure~\ref{fig:shap_rf_iris_summary_plots}). However, in Logistic Regression the \textit{"petal length"} feature impacts the most in making overall predictions (figure~\ref{fig:shap_lr_iris_summary_plots}). 
\begin{figure}[H]
	\vspace*{0mm}
	\centering
	\subfloat[Random Forest, SHAP\label{fig:shap_rf_iris_summary_plots}]{{\includegraphics[width=5cm,height=3.5cm]{shap_rf_iris_summary_plot.png} }}%
	\qquad
	\subfloat[Logistic Regression, SHAP\label{fig:shap_lr_iris_summary_plots}]{{\includegraphics[width=5cm,height=3.5cm]{shap_lr_iris_summary_plot.png} }}%
	\caption{\textbf{SHAP} explanations summary plots: The figure shows the summary plots of explanations generated using SHAP for both models. It presents that SHAP gives the explanation vector for each instance of interest, and the figure contains a summary of all the explanations vectors.}%
	\label{fig:shap_iris_summary_plots}%
\end{figure}

It is apparent from the above interpretations that the explanations generated by explanation methods (LIME and SHAP) depends on a black-box model regardless of the same predictions. Moreover, the explanations vary and there is need to quantify the variations in the explanations. It helps to understand the nature of a black-box model and the importance of explanation method.

\subsubsection{Evaluating Explanations and Results}\label{sec:evaluating_explanations_and_results_part1}
As now it is established how predictive models are built and how to generated explanation vectors using LIME \citep{ribeiro2016should} and SHAP \citep{lundberg2017unified}. It is feasible to apply robustness measures defined in Section~\ref{sec:lipschitz_estimation_as_a_stability_measure} \citep{alvarez2018robustness} and Section~\ref{sec:lipschitz_estimation_as_a_separability_measure}.

Similar to the previous section, this section also uses iris dataset examples first to present the worst-deviation case (Section~\ref{sec:defining_explanation_evaluation_framework}) and then the dataset-level estimations for each model (Random Forest and Logistic Regression) and dataset (iris, ionosphere, glass).
The main ideas presented were the need to quantify the variation occurs in the explanations concerning the inputs. 1) Local Explanation Stabiliy: If inputs do not differ widely, then the corresponding explanations should not vary largely. 2) Local Explanation Separability: If explanations do not differ widely, then the corresponding inputs should not vary largely. The defined robustness measures gives a mechanism to quantify the variations and evaluate the explanation methods.

\subsubsection*{Local Explanation Stability Measure}
In figure~\ref{fig:iris_rf_deviation_stability}, an example of finding the worst-case deviation is presented. The illustration aims to show that which similar instances are having a wide difference in explanations.  Figure~\ref{fig:iris_rf_deviation_stability_shap} shows the deviation scenario of SHAP explanations when interpreting the Random Forest for a point of interest $x_i$. It shows the deviation of explanation which maximises the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}. However, the value of Lipschitz estimate ($L=0.20$) is low in this specific case, but it still captures the change of positive to a negative contribution of the feature \textit{"sepal length"}.

%https://tex.stackexchange.com/questions/42968/
%reduction-of-space-between-two-sub-figures
% https://latex.org/forum/viewtopic.php?t=28543
\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[SHAP (L=0.20)\label{fig:iris_rf_deviation_stability_shap}]{
		{\includegraphics[width=0.49\linewidth,height=7cm]{iris_rf_deviation_stability_shap.png}}}%
	\hspace*{\fill}%
	\subfloat[LIME (L=2.80)\label{fig:iris_rf_deviation_stability_lime}]{
		{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_rf_deviation_stability_lime.png}}}%
	\caption{LIME and SHAP explanations deviation examples: \textbf{Top}: example $x_i$ from the IRIS dataset and its explanations (attributions). \textbf{Bottom}: Explanations generated for the \textbf{Random Forest} model that maximising the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}%
	\label{fig:iris_rf_deviation_stability}%
\end{figure}

On the other side, figure~\ref{fig:iris_rf_deviation_stability_lime} shows the deviation scenario of LIME explanations when interpreting the Random Forest for a point of interest $x_i$. In contrast, LIME explanations show a wide deviation. It maximises the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation} and gives the value of $L=2.80$. It seems that even a slight difference in feature values of \textit{"petal length"} and \textit{"petal width"} impacts largely on LIME explanations.

%\begin{center}
%	\begin{figure}[H]
%	\includegraphics[width=1.0\columnwidth]{iris_rf_deviation_stability.png}
%		\vspace*{-5mm}
%		\caption{LIME and SHAP explanations deviation examples on interpreting random forest model: \textbf{Top}: example $x_i$ from the \textit{iris} dataset and its explanations (attributions). \textbf{Bottom}: explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%		\label{fig:iris_rf_deviation_stability}
%	\end{figure}
%\end{center}

Similarly, the deviation scenarios in explanations generated for the Logistic Regression model are presented in figure~\ref{fig:iris_lr_deviation_stability}. The first noticeable thing is that even for the same point of interest $x_i$, LIME and SHAP provides different explanations when treating Logistic Regression as a black-box model (top row). In comparison to figure~\ref{fig:iris_rf_deviation_stability}, here SHAP and LIME treat \textit{"petal width"} feature as a negative contributor. LIME explanation also shows that \textit{"sepal length"} feature contributes negatively.

Figure~\ref{fig:iris_lr_deviation_stability} also illustrates both deviation cases of LIME and SHAP explanations for Logistic Regression model. In figure~\ref{fig:iris_lr_deviation_stability_shap}, an explanation generated using SHAP maximises the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation} and gives the value $L=0.157$. It shows that the deviated instance explanation (bottom left) has \textit{"sepal width"} feature as a negative contributor instead of positive. Now, figure~\ref{fig:iris_lr_deviation_stability_lime} shows the deviation case of LIME explanations. In contrast to SHAP (figure~\ref{fig:iris_lr_deviation_stability_lime}), LIME explanations deviate largely and maximise the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation} with the value of $L=2.03$. 

\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[SHAP (L=0.157)\label{fig:iris_lr_deviation_stability_shap}]{{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_lr_deviation_stability_shap.png}}}%
	\hspace*{\fill}%
	\subfloat[LIME (L=2.03)\label{fig:iris_lr_deviation_stability_lime}]{{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_lr_deviation_stability_lime.png} }}%
	\caption{LIME and SHAP explanations deviation examples: \textbf{Top}: example $x_i$ from the IRIS dataset and its explanations (attributions). \textbf{Bottom}: Explanations generated for the \textbf{Logistic Regression} model that maximising the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}%
	\label{fig:iris_lr_deviation_stability}%
\end{figure}

%\begin{center}
%	\begin{figure}[H]
%		\includegraphics[width=1.0\columnwidth]{iris_lr_deviation_stability.png}
%		\vspace*{-5mm}
%		\caption{LIME and SHAP explanations deviation examples on interpreting logistic regression model: \textbf{Top}: example $x_i$ from the \textit{iris} dataset and its explanations (attributions). \textbf{Bottom}: explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%		\label{fig:iris_lr_deviation_stability}
%	\end{figure}
%\end{center}


Likewise, the local Lipschitz estimates as per \eqref{eq:lipschitz_estimation} are calculated for each test point of UCI datasets (iris, ionosphere, glass) used in experiments. Figure~\ref{fig:UCI_datasets_stability_estimates} illustrates the estimations performed for both predictive models on test points of datasets using LIME and SHAP. It seems that SHAP has low deviations values in explanations generated on test points of iris dataset as compared to LIME. However, the difference in the mean is more significant when the Random Forest (figure~\ref{fig:UCI_datasets_rf_stability_estimates}) model is used as a black-box instead of Logistic Regression (figure~\ref{fig:UCI_datasets_lr_stability_estimates}).

Moreover, it appears that SHAP provides less deviated explanations also on test points of ionosphere dataset. In this case, explanations generated for Random Forest (figure~\ref{fig:UCI_datasets_rf_stability_estimates}) model also gives low values of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation} as compared to Logistic Regression (figure~\ref{fig:UCI_datasets_lr_stability_estimates}). However, LIME seems to provide slightly more stable explanations on test points of glass datasets. In this case, it looks that the Logistic Regression (figure~\ref{fig:UCI_datasets_lr_stability_estimates}) model gives low values of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation} as compared to Random Forest (figure~\ref{fig:UCI_datasets_rf_stability_estimates}), though the difference in the mean values of the estimations seems equal for both models.

\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[Random forest as black-box \& estimations as per \eqref{eq:lipschitz_estimation}\label{fig:UCI_datasets_rf_stability_estimates}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{UCI_datasets_rf_stability_estimates.png}}}%
	\hspace*{\fill}%
	\subfloat[Logistic Regression as black-box \& estimations as per \eqref{eq:lipschitz_estimation}\label{fig:UCI_datasets_lr_stability_estimates}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{UCI_datasets_lr_stability_estimates.png} }}%
	\caption{Dataset-level local Lipschitz estimates as per \eqref{eq:lipschitz_estimation} computed for two predictive models on test points of various UCI classification datasets.}%
	\label{fig:UCI_datasets_stability_estimates}%
\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=1.0\linewidth]{iris_rf_robustness_lipschitz_estimates.png}
%	\vspace*{-5mm}
%	\caption{LIME and SHAP dataset level explanations on interpreting \textit{Random Forest} models: Explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%	\label{fig:iris_rf_robustness_lipschitz_estimates}
%\end{figure} 

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=1.0\linewidth]{iris_lr_robustness_lipschitz_estimates.png}
%	\vspace*{-5mm}
%	\caption{LIME and SHAP dataset level explanations on interpreting \textit{Logistic Regression} models: Explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%	\label{fig:iris_lr_robustness_lipschitz_estimates}
%\end{figure} 

The aggregated or mean values of local Lipschitz estimates in the above multiple scenarios also depict the evaluation for explanation methods (LIME and SHAP). Table~\ref{table:lipschitz_estimation_aggregated_datasets_UCI} contains these mean values of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation} computed in figure~\ref{fig:UCI_datasets_stability_estimates}.

\begin{table}[H]
	\caption{Mean values of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation} computed in Figure~\ref{fig:UCI_datasets_stability_estimates}}
	\label{table:lipschitz_estimation_aggregated_datasets_UCI}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Dataset} & \textbf{Model} & \textbf{LIME} & \textbf{SHAP} \\ \hline
			
			Iris & Random Forest & 0.94 & 0.22  \\ \hline
			& Logistic Regression & 0.70 & 0.31 \\ \hline
			
			Ionosphere & Random Forest & 0.10 & 0.08 \\ \hline
			& Logistic Regression & 0.33 & 0.12 \\ \hline
			
			Glass & Random Forest & 0.09 & 0.10 \\ \hline
			& Logistic Regression & 0.03 & 0.04 \\ \hline
			
		\end{tabular}
	\end{center}
\end{table}

It shows that a Random Forest model trained on iris and ionosphere datasets and explained by explanation method SHAP gives the lowest mean value of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation}. However, for glass dataset, a combination of the Logistic Regression model and LIME gives the lowest mean value of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation}. Here, the lowest values represents that the combinations which are likley to produce less variant explanations for similar instances.

\subsubsection*{Local Explanation Separability Measure}
Applying a local explanation separability measure reveals important results. Figure~\ref{fig:iris_rf_deviation_separability} shows an example of minimising the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure} for a point of interest $x_i$. It means that which similar explanations are having a wide difference in inputs.

Figure~\ref{fig:iris_rf_deviation_separability} shows the results when the Random Forest model is interpreted using SHAP. A model classifies both inputs correctly and assigns probability values of above 99\% to correct class labels, \textit{p("versicolor")}=0.996 (top left) and \textit{p("setosa")}=1.0 (bottom left). It appears that a model easily differentiates the difference in feature values while making predictions. However, it seems that SHAP does not provide different explanations for different class labels. It gives an almost similar explanation for these two different predictions. 

%https://tex.stackexchange.com/questions/42968/
%reduction-of-space-between-two-sub-figures
% https://latex.org/forum/viewtopic.php?t=28543
\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[SHAP (L=0.014)\label{fig:iris_rf_deviation_separability_shap}]{
		{\includegraphics[width=0.49\linewidth,height=7cm]{iris_rf_deviation_separability_shap.png}}}%
	\hspace*{\fill}%
	\subfloat[LIME (L=0.058)\label{fig:iris_rf_deviation_separability_lime}]{
		{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_rf_deviation_separability_lime.png}}}%
	\caption{LIME and SHAP explanations separability deviation examples: \textbf{Top}: example $x_i$ from the IRIS dataset and its explanations (attributions). \textbf{Bottom}: Explanations generated for the \textbf{Random Forest} model that minimising the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}.}%
	\label{fig:iris_rf_deviation_separability}%
\end{figure}


On the other side, figure~\ref{fig:iris_rf_deviation_separability_lime} shows the plot when LIME explanations are used to minimise the Lipschitz estimate for a point of interest $x_i$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}. In contrast to SHAP explanations, LIME provides better separability results. The first reason is that for LIME explanations, a deviated example is also classified to the same class label \textit{"versicolor"} as a point of interest $x_i$. The other reason is that LIME seems to capture the difference in feature values, i.e. a \textit{"sepal length"} contributes negatively in deviated instance explanation and that drops the prediction probability value from \textit{p("versicolor")}=0.996 (top right) to \textit{p("versicolor")}=0.85 (bottom right).


Similarly, figure~\ref{fig:iris_lr_deviation_separability} shows an example of applying separability measure \eqref{eq:lipschitz_estimation_as_a_separability_measure} on a setting where Logistic Regression model is used as a black-box model. Replacing a predictive model to Logistic Regression shows the drastic change in the worst-case deviation results. It seems that in this setting, both SHAP and LIME lacks a real separability in producing different explanations for inputs which are widely different in feature values.
%https://tex.stackexchange.com/questions/42968/
%reduction-of-space-between-two-sub-figures
% https://latex.org/forum/viewtopic.php?t=28543
\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[SHAP (L=0.046)\label{fig:iris_lr_deviation_separability_shap}]{
		{\includegraphics[width=0.49\linewidth,height=7cm]{iris_lr_deviation_separability_shap.png}}}%
	\hspace*{\fill}%
	\subfloat[LIME (L=0.086)\label{fig:iris_lr_deviation_separability_lime}]{
		{\includegraphics[width=0.49\linewidth ,height=7cm]{iris_lr_deviation_separability_lime.png}}}%
	\caption{LIME and SHAP explanations separability deviation examples: \textbf{Top}: example $x_i$ from the IRIS dataset and its explanations (attributions). \textbf{Bottom}: Explanations generated for the \textbf{Random Forest} model that minimising the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}.}%
	\label{fig:iris_lr_deviation_separability}%
\end{figure}

It is illustrative in figure~\ref{fig:iris_lr_deviation_separability_shap} that even being correctly predicted to two different class labels and having a significant shift in feature values, SHAP provides a similar explanation for both inputs. Figure~\ref{fig:iris_lr_deviation_separability_lime} gives slightly better value but produces similar results. It also gives similar explanations for inputs which differ widely in feature values and belong to separate class labels.

Finally, figure~\ref{fig:UCI_datasets_separability_estimates} presents the calculated local Lipschitz estimates as a separability measure for each test point of UCI classification dataset (iris, ionosphere, glass) using LIME and SHAP. In figure~\ref{fig:UCI_datasets_rf_separability_estimates}, it appears that SHAP explanations for Random Forest model and iris have low estimated separability \eqref{eq:lipschitz_estimation_as_a_separability_measure} mean value as compared to LIME explanations. However, the case is opposite if a model is a Logistic Regression. It is illustrative in figure~\ref{fig:UCI_datasets_lr_separability_estimates} that SHAP explanations have high estimated separability \eqref{eq:lipschitz_estimation_as_a_separability_measure} mean value compared to LIME explanations.

In the case of ionosphere dataset, the SHAP explanations for both predictive models result in low estimated separability \eqref{eq:lipschitz_estimation_as_a_separability_measure} mean values as compare to LIME explanations. Though if we make here a small comparison between both models, then explanations generated for Logistic Regression model follow more separability than the explanations for the Random Forest model.

\begin{figure}[H]
	%	\vspace*{0mm}
	%	\centering
	\subfloat[Random forest as black-box \& estimations as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}\label{fig:UCI_datasets_rf_separability_estimates}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{UCI_datasets_rf_separability_estimates.png}}}%
	\hspace*{\fill}%
	\subfloat[Logistic Regression as black-box \& estimations as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}\label{fig:UCI_datasets_lr_separability_estimates}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{UCI_datasets_lr_separability_estimates.png} }}%
	\caption{Dataset-level local Lipschitz estimates as per \eqref{eq:lipschitz_estimation_as_a_separability_measure} computed for two predictive models on test points of various UCI classification datasets.}%
	\label{fig:UCI_datasets_separability_estimates}%
\end{figure}

In the case for glass dataset, it looks that when a model is Random Forest, then the SHAP explanations seem to follow slightly more separability \eqref{eq:lipschitz_estimation_as_a_separability_measure}  as compared to LIME explanations (figure~\ref{fig:UCI_datasets_rf_separability_estimates}). However, the results are opposite when a black-box model is Logistic Regression. In figure~\ref{fig:UCI_datasets_lr_separability_estimates}, LIME explanations have high estimated separability \eqref{eq:lipschitz_estimation_as_a_separability_measure}  mean value compared to SHAP explanations.

In the end, Table~\ref{table:lipschitz_estimation_separability_aggregated_datasets_UCI} presents the mean values of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation_as_a_separability_measure} for all the cases shown in figure~\ref{fig:UCI_datasets_separability_estimates}. According to \eqref{eq:lipschitz_estimation_as_a_separability_measure}, the goal was to minimise the value in order to see which experimental settings produce more different explanations for different instances.

\begin{table}[H]
	\caption{Mean values of local Lipschitz estimates as per \eqref{eq:lipschitz_estimation_as_a_separability_measure} computed in figure~\ref{fig:UCI_datasets_separability_estimates}}
	\label{table:lipschitz_estimation_separability_aggregated_datasets_UCI}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Dataset} & \textbf{Model} & \textbf{LIME} & \textbf{SHAP} \\ \hline
			
			Iris & Random Forest & 0.022 & 0.016  \\ \hline
			& Logistic Regression & 0.039 & 0.067 \\ \hline
			
			Ionosphere & Random Forest & 0.025 & 0.017 \\ \hline
			& Logistic Regression & 0.085 & 0.057 \\ \hline
			
			Glass & Random Forest & 0.0012 & 0.0013 \\ \hline
			& Logistic Regression & 0.0013 & 0.0012 \\ \hline
			
		\end{tabular}
	\end{center}
\end{table}

It appears that for iris dataset a combination of Logistic Regression interpreted by SHAP outputs explanations which represent more separability compared to other combinations. For ionosphere dataset, LIME explanations follow more separability when a predictive model is Logistic Regression. For glass dataset, a Random Forest model with SHAP explainer and a LIME explainer with a Logistic Regression model gives the same mean values and on average outputs the same explanations regarding separability measure.


\subsection{Interpreting Convolutional Neural Network (CNN)}\label{sec:interpret_convolutional_neural_network}
In the second part of the experiments, a Convolutional Neural Network (CNN) trained on MNIST handwritten digits dataset is used as a black-box model. Overall the experiments pipeline is the same as used in Section 4.1, but there is a slight difference. At the first step, 1) prepare the training and test set 2) train a Convolutional Neural Network (CNN) with the best possible accuracy. 3) predictions are explained using DeepExplain framework, which comprises various neural network interpretability models (\textit{Saliency, Grad*Input, e-LRP, Occlusion, Deeplift}). 4) calculate the local robustness of explanations using measures defined in Section~\ref{sec:defining_explanation_evaluation_framework} for test points. 

\subsubsection{Dataset}\label{sec:dataset_MNIST}
The MNIST handwritten digits dataset is primarily used in performing all the experiments in the following sections. However, apart from original dataset samples, Section~\ref{sec:CNN_evaluating_explnations_and_resuls} uses a small artificial dataset created by adding Gaussian noise to showcase the results.

Table~\ref{table:datasets_MNIST} shows the attributes for MNIST dataset of handwritten digits. It contains 60,000 training examples and a test set of 10,000 examples. The total number of columns or attributes are 784, excluding the class label. Each image has a height of 28 pixels and a width of 28 pixels, which gets in a total of 784 pixels. Pixels are row-wise organised, and the pixel-value is an integer between 0 and 255, 0 means background (white), 255 means foreground (black). There are a total of 10 class labels in training examples, and each image has one label value range from  0 to 9, which represents the class of an image.

\begin{table}[H]
	\caption{MNIST Handwritten Digits Dataset }
	\label{table:datasets_MNIST}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline	
			\textbf{Attributes} & \textbf{Training examples} & \textbf{Testing examples} & \textbf{Classes} \\ \hline
			784 & 60,000 & 10,000 & 10 \\ \hline
		\end{tabular}
	\end{center}
\end{table}

As mentioned above, a new small dataset is also created for demonstrating the evaluation of explanations results. It draws a random example from a training example and samples new images by adding Gaussian noise. The noise ratio slightly gets increase during the sampling, and a total of 5 to 7 instances are finally drawn at the end of sampling.

\subsubsection{Training and Evaluating CNN}
To proceed further in experiments, a CNN trained on MNIST dataset used as a black-box model. Several models with different properties were tried out in order to get the best accuracy value of 0.99 on the test set. The selected model contains a total of five layers, two \textit{Con2D}, one \textit{Maxpooling2D} and two \textit{Dense} layers.

In the first Con2D layer, a\textit{ kernel\_size=(3,3)} and an \textit{activation='relu'} with \textit{input\_shape=(28, 28, 1)} are used along with 32 output filters. The second Con2D layer also contained a \textit{kernel\_size=(3,3)} and an \textit{activation='relu'} with a value of 64 for filters. A Maxpooling2D layer with \textit{pooling=(2,2)} and a \textit{dropout=0.25} was applied on top of the second layer to reduce the calculations of weights. To perform the classfication, a final output was flattend in order to feed it to the feed forward network who contains the last two \textit{Dense} fully-connected layers. Finally, an activation='softmax' in the output layer helps to make the classification for images.

As visible in figure~\ref{fig:cnn_training_plots}, a total of 10 epochs are used to train the network. A \textit{'rmsprop'} value is used for an \textit{opitmizer} and \textit{batch\_size} was set to 128 instances. Moreover, the figure shows the loss, accuracy and error patterns throughout the training process.

\begin{figure}[H]
	%	\vspace*{0mm}
	\centering
	\subfloat[Graph of Log-loss\label{fig:cnn_log_loss_plot}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{cnn_log_loss_plot.png}}}%
	\hspace*{\fill}%
	\subfloat[Graph of Accuracy\label{fig:cnn_accuracy_plot}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{cnn_accuracy_plot.png} }}%
	\qquad
	\subfloat[Graph of Mean sqaured error\label{fig:cnn_mse_plot}]{{\includegraphics[width=0.49\linewidth ,height=4.5cm]{cnn_mse_plot.png} }}%
	\caption{Training and evaluation of Convolutional Neural Network (CNN) on MNIST handwritten digits dataset.}%
	\label{fig:cnn_training_plots}%
\end{figure}

\subsubsection{Interpreting CNN using DeepExplain Framework}
Now, having a CNN trained model which can make accurate predictions, it can serve as a black-box model to DeepExplain framework (Section~\ref{sec:DeepExplain}). The framework contains attribution methods which help to interpret the predictions and generate attribution maps. Figure~\ref{fig:interpreting_CNN_using_DeepExplain} demonstrates these attribution maps generated for randomly selected instances from the test-set.

It is visible from figure~\ref{fig:interpreting_CNN_using_DeepExplain} that multiple attribution methods are used to generate the heatmaps. These heatmaps depict the positive and negative contributions of features in the activation of a target output. The red highlighted colour shows the positive influence of features over an activation, whereas blue indicates that these features have a suppressing effect on the activation of a target neuron. Moreover, attribution maps excluding 'saliency' look very similar to each other, but it is hard to tell the similarity or difference among them. 

\begin{figure}[H]
	\vspace*{0mm}
	\centering
	\includegraphics[width=1.0\linewidth]{digit_0_interpret.png}
%	\caption{digit 0 interpret}%
%	\label{fig:digit_0_interpret}%
\end{figure}

\begin{figure}[H]
	\vspace*{-10 mm}
	\centering
	\includegraphics[width=1.0\linewidth]{digit_1_interpret.png}
	\caption{Interpreting CNN prediction by generating attribution maps using DeepExplain framework.}%
	\label{fig:interpreting_CNN_using_DeepExplain}%
\end{figure}
 
Explanation robustness measures (Section~\ref{sec:defining_explanation_evaluation_framework}) help to such measure the variation in the attribution maps. Measuring the variation not only helps to understand the workflow of the CNN model but also evaluates the attribution methods. It tells that which attribution method best describes the right activation of neurons for a target output.

\subsubsection{Evaluating Explanations and Results}\label{sec:CNN_evaluating_explnations_and_resuls}
This section shows the evaluation of explanations generated by DeepExplain framework and presents the results. Similar to Section~\ref{sec:evaluating_explanations_and_results_part1}, the local explanation robustness measures (Section~\ref{sec:defining_explanation_evaluation_framework}) are applied to extract the valuable results. In this scenario, a trained convolutional neural network model serves as a black box and attribution methods provided by DeepExplain produce attribution maps or explanation vectors. The robustness measures used in the following experiments are 1) Local Explanation Stability: If inputs do not differ widely, then the corresponding explanations should not vary largely. 2) Local Explanation Separability: If explanations do not differ widely, then the corresponding inputs should not vary largely.

In the beginning, the experiments use an artificial dataset (section~\ref{sec:dataset_MNIST}) to generate the explanations and estimate the local explanation robustness.  Next, the experiments use the test-set and selected instances to demonstrate the results for local explanation stability and local explanation separability measures. In the end, box-plots present the dataset-level estimations of robustness measures.

Following figure~\ref{fig:digit3_noise_ratio} shows the result of applying attribution methods on a small artificial dataset. A randomly selected instance from the test set is used to generate the six versions of perturbed instances which creates the small dataset. The noise ratio varies for each perturbed instance, as shown in the first column $\sigma$ value represents the noise variation, and $p(3)$ represents the predicted class probabilities. As it is illustrative, the top row shows the original image and its corresponding attribution maps, whereas bottom rows present the attribution maps for each perturbed instance. The \& sign in the bottom rows represents the ratio $||f(x) - f(x\prime)||_2/||x - x\prime||_2$ for the perturbed instance. By maximising the ratio as per \eqref{eq:lipschitz_estimation} gives the worst-deviation case of local explanation stability measure for this small dataset.

\begin{figure}[H]
	\vspace*{0mm}
	\centering
	\includegraphics[width=1.0\linewidth]{lc_ratio_noise_digit_3.png}
	\caption{Explanations of randomly selected MNIST digit instance (top row) and six variations of it with Guassian noise. The perturbed instances are labeled with probability and noise $\sigma$ value. The \& represents a ratio $||f(x) - f(x\prime)||_2/||x - x\prime||_2$ for perturbed instance.}%
	\label{fig:digit3_noise_ratio}%
\end{figure}

The worst-deviation case is visible in figure~\ref{fig:digit3_noise_deviations}, which shows that inputs are similar but lead to different explanations. It is observed earlier that all perturbed instances are predicted correctly with 1.0 probability which tells that a CNN model easily differentiates the noise. However, it is not the case for attribution methods applied to generate the explanations. Notably, an attribution map extracted using \textit{Saliency} method appears to have a wide difference from the corresponding original map. Apart from \textit{Saliency} method, all other attribution methods seem to produce similar results, having a close range of estimated Lipschitz values $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.

\begin{figure}[H]
%	\vspace*{-5mm}
	\centering
	\subfloat[Saliency]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_Saliency.png} }}%
	\qquad
	\subfloat[Grad*input]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_Grad*input.png} }}%
	\qquad
	\subfloat[e-LRP]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_e-LRP.png} }}%
	\qquad
	\subfloat[Occlusion]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_Occlusion.png} }}%
	\qquad
	\subfloat[Deeplift]{{\includegraphics[width=3.5cm]{noise_digit_3_deviation_Deeplift.png} }}%
	\caption{Noise Dataset: Worst-case deviations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}%
	\label{fig:digit3_noise_deviations}%
\end{figure}

Similarly, figure~\ref{fig:digit3_deviations} shows the worst-case deviations, which maximise the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}. However, a neighbourhood in this scenario is all the images which have a similar label; for example, all digit images with label three. As shown in the figure~\ref{fig:digit3_deviations}, \textit{Saliency} method gives the maximum $L(x_i)$ value as compared to other methods. It indicates that the Saliency method is more likely to produce unstable explanations. On the other side, \textit{DeepLift} gives the minimum $L(x_i)$ value which makes it the most stable method as compare to others. \textit{Grad*Input} and \textit{e-LRP} methods output almost the similar values, whereas Occlusion estimates a slighly higher value.

%%% ['Saliency', 'Grad*input', 'e-LRP', 'Occlusion', 'Deeplift']
\begin{figure}[H]
	\vspace*{-5mm}
	\centering
	\subfloat[Saliency]{{\includegraphics[width=3.5cm]{digit_3_deviation_Saliency.png} }}%
	\qquad
	\subfloat[Grad*input]{{\includegraphics[width=3.5cm]{digit_3_deviation_Grad*input.png} }}%
	\qquad
	\subfloat[e-LRP]{{\includegraphics[width=3.5cm]{digit_3_deviation_e-LRP.png} }}%
	\qquad
	\subfloat[Occlusion]{{\includegraphics[width=3.5cm]{digit_3_deviation_Occlusion.png} }}%
	\qquad
	\subfloat[Deeplift]{{\includegraphics[width=3.5cm]{digit_3_deviation_Deeplift.png} }}%
	\caption{Worst-case deviations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}%
	\label{fig:digit3_deviations}%
\end{figure}

The above results are produced as the \citet{alvarez2018robustness} showed how to apply such robustness measure \eqref{eq:lipschitz_estimation} on explanations. However, in the original work, the image with label seven was used to demonstrate the results. As described earlier, extending the \citet{alvarez2018robustness}'s method, we introduced a new robustness measure \eqref{eq:lipschitz_estimation_as_a_separability_measure}. Likewise, we applied it on explanations and results are visible in the following figure~\ref{fig:digit3_deviations_separability}.

Interestingly, the results reveal different aspects regarding evaluating the explanations. In this scenario, it minimises the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure} therefore the neighbourhood was all the other images with a non-similar label, e.g., all images which do not have label three. Intuitively, the point of interest image is close to the image which should have a label eight; however, it is not the case. 

It appears the CNN model accurately differentiates and classifies the deviation images, but explanations generated by attribution methods do not have such a difference. As it is visible, \textit{Saliency} and \textit{Occlusion} confuse the explanation of the point of interest image with an explanation of an image with label two, whereas, the explanations generated through \textit{Grad*Input}, \textit{e-LRP} and \textit{Deeplift} tend to confuse it with an image of label zero.

\begin{figure}[H]
	\vspace*{-5mm}
	\centering
	\subfloat[Saliency]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_Saliency.png} }}%
	\qquad
	\subfloat[Grad*input]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_Grad*input.png} }}%
	\qquad
	\subfloat[e-LRP]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_e-LRP.png} }}%
	\qquad
	\subfloat[Occlusion]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_Occlusion.png} }}%
	\qquad
	\subfloat[Deeplift]{{\includegraphics[width=3.5cm]{digit_3_separability_deviation_Deeplift.png} }}%
	\caption{Worst-case deviations for the minimising of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}.}%
	\label{fig:digit3_deviations_separability}%
\end{figure}

In the end, the following figure~\ref{fig:mnist_robustness_lipschitz_estimates} and \ref{fig:mnist_robustness_lipschitz_estimates_separability} show dataset-level robustness by repeating the above calculation for 100 randomly selected instances. In the figure~\ref{fig:mnist_robustness_lipschitz_estimates}, it looks as expected that \textit{Saliency} method gives the highest average value for maximising the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}, making it the most unstable method among others.  On the other side, \textit{Deeplift} method gives the lowest average value, whereas the other three methods (\textit{Grad*Input}, \textit{e-LRP}, \textit{Occlusion}) lie close to each other.

On the other hand, figure~\ref{fig:mnist_robustness_lipschitz_estimates_separability} shows the values for minimising the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}. In this case, \textit{Deeplift} method gives the minimum average value, which indicates that among all other methods, it lacks the most to having a separability in explanations. Similar to the previous case, \textit{Grad*Input}, \textit{e-LRP} and \textit{Occlusion} methods output values which are close to each other. \textit{Saliency} method shows the most separability in explanations by giving the highest average value.

\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{mnist_robustness_lipschitz_estimates.png}
	\vspace*{-5mm}
	\caption{Explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
	\label{fig:mnist_robustness_lipschitz_estimates}
\end{figure}


\begin{figure}[H]
	\includegraphics[width=1.0\linewidth]{mnist_robustness_lipschitz_estimates_separability.png}
	\vspace*{-5mm}
	\caption{Explanations for the minimising of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation_as_a_separability_measure}.}
	\label{fig:mnist_robustness_lipschitz_estimates_separability}
\end{figure} 

%%% mnist_robustness_lipschitz_estimates_old 
%\begin{figure}[H]
%	\includegraphics[width=1.0\linewidth]{mnist_robustness_lipschitz_estimates_old.png}
%	\vspace*{-5mm}
%	\caption{Explanations for the maximizer of the Lipschitz estimate $L(x_i)$ as per \eqref{eq:lipschitz_estimation}.}
%	\label{fig:mnist_robustness_lipschitz_estimates}
%\end{figure} 


%%% CommentDigit 7 experiemnts
%\begin{figure}[H]
%	\vspace*{-5mm}
%	\centering
%	\subfloat[Saliency]{{\includegraphics[width=3cm]{digit_7_deviation_Saliency.png} }}%
%	\qquad
%	\subfloat[Grad*input]{{\includegraphics[width=3cm]{digit_7_deviation_Grad*input.png} }}%
%	\qquad
%	\subfloat[e-LRP]{{\includegraphics[width=3cm]{digit_7_deviation_e-LRP.png} }}%
%	\qquad
%	\subfloat[Occlusion]{{\includegraphics[width=3cm]{digit_7_deviation_Occlusion.png} }}%
%	\qquad
%	\subfloat[Deeplift]{{\includegraphics[width=3cm]{digit_7_deviation_Deeplift.png} }}%
%	\caption{Figures side by side}%
%	\label{fig:digit7_deviations}%
%\end{figure}



\subsection{Results Summary}\label{sec:results_summary} %\subsection{}

%In contrast To Random Forest model, Logistic Regression model predicts the class label \textit{"setosa"} with the $p(setosa) = 0.85$ and gives $p(versicolor) = 0.15$ to class \textit{"versicolor"}. It is visible that both classifiers correctly predict class \textit{"setosa"}, but what made a Logistic Regression model to assign a probability of $p=0.15$ to class \textit{"versicolor"}? In the Figure~\ref{fig:lime_lr_iris_instance_explanation}, it appears that the classifier does it mainly due to a \textit{"petal width"} feature. Though the prediction result is same but an interpretation is different which shows that the models treat feature values in different ways. Therefore, an explanation vector generated for a Logistic Regression prediction contains different values for each feature.
%
%In contrast To Random Forest model, Logistic Regression model predicts the class label \textit{"setosa"} with the $p(setosa) = 0.85$ and gives $p(versicolor) = 0.15$ to class \textit{"versicolor"}. It is visible that both classifiers correctly predict class \textit{"setosa"}, but what made a Logistic Regression model to assign a probability of $p=0.15$ to class \textit{"versicolor"}? In the Figure~\ref{fig:lime_lr_iris_instance_explanation}, it appears that the classifier does it mainly due to a \textit{"petal width"} feature. Though the prediction result is same but an interpretation is different which shows that the models treat feature values in different ways. Therefore, an explanation vector generated for a Logistic Regression prediction contains different values for each feature.


\section{Discussions and Conclusion}\label{sec:conclusion} %\Section{}
\subsection{Limitations} %\subsection{}
\subsection{Future Work} %\subsection{}
\subsection{Conclusion} %\subsection{}

\bibliographystyle{apalike} % alphas
%\bibliographystyle{babplain-lf}
%\bibliographystyle{plain}
%\bibliographystyle{abbrv} % numbers
\bibliography{ref}

% --- Appendices ---

% uncomment the following

% \newpage
% \appendix
% 
% \section{Example appendix}



%\subsubsection{Lipschitz Estimation as a Faithfullness Measure}\label{sec:lipschitz_estimation_as_a_faithfullness_measure} %% \subsubsection
%Utilising the concepts provided in \citep{alvarez2018robustness}, this work proposed a new \textit{faithfullness} measure which evaluates that \textit{"What features are truly relevant?}. Let $h$ be a function which takes $x_i$ and removes or obscure features. The idea is to remove a feature of point $x_i$ to get a new point $x_i\prime$ and then calculate the lipzchitz estimate. Applying this measure shows that which explanation methods are considering which features as more relevant. 


%Quick comment on Alg. 1: it is not clear from the algorithm what is being computed there. Presumably for each $x_{i}$ in X you want to compute maximal |f(x)-f($x_{i}$)|/[x-$x_{i}$| where $x \in N(x_{i})?$ You could also explain what the numbers in Tab. 1 are. The level of detail here is such that I could replicate your experiment, if I had the data and if I wanted to. E.g., you need to be more explicit in the algorithm and also define what you mean by âaggregationâ in Tab. 1.


%\textit{Sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explaining and defining Explanation Evaluation Framework (RQ3)}
%\end{itemize}

%\begin{definition}{Stability:}
%	If data points, \(\left\{x_{i},...,x_{n}\right\} \in X,i \in \left\{1,...,N\right\}\) are classified among predicted labels \(\left\{l_{j},...,l_{k}\right\} \in L, j \in \left\{1,...,K\right\}\), then corresponding explanations \(\left\{e_{i},...,e_{n}\right\} \in E\) should form the $K$ clusters  \(\left\{c_{j},...,c_{k}\right\} \in C\) which, follow the similar classification as predicted labels.
%	\begin{align}
%	\forall i,j,k \in N \;\;\; f\left(x_{i}\right) = f\left(x_{j}\right), \;\; f\left(x_{i}\right) \neq f\left(x_{k}\right) \Rightarrow \\
%	sim(g(f(x_{i})), g(f(x_{j}))) < sim(g(f(x_{i})), g(f(x_{k}))) 
%	 \\ symbols new line in maths
%	 \; symbols space in maths
%	\end{align}
%\end{definition}

%\subsubsection{Stability Test} %\subsubsection{}
%\textit{Sub-sub-section Scope.}
%\begin{itemize}
%	\item \textit{Explain what stability means in literature}
%	\item \textit{Explain a stability evaluation test algorithm, which is developed in this research.}
%\end{itemize}

%\begin{algorithm}
%	\caption{Training Predictive Models Procedure}
%	\begin{algorithmic}[1]
%		\State Split dataset into training-testing sets.
%		\State Train predictive model \textit{'f'} using training set.
%		\State Predict labels for testing set using predictive model \textit{'f'}
%%		\State Show results
%	\end{algorithmic}
%\end{algorithm} \newline



%\begin{table}[H]
%	\caption{Aggregated Local Lipschitz Continuity}
%	\label{tab:quantitative_experimentation_iris}
%	\begin{center}
%		\begin{tabular}{@{}ccccc@{}}
%			\toprule
%			& $\epsilon$ & LIME & SHAP \\ \midrule
%			
%			% Logistic Regression, Iris
%			\multicolumn{1}{|c|}{Logistic Regression, Iris} &  \multicolumn{1}{c|}{\textit{0.75}} &  \multicolumn{1}{c|}{0.72} & \multicolumn{1}{c|}{0.33} \\ 
%			\midrule
%			
%			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.00}} &  \multicolumn{1}{c|}{0.73} & \multicolumn{1}{c|}{0.34} \\ 
%			\midrule 
%			
%			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.25}} & \multicolumn{1}{c|}{0.74} & \multicolumn{1}{c|}{0.35} \\ 
%			\midrule 		
%			
%			% Random Forest, Iris
%			\multicolumn{1}{|c|}{Random Forest, Iris} &  \multicolumn{1}{c|}{\textit{0.75}} &  \multicolumn{1}{c|}{0.91} & \multicolumn{1}{c|}{0.21} \\ 
%			\midrule
%			
%			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.00}} &  \multicolumn{1}{c|}{0.94} & \multicolumn{1}{c|}{0.22} \\ 
%			\midrule 
%			
%			\multicolumn{1}{|c|}{} &  \multicolumn{1}{c|}{\textit{1.25}} & \multicolumn{1}{c|}{0.94} & \multicolumn{1}{c|}{0.22} \\ 
%			\midrule 		
%		
%		\end{tabular}
%	\end{center}
%\end{table}

\end{document}